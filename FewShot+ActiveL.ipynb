{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FewShot+ActiveL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "https://github.com/vahidseydi/CGN/blob/main/GCN.ipynb",
      "authorship_tag": "ABX9TyMmgjeoBS+cFvlM605ukByV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vahidseydi/CGN/blob/main/FewShot%2BActiveL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQrGTMcwGbkt"
      },
      "source": [
        "#Downloading dataset file from Github"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnHBK6RWuN9j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff71d69a-4fc5-48a0-d880-40bd848a6955"
      },
      "source": [
        "#ابتدا دیتاست موجود در فولدر دیتا در آدرس گیت هاب پروژه، را در این نوت بوک دخیره می کنیم\n",
        "! wget 'https://github.com/vahidseydi/CGN/blob/main/Data/amazon_electronics_computers%20(1).npz?raw=true'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-16 06:34:29--  https://github.com/vahidseydi/CGN/blob/main/Data/amazon_electronics_computers%20(1).npz?raw=true\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/vahidseydi/CGN/raw/main/Data/amazon_electronics_computers%20(1).npz [following]\n",
            "--2021-05-16 06:34:29--  https://github.com/vahidseydi/CGN/raw/main/Data/amazon_electronics_computers%20(1).npz\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/vahidseydi/CGN/main/Data/amazon_electronics_computers%20(1).npz [following]\n",
            "--2021-05-16 06:34:29--  https://raw.githubusercontent.com/vahidseydi/CGN/main/Data/amazon_electronics_computers%20(1).npz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 31921488 (30M) [application/octet-stream]\n",
            "Saving to: ‘amazon_electronics_computers (1).npz?raw=true’\n",
            "\n",
            "amazon_electronics_ 100%[===================>]  30.44M   169MB/s    in 0.2s    \n",
            "\n",
            "2021-05-16 06:34:30 (169 MB/s) - ‘amazon_electronics_computers (1).npz?raw=true’ saved [31921488/31921488]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INK67X5IGkkc"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MYbUcRVdtDe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a34c9ec1-941a-482b-c97a-4603012e0549"
      },
      "source": [
        "import numpy as np\n",
        "#فایل های موجود در دیتاست را می خوانیم\n",
        "npz_data=np.load('/content/amazon_electronics_computers (1).npz?raw=true')\n",
        "npz_data.files"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['adj_data',\n",
              " 'adj_indices',\n",
              " 'adj_indptr',\n",
              " 'adj_shape',\n",
              " 'attr_data',\n",
              " 'attr_indices',\n",
              " 'attr_indptr',\n",
              " 'attr_shape',\n",
              " 'labels',\n",
              " 'class_names']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmNiWW8bd-Xx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "580916ee-9ed3-4f5b-981d-9e8e4465da4e"
      },
      "source": [
        "#کلاس های موجود در دیتاست شامل 10 کلاس می باشد\n",
        "class_names =npz_data['class_names']\n",
        "class_names"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Desktops', 'Data Storage', 'Laptops', 'Monitors',\n",
              "       'Computer Components', 'Video Projectors', 'Routers', 'Tablets',\n",
              "       'Networking Products', 'Webcams'], dtype='<U19')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFpxP_MNMmCX"
      },
      "source": [
        "#تابع زیر برای جداسازی داده های آموزش، تست و اعتبارسنجی استفاده می شود\n",
        "#برابر با تمام داده ها می باشد y\n",
        "labels =npz_data['labels']\n",
        "y=labels.size\n",
        "#p for calculating percentage for idx_train,val,test\n",
        "p = lambda x: x*y/100"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9NewBDvWAdE"
      },
      "source": [
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import torch\n",
        "from scipy.sparse import  csr_matrix\n",
        "\n",
        "\n",
        "def load_data():\n",
        " \n",
        " features = sp.csr_matrix((npz_data['attr_data'], npz_data['attr_indices'], npz_data['attr_indptr']),shape=npz_data['attr_shape'])\n",
        "\n",
        " # build graph\n",
        " \n",
        " adj= sp.csr_matrix(sp.csr_matrix((npz_data['adj_data'], npz_data['adj_indices'], npz_data['adj_indptr']),shape=npz_data['adj_shape']))\n",
        " \n",
        " # build symmetric adjacency matrix\n",
        "\n",
        " adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
        " adj = normalize(adj + sp.eye(adj.shape[0]))\n",
        " #اول اعداد 0 و 1 هست بعد از نرمالایز تغییر میکند \n",
        "\n",
        " labels=npz_data['labels']\n",
        " #لیبل ها را یکی اضافه می کنیم تا لیبل 0 را به داده های بدون لیبل دهیم\n",
        " labels=labels+1\n",
        "\n",
        " #جداسازی داده ها \n",
        " idx_train =range(round(p(40)))\n",
        " idx_val = range(idx_train[-1],idx_train[-1]+round(p(30)))\n",
        " idx_test =range(idx_val[-1],idx_val[-1]+round(p(30)))\n",
        "\n",
        " idx_train = torch.LongTensor(idx_train)\n",
        " idx_val = torch.LongTensor(idx_val)\n",
        " idx_test = torch.LongTensor(idx_test)\n",
        "\n",
        " features = torch.FloatTensor(np.array(features.todense()))\n",
        " adj = torch.FloatTensor(np.array(adj.todense()))\n",
        " labels = torch.LongTensor(labels)\n",
        "\n",
        "\n",
        " return adj, features, labels, idx_train, idx_val, idx_test"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9YheZ8yNTSL"
      },
      "source": [
        "def normalize(mx):\n",
        "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
        "    rowsum = np.array(mx.sum(1))\n",
        "    #sum in every row \n",
        "    r_inv = np.power(rowsum, -1).flatten()\n",
        "    # every sum to the power of -1 \n",
        "    r_inv[np.isinf(r_inv)] = 0.\n",
        "    r_mat_inv = sp.diags(r_inv)\n",
        "    #diagonal matrice \n",
        "    mx = r_mat_inv.dot(mx)\n",
        "    return mx"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iha5MQaYDJpG"
      },
      "source": [
        "def accuracy(output, labels):\n",
        "    preds = output.max(1)[1].type_as(labels)\n",
        "    correct = preds.eq(labels).double()\n",
        "    correct = correct.sum()\n",
        "    return correct / len(labels)    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQLBOry4WJvV"
      },
      "source": [
        "adj, features, labels, idx_train, idx_val, idx_test = load_data()\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsHyFUvEUMFQ"
      },
      "source": [
        "\n",
        "\n",
        "#Constructing Train data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0-99ajritPS"
      },
      "source": [
        "#20% of train data for labeled\n",
        "#80% of train data for unlabeled\n",
        "#جداسازی داده های با لیبل و بدون لیبل برای آموزش \n",
        "\n",
        "p_train = lambda x: x*len(idx_train)/100\n",
        "\n",
        "idx_labeled_train =range(round(p_train(20)))\n",
        "idx_unlabeled_train = range(idx_labeled_train[-1],idx_labeled_train[-1]+round(p_train(80)))\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUovgcgTOAmt",
        "outputId": "caefe2b9-a32e-4b09-ab96-8d21d9bd6111"
      },
      "source": [
        "print(idx_labeled_train,idx_unlabeled_train)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "range(0, 1100) range(1099, 5500)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6003R_bp4Xkz"
      },
      "source": [
        "#اندیس مربوط به هر کلاسی را جدا می کنیم\n",
        "class_1=[i for i,x in enumerate(labels[idx_labeled_train]) if x==1] \n",
        "class_2=[i for i,x in enumerate(labels[idx_labeled_train]) if x==2] \n",
        "class_3=[i for i,x in enumerate(labels[idx_labeled_train]) if x==3]\n",
        "class_4=[i for i,x in enumerate(labels[idx_labeled_train]) if x==4] \n",
        "class_5=[i for i,x in enumerate(labels[idx_labeled_train]) if x==5] \n",
        "class_6=[i for i,x in enumerate(labels[idx_labeled_train]) if x==6] \n",
        "class_7=[i for i,x in enumerate(labels[idx_labeled_train]) if x==7] \n",
        "class_8=[i for i,x in enumerate(labels[idx_labeled_train]) if x==8] \n",
        "class_9=[i for i,x in enumerate(labels[idx_labeled_train]) if x==9] \n",
        "class_10=[i for i,x in enumerate(labels[idx_labeled_train]) if x==10] "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DEj19oZ9yVr"
      },
      "source": [
        "#می خواهیم از هر کلاسی به صورت رندوم سمپل انتخاب کنیم\n",
        "import random\n",
        "Random_idx_train=[]\n",
        "#با تعداد شات تعیین می کنیم که از هر کلاس چند سمپل نیاز داریم\n",
        "N_shot=1\n",
        "for i in range(N_shot):\n",
        "  Random_idx_train.append(random.choice(class_1))\n",
        "  Random_idx_train.append(random.choice(class_2))\n",
        "  Random_idx_train.append(random.choice(class_3))\n",
        "  Random_idx_train.append(random.choice(class_4))\n",
        "  Random_idx_train.append(random.choice(class_5))\n",
        "  Random_idx_train.append(random.choice(class_6))\n",
        "  Random_idx_train.append(random.choice(class_7))\n",
        "  Random_idx_train.append(random.choice(class_8))\n",
        "  Random_idx_train.append(random.choice(class_9))\n",
        "  Random_idx_train.append(random.choice(class_10))\n",
        "\n",
        "#Random_idx_train#اندیس "
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1_KllTImV3U"
      },
      "source": [
        "#اگر بخواهیم از هر کلاس یک سمپل انتخاب کنیم مجموعا 10 سمپل با لیبل داریم \n",
        "#اگر بخواهیم نسبت 20 به 80 را رعایت کنیم پس لازم است 40 داده بدون لیبل هم انتخاب کنیم\n",
        "\n",
        "hiddenIndex_train=[]\n",
        "\n",
        "for i in range(40*N_shot):\n",
        "  hiddenIndex_train.append(random.choice(idx_unlabeled_train))\n",
        "#hiddenIndex_train  "
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1C808icQZ2u_"
      },
      "source": [
        "#مجموع اندیس های آموزش شامل اندیس های زیر می باشد:\n",
        "#traindata_idx=Random_idx_train+hiddenIndex_train\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRhvyy9HgXT-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "037b47a8-9e08-4f72-baa4-d8e5ec1682a0"
      },
      "source": [
        "#labels_train_hidden:\n",
        "#تمام لیبل ها حتی هیدن ها،برای استفاده در اکتیولرنینگ\n",
        "labels_train_hidden=torch.hstack((labels[Random_idx_train],labels[hiddenIndex_train]))\n",
        "labels_train_hidden.shape"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([50])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8OE8yFZhkFvI",
        "outputId": "2dd84605-2c31-4c58-b3a2-7b6871ee3ad5"
      },
      "source": [
        "labels[Random_idx_train]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOr1K7YJm7td"
      },
      "source": [
        "#labels[Random_idx_train]==labels_train_hidden[0:10*N_shot]\n",
        "#labels[hiddenIndex_train]==labels_train_hidden[10*N_shot:10*N_shot*4+10*N_shot]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7Lwde1zkTe9"
      },
      "source": [
        "labels_train=labels_train_hidden\n",
        "#با توجه به اینکه از هر کلاس چند تا سمپل انتخاب کردیم از شماره آخری بقیه مربوط به هیدن ها میشود که صفر میذاریم\n",
        "labels_train[10*N_shot+1:]=0"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EteMi10CkrbR"
      },
      "source": [
        "#labels_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FiNVOiOZf9OM",
        "outputId": "3c4b9226-d48d-4700-e464-dc04ac3fcb68"
      },
      "source": [
        "#ایجاد ماتریس ویژگی ها از داده های با لیبل و بدون لیبل\n",
        "features_train=torch.vstack((features[Random_idx_train],features[hiddenIndex_train]))\n",
        "features_train.shape"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([50, 767])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R6bLiV3Ynb0l",
        "outputId": "d2925573-cd31-4f10-ef24-545978c06119"
      },
      "source": [
        "features_train[0:10*N_shot]==features[Random_idx_train]\n",
        "features_train[10*N_shot:10*N_shot*4+10*N_shot]==features[hiddenIndex_train]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[True, True, True,  ..., True, True, True],\n",
              "        [True, True, True,  ..., True, True, True],\n",
              "        [True, True, True,  ..., True, True, True],\n",
              "        ...,\n",
              "        [True, True, True,  ..., True, True, True],\n",
              "        [True, True, True,  ..., True, True, True],\n",
              "        [True, True, True,  ..., True, True, True]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 527
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28I52FrOiii1",
        "outputId": "e3562812-30ef-40f6-ac34-160dcc56d70e"
      },
      "source": [
        "#adj[1:10*N_shot,1:10*N_shot].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([9, 9])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 528
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4x-xGrv0Sw0"
      },
      "source": [
        "#ایجاد ماتریس مجاورت از داده های با لیبل و بدون لیبل\n",
        "adj_train=torch.vstack((adj[Random_idx_train],adj[hiddenIndex_train]))\n",
        "\n",
        "for i in hiddenIndex_train :\n",
        "    Random_idx_train.append(i)\n",
        "    \n",
        "#ایجاد تقارن در ماتریس مجاورت\n",
        "adj_train=adj_train[0:10*N_shot*4+10*N_shot,Random_idx_train]"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7_IwfbRKIt_",
        "outputId": "4b75b9c8-f702-48ff-f32c-fea1594d8362"
      },
      "source": [
        "#adj_train.shape"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([50, 50])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kerWgiRDBcDP",
        "outputId": "ac0feb1f-bcf9-419d-f264-9a2859c5e3e6"
      },
      "source": [
        "#Random_idx_train[2]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "226"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 530
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCpOOU5-83cf",
        "outputId": "ee1d2b8f-727b-44bf-ae6d-68183fc26d87"
      },
      "source": [
        "#adj[717].shape"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([13752])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4yLSWzV83Z6",
        "outputId": "852e77ae-613a-4024-a4ba-bf3797164643"
      },
      "source": [
        "#adj_train[0].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([50])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 532
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08hYvuat7Z5w",
        "outputId": "e74f53e3-9ac5-45cc-cdd2-a3684f51e9e4"
      },
      "source": [
        "#adj[782,782]\n",
        "#adj[30,30]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.1429)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 533
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elrp9c9FTi2w"
      },
      "source": [
        "#Constructing Test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jorUo0cyTv7o"
      },
      "source": [
        "##داده های تست و اعتبارسنجی نیز دقیقا مشابه داده های آموزش ایجاد می شوند##\n",
        "\n",
        "#20% of test data for labeled\n",
        "#80% of test data for unlabeled\n",
        "\n",
        "p_test = lambda x: x*len(idx_test)/100\n",
        "\n",
        "idx_labeled_test =range(round(p_test(20)))\n",
        "idx_unlabeled_test = range(idx_labeled_test[-1],idx_labeled_test[-1]+round(p_test(80)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZR91mjVTv7q",
        "outputId": "93ae0a23-2fb6-4b9e-9ac3-91bdda588cf5"
      },
      "source": [
        "print(idx_labeled_test,idx_unlabeled_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "range(0, 825) range(824, 4125)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRdkBXRUTv7s"
      },
      "source": [
        "#اندیس مربوط به هر کلاسی را جدا می کنیم\n",
        "class_1=[i for i,x in enumerate(labels[idx_labeled_test]) if x==1] \n",
        "class_2=[i for i,x in enumerate(labels[idx_labeled_test]) if x==2] \n",
        "class_3=[i for i,x in enumerate(labels[idx_labeled_test]) if x==3]\n",
        "class_4=[i for i,x in enumerate(labels[idx_labeled_test]) if x==4] \n",
        "class_5=[i for i,x in enumerate(labels[idx_labeled_test]) if x==5] \n",
        "class_6=[i for i,x in enumerate(labels[idx_labeled_test]) if x==6] \n",
        "class_7=[i for i,x in enumerate(labels[idx_labeled_test]) if x==7] \n",
        "class_8=[i for i,x in enumerate(labels[idx_labeled_test]) if x==8] \n",
        "class_9=[i for i,x in enumerate(labels[idx_labeled_test]) if x==9] \n",
        "class_10=[i for i,x in enumerate(labels[idx_labeled_test]) if x==10] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AE5Sw2ZTv7t"
      },
      "source": [
        "# از هر کلاسی بتوانیم به صورت رندوم سمپل انتخاب کنیم\n",
        "import random\n",
        "Random_idx_test=[]\n",
        "\n",
        "for i in range(N_shot):\n",
        "  Random_idx_test.append(random.choice(class_1))\n",
        "  Random_idx_test.append(random.choice(class_2))\n",
        "  Random_idx_test.append(random.choice(class_3))\n",
        "  Random_idx_test.append(random.choice(class_4))\n",
        "  Random_idx_test.append(random.choice(class_5))\n",
        "  Random_idx_test.append(random.choice(class_6))\n",
        "  Random_idx_test.append(random.choice(class_7))\n",
        "  Random_idx_test.append(random.choice(class_8))\n",
        "  Random_idx_test.append(random.choice(class_9))\n",
        "  Random_idx_test.append(random.choice(class_10))\n",
        "\n",
        "#Random_idx_test#اندیس "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rv7SWldYTv7u"
      },
      "source": [
        "#اگر بخواهیم از هر کلاس یک سمپل انتخاب کنیم مجموعا 10 سمپل با لیبل داریم \n",
        "#اگر بخواهیم نسبت 20 به 80 را ارعایت کنیم پس لازم است 40 داده بدون لیبل هم انتخاب کنیم\n",
        "\n",
        "hiddenIndex_test=[]\n",
        "\n",
        "for i in range(40*N_shot):\n",
        "  hiddenIndex_test.append(random.choice(idx_unlabeled_test))\n",
        "#hiddenIndex_test "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDcysoakTv7v",
        "outputId": "15edf784-0ab6-42e0-d8b4-aa8b6cdce1c4"
      },
      "source": [
        "labels_test_hidden=torch.hstack((labels[Random_idx_test],labels[hiddenIndex_test]))\n",
        "labels_test_hidden.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([50])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 539
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRnuyYHiTv7w"
      },
      "source": [
        "labels_test=labels_test_hidden\n",
        "#با توجه به اینکه از هر کلاس چند تا سمپل انتخاب کردیم از شماره آخری بقیه مربوط به هیدن ها میشود که صفر میذاریم\n",
        "labels_test[10*N_shot+1:]=0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xtqi7LwXTv7x",
        "outputId": "24caa018-bc94-48fe-b00a-b449622bc8e3"
      },
      "source": [
        "features_test=torch.vstack((features[Random_idx_test],features[hiddenIndex_test]))\n",
        "features_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([50, 767])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 541
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnW1mVA5Tv7y",
        "outputId": "a6a70876-7d3d-4224-bef8-a12f4cda50c1"
      },
      "source": [
        "features_test[0:10*N_shot]==features[Random_idx_test]\n",
        "features_test[10*N_shot:10*N_shot*4+10*N_shot]==features[hiddenIndex_test]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[True, True, True,  ..., True, True, True],\n",
              "        [True, True, True,  ..., True, True, True],\n",
              "        [True, True, True,  ..., True, True, True],\n",
              "        ...,\n",
              "        [True, True, True,  ..., True, True, True],\n",
              "        [True, True, True,  ..., True, True, True],\n",
              "        [True, True, True,  ..., True, True, True]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 542
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3CLd-KVTv7z"
      },
      "source": [
        "adj_test=torch.vstack((adj[Random_idx_test],adj[hiddenIndex_test]))\n",
        "\n",
        "for i in hiddenIndex_test :\n",
        "    Random_idx_test.append(i)\n",
        "\n",
        "adj_test=adj_test[0:10*N_shot*4+10*N_shot,Random_idx_test]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dyz6oDB6Yyuf"
      },
      "source": [
        "#Constructing Validation Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRkhidE6YWTu"
      },
      "source": [
        "#20% of val data for labeled\n",
        "#80% of val data for unlabeled\n",
        "\n",
        "p_val = lambda x: x*len(idx_val)/100\n",
        "\n",
        "idx_labeled_val =range(round(p_val(20)))\n",
        "idx_unlabeled_val = range(idx_labeled_val[-1],idx_labeled_val[-1]+round(p_val(80)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqHcKhKYYWTw",
        "outputId": "0d92d9c7-4f1b-45df-9745-a72a0eb4a320"
      },
      "source": [
        "print(idx_labeled_val,idx_unlabeled_val)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "range(0, 825) range(824, 4125)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hR67HnajYWUE"
      },
      "source": [
        "#اندیس مربوط به هر کلاسی را جدا می کنیم\n",
        "class_1=[i for i,x in enumerate(labels[idx_labeled_val]) if x==1] \n",
        "class_2=[i for i,x in enumerate(labels[idx_labeled_val]) if x==2] \n",
        "class_3=[i for i,x in enumerate(labels[idx_labeled_val]) if x==3]\n",
        "class_4=[i for i,x in enumerate(labels[idx_labeled_val]) if x==4] \n",
        "class_5=[i for i,x in enumerate(labels[idx_labeled_val]) if x==5] \n",
        "class_6=[i for i,x in enumerate(labels[idx_labeled_val]) if x==6] \n",
        "class_7=[i for i,x in enumerate(labels[idx_labeled_val]) if x==7] \n",
        "class_8=[i for i,x in enumerate(labels[idx_labeled_val]) if x==8] \n",
        "class_9=[i for i,x in enumerate(labels[idx_labeled_val]) if x==9] \n",
        "class_10=[i for i,x in enumerate(labels[idx_labeled_val]) if x==10] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9H3cAsQYWUF"
      },
      "source": [
        "# از هر کلاسی بتوانیم به صورت رندوم سمپل انتخاب کنیم\n",
        "import random\n",
        "Random_idx_val=[]\n",
        "\n",
        "for i in range(N_shot):\n",
        "  Random_idx_val.append(random.choice(class_1))\n",
        "  Random_idx_val.append(random.choice(class_2))\n",
        "  Random_idx_val.append(random.choice(class_3))\n",
        "  Random_idx_val.append(random.choice(class_4))\n",
        "  Random_idx_val.append(random.choice(class_5))\n",
        "  Random_idx_val.append(random.choice(class_6))\n",
        "  Random_idx_val.append(random.choice(class_7))\n",
        "  Random_idx_val.append(random.choice(class_8))\n",
        "  Random_idx_val.append(random.choice(class_9))\n",
        "  Random_idx_val.append(random.choice(class_10))\n",
        "\n",
        "#Random_idx_val#اندیس "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0eRUuhJqYWUH"
      },
      "source": [
        "#اگر بخواهیم از هر کلاس یک سمپل انتخاب کنیم مجموعا 10 سمپل با لیبل داریم \n",
        "#اگر بخواهیم نسبت 20 به 80 را ارعایت کنیم پس لازم است 40 داده بدون لیبل هم انتخاب کنیم\n",
        "\n",
        "hiddenIndex_val=[]\n",
        "\n",
        "for i in range(40*N_shot):\n",
        "  hiddenIndex_val.append(random.choice(idx_unlabeled_val))\n",
        "#hiddenIndex_val "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GGJZ-_e4YWUI",
        "outputId": "783a7176-1c88-44ac-c096-fc891ce59f11"
      },
      "source": [
        "labels_val_hidden=torch.hstack((labels[Random_idx_val],labels[hiddenIndex_val]))\n",
        "labels_val_hidden.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([50])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 549
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEhEarXCYWUI"
      },
      "source": [
        "labels_val=labels_val_hidden\n",
        "#با توجه به اینکه از هر کلاس چند تا سمپل انتخاب کردیم از شماره آخری بقیه مربوط به هیدن ها میشود که صفر میذاریم\n",
        "labels_val[10*N_shot+1:]=0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3F00cnT2YWUJ",
        "outputId": "3da69aa9-aa60-4135-e7ae-c9a1197e888f"
      },
      "source": [
        "features_val=torch.vstack((features[Random_idx_val],features[hiddenIndex_val]))\n",
        "features_val.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([50, 767])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 551
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_oHe-7abYWUK",
        "outputId": "19f3fff4-f4fc-4ea8-81dc-a4b936cff54b"
      },
      "source": [
        "features_val[0:10*N_shot]==features[Random_idx_val]\n",
        "features_val[10*N_shot:10*N_shot*4+10*N_shot]==features[hiddenIndex_val]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[True, True, True,  ..., True, True, True],\n",
              "        [True, True, True,  ..., True, True, True],\n",
              "        [True, True, True,  ..., True, True, True],\n",
              "        ...,\n",
              "        [True, True, True,  ..., True, True, True],\n",
              "        [True, True, True,  ..., True, True, True],\n",
              "        [True, True, True,  ..., True, True, True]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 552
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJhiH9xIYWUL"
      },
      "source": [
        "adj_val=torch.vstack((adj[Random_idx_val],adj[hiddenIndex_val]))\n",
        "\n",
        "for i in hiddenIndex_val :\n",
        "    Random_idx_val.append(i)\n",
        "\n",
        "adj_val=adj_val[0:10*N_shot*4+10*N_shot,Random_idx_val]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rXMvNIZF6BU"
      },
      "source": [
        "# Constructing hidden_labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GL4nZ9-pMmSM"
      },
      "source": [
        "#hidden_labels:\n",
        "#برای استفاده در قسمت اکتیولرنینگ که فقط بر روی هیدن لیبل ها اعمال شود\n",
        "#سایز آن به اندازه تعداد کل سمپل ها می باشد\n",
        "hidden_labels_train=np.zeros((10*N_shot*4+10*N_shot), dtype='int')\n",
        "hidden_labels_train[10*N_shot:10*N_shot*4+10*N_shot]=1\n",
        "#RuntimeError: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead. site:stackoverflow.com\n",
        "#برای همین به تنسور تبدیلش کردم\n",
        "hidden_labels_train=torch.tensor(hidden_labels_train)\n",
        "#hidden_labels_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSwwuzUJFR1n"
      },
      "source": [
        "hidden_labels_test=np.zeros((10*N_shot*4+10*N_shot), dtype='int')\n",
        "hidden_labels_test[10*N_shot:10*N_shot*4+10*N_shot]=1\n",
        "hidden_labels_test=torch.tensor(hidden_labels_test)\n",
        "#hidden_labels_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46SwJE2eFRj4"
      },
      "source": [
        "hidden_labels_val=np.zeros((10*N_shot*4+10*N_shot), dtype='int')\n",
        "hidden_labels_val[10*N_shot:10*N_shot*4+10*N_shot]=1\n",
        "hidden_labels_val=torch.tensor(hidden_labels_val)\n",
        "#hidden_labels_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScJWIK7yGDcY"
      },
      "source": [
        "#Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "us3XSHLHDWoP"
      },
      "source": [
        "\n",
        "import math\n",
        "\n",
        "import torch\n",
        "\n",
        "from torch.nn.parameter import Parameter\n",
        "\n",
        "#class parameter:\n",
        "#پارامترها را کش می کند\n",
        "\n",
        "from torch.nn.modules.module import Module\n",
        "\n",
        "\n",
        "class GraphConvolution(Module):\n",
        "    \"\"\"\n",
        "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features ,out_features, bias=True):\n",
        "        super(GraphConvolution, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        \n",
        "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
        "        if bias:\n",
        "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
        "        self.weight.data.uniform_(-stdv, stdv)\n",
        "        if self.bias is not None:\n",
        "            self.bias.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self, input, input_adj):\n",
        "        support = torch.mm(input, self.weight)\n",
        "        #torch.mm=matrix multiplication\n",
        "        #torch.spmm=Sparse matrix multiplication\n",
        "        #ضرب ویژگی ها در ماتریس مجاورت\n",
        "        output = torch.spmm(input_adj, support)\n",
        "        if self.bias is not None:\n",
        "            return output + self.bias\n",
        "        else:\n",
        "            return output\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vTcR4OmGJIR"
      },
      "source": [
        "#Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXDaJHg7DeHl"
      },
      "source": [
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self, nfeat, nhid,nclass, dropout):\n",
        "        super(GCN, self).__init__()\n",
        "\n",
        "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
        "        self.gc2 = GraphConvolution(nhid, nclass)\n",
        "        self.dropout = dropout\n",
        "        #self.mode=mode\n",
        "        \n",
        "    def forward(self, x, input_adj,hidden_labels):\n",
        "        #if N-shot=10 then samples=500\n",
        "        #features_num=767\n",
        "        #x.shape=500*767\n",
        "        #samples=500,features=767\n",
        "        x = F.relu(self.gc1(x, input_adj))\n",
        "        #x.shape=500*16\n",
        "        #hidden_unit=16\n",
        "        x = F.dropout(x, self.dropout,training=self.training)\n",
        "        #training=self.training parameter of drop out func\n",
        "        #x.shape=500*16\n",
        "        #hidden_unit=16\n",
        "        #طبق مقاله مربوط به اکتیولرنینگ بعد از لایه اول اکتیو لرنینگ را اجرا می کنیم تا تصمیم گیری شود که لیبل کدام یک از داده های هیدن را اضافه کنیم\n",
        "        x,decision=self.active(x,hidden_labels)\n",
        "        x = self.gc2(x, input_adj)\n",
        "        #x.shape=500*11\n",
        "        #class_num=11\n",
        "        return F.log_softmax(x, dim=1),decision\n",
        "\n",
        "    def active(self, x,hidden_labels):\n",
        "        #x.shape=500*16\n",
        "        x_active = torch.transpose(x, 0, 1) \n",
        "        x_active=x_active.unsqueeze_(0)\n",
        "        #unsqueeze_داده 2 بعدی را به 3 بعدی تبدیل می کند و بعد اول را 1 می کند\n",
        "        #این کار را برای آنکه لایه کانولوشن درست کار کند انجام دادم\n",
        "        #x_active.shape=1*16*500\n",
        "        #x_active.shape[1]=16\n",
        "\n",
        "        conv_active_1 = nn.Conv1d(x_active.shape[1],x_active.shape[1],1)\n",
        "        bn_active = nn.BatchNorm1d(x_active.shape[1])\n",
        "        conv_active_2 = nn.Conv1d(x_active.shape[1],1,1)\n",
        "\n",
        "        x_active = conv_active_1(x_active)\n",
        "        #x_active.shape=1*16*500\n",
        "        x_active = F.leaky_relu(bn_active(x_active))\n",
        "        #x_active.shape=1*16*500\n",
        "        x_active = conv_active_2(x_active)\n",
        "        #x_active.shape=1*1*500 \n",
        "        x_active = torch.transpose(x_active, 0, 1) \n",
        "        x_active = x_active.squeeze(0) \n",
        "        #x_active.shape=1*500 \n",
        "        #squeeze_ داده 3 بعدی را به 2 بعدی تبدیل می کند و مجدداورودی به حالت اول بر می گردد\n",
        "\n",
        "        x_active = x_active - (1-hidden_labels)*1e8\n",
        "        x_active = F.softmax(x_active,0)\n",
        "        #x_active.shape=1*500 \n",
        "        \n",
        "        #برای آنکه فقط روی هیدن لیبل ها تصمیم گیری شود، با ضرب زیر مقدار مربوط به سمپل های با لیبل صفر می شود\n",
        "        x_active = x_active*hidden_labels \n",
        "        decision = torch.argmax(x_active, 1)\n",
        "        \n",
        "        return x,decision"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYdsEgftGRS8"
      },
      "source": [
        "#Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kAvN1C2QDq89",
        "outputId": "fe7d4efd-adf4-442f-cc36-9522dbf722bc"
      },
      "source": [
        "\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "import time\n",
        "import argparse\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "# Training settings\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
        "                    help='Disables CUDA training.')\n",
        "parser.add_argument('--fastmode', action='store_true', default=False,\n",
        "                    help='Validate during training pass.')\n",
        "parser.add_argument('--seed', type=int, default=42, help='Random seed.')\n",
        "parser.add_argument('--epochs', type=int, default=1500,\n",
        "                    help='Number of epochs to train.')\n",
        "parser.add_argument('--lr', type=float, default=0.01,\n",
        "                    help='Initial learning rate.')\n",
        "parser.add_argument('--weight_decay', type=float, default=5e-4,\n",
        "                    help='Weight decay (L2 loss on parameters).')\n",
        "parser.add_argument('--hidden', type=int, default=16,\n",
        "                    help='Number of hidden units.')\n",
        "parser.add_argument('--dropout', type=float, default=0.5,\n",
        "                    help='Dropout rate (1 - keep probability).')\n",
        "\n",
        "#args = parser.parse_args()\n",
        "#error midad khate bala,khate paein jaigozin shod\n",
        "\n",
        "args = parser.parse_known_args()[0]\n",
        "\n",
        "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
        "\n",
        "np.random.seed(args.seed)\n",
        "torch.manual_seed(args.seed)\n",
        "if args.cuda:\n",
        "    torch.cuda.manual_seed(args.seed)\n",
        "\n",
        "\n",
        "# Model and optimizer\n",
        "model = GCN(nfeat=features.shape[1],\n",
        "            nhid=args.hidden,\n",
        "            nclass=class_names.size+1,\n",
        "            #بدلیل صفر کردن لیبل هیدن ها تعداد کلاسها یکی بیشتر میشود\n",
        "            dropout=args.dropout)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(),\n",
        "                       lr=args.lr, weight_decay=args.weight_decay)\n",
        "\n",
        "# to set cuda as your device if possible\n",
        "#training on  GPU\n",
        "\n",
        "if args.cuda:\n",
        "    model.cuda()\n",
        "    features_test = features_test.cuda()\n",
        "    adj_test = adj_test.cuda()\n",
        "    labels_test = labels_test.cuda()\n",
        "    features_train = features_train.cuda()\n",
        "    adj_train = adj_train.cuda()\n",
        "    labels_train = labels_train.cuda()\n",
        "    features_val = features_val.cuda()\n",
        "    adj_val = adj_val.cuda()\n",
        "    labels_val = labels_val.cuda()\n",
        "    # train:adjust the weights on the neural network\n",
        "    # validation:used to minimize overfitting\n",
        " \n",
        "def test(): \n",
        "    model.eval()\n",
        "    output,act_dec = model(features_test,adj_test,hidden_labels_test)\n",
        "    \n",
        "    #مقدار تصمیم گیری شده در اکتیولرنینگ یک اندیس از هیدن لیبل هاست که بیشترین تاثیر در بهبود خطاهای شبکه را دارد\n",
        "    #(در واقع مقدارش را صفر میکنیم)لیبل آن را بدست آورده و به لیبل ها اضافه می کنیمو از هیدن ها هم آن را حذف میکنیم\n",
        "    Uncover_label=labels_test_hidden[act_dec]\n",
        "    labels_test[act_dec]=Uncover_label\n",
        "    hidden_labels_test[act_dec]=0\n",
        "\n",
        "    loss_test = F.nll_loss(output, labels_test)\n",
        "    acc_test = accuracy(output, labels_test)\n",
        "    print(\"Test set results:\",\n",
        "          \"loss= {:.4f}\".format(loss_test.item()),\n",
        "          \"accuracy= {:.4f}\".format(acc_test.item()))\n",
        "\n",
        "def train(epoch):\n",
        "  model.train()\n",
        "  optimizer.zero_grad()\n",
        "  output,act_dec = model(features_train,adj_train,hidden_labels_train)\n",
        "\n",
        "  Uncover_label=labels_train_hidden[act_dec]\n",
        "  labels_train[act_dec]=Uncover_label\n",
        "  hidden_labels_train[act_dec]=0\n",
        "\n",
        "  loss_train = F.nll_loss(output, labels_train)\n",
        "  acc_train = accuracy(output,labels_train)\n",
        "  # Computing the gradients necessary to adjust the weights\n",
        "  loss_train.backward()\n",
        "  # Updating the weights of the neural network\n",
        "  optimizer.step()\n",
        "  losses.append(loss_train.item())\n",
        "  acc.append(acc_train.item())\n",
        "\n",
        "  if not args.fastmode:\n",
        "    # Evaluate validation set performance separately,\n",
        "    # deactivates dropout during validation run.\n",
        "    model.eval()\n",
        "    output,act_dec = model(features_val, adj_val,hidden_labels_val)\n",
        "    \n",
        "    Uncover_label=labels_val_hidden[act_dec]\n",
        "    labels_val[act_dec]=Uncover_label\n",
        "    hidden_labels_val[act_dec]=0\n",
        "  \n",
        "    loss_val = F.nll_loss(output, labels_val)\n",
        "    acc_val = accuracy(output, labels_val)\n",
        "    losses_val.append(loss_val.item())\n",
        "    acc_valid.append(acc_val.item())\n",
        "\n",
        "    print('Epoch: {:04d}'.format(epoch+1),\n",
        "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
        "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
        "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
        "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
        "          'time: {:.4f}s'.format(time.time() - t))\n",
        "\n",
        "# Train model\n",
        "t_total = time.time()\n",
        "losses = []\n",
        "acc=[]\n",
        "losses_val = []\n",
        "acc_valid=[]\n",
        "t = time.time()\n",
        "for epoch in range(args.epochs):\n",
        "    train(epoch)\n",
        "\n",
        "# Testing\n",
        "test()\n",
        "\n",
        "print(model)\n",
        "\n",
        "#plotting loss_train_val:\n",
        "\n",
        "plt.plot(np.array(losses),label ='loss_train Plot')\n",
        "plt.plot(np.array(losses_val),label ='loss_val Plot')\n",
        "plt.title('loss_train_validation Plot')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('value')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "#plotting acc_train_val:\n",
        "\n",
        "plt.plot(np.array(acc),label ='Accuracy_train Plot')\n",
        "plt.plot(np.array(acc_valid),label ='Accuracy_val Plot')\n",
        "plt.title('acc_train_validation Plot')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('value')\n",
        "plt.legend()\n",
        "plt.show()    \n",
        "\n",
        "print(\"Optimization Finished!\")\n",
        "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0001 loss_train: 2.3390 acc_train: 0.0200 loss_val: 2.3000 acc_val: 0.0800 time: 0.0066s\n",
            "Epoch: 0002 loss_train: 2.3118 acc_train: 0.0800 loss_val: 2.2696 acc_val: 0.1400 time: 0.0136s\n",
            "Epoch: 0003 loss_train: 2.2864 acc_train: 0.2400 loss_val: 2.2361 acc_val: 0.7800 time: 0.0208s\n",
            "Epoch: 0004 loss_train: 2.2692 acc_train: 0.7600 loss_val: 2.2000 acc_val: 0.7800 time: 0.0286s\n",
            "Epoch: 0005 loss_train: 2.2239 acc_train: 0.7800 loss_val: 2.1610 acc_val: 0.7800 time: 0.0457s\n",
            "Epoch: 0006 loss_train: 2.1962 acc_train: 0.7600 loss_val: 2.1230 acc_val: 0.7800 time: 0.0581s\n",
            "Epoch: 0007 loss_train: 2.1492 acc_train: 0.7800 loss_val: 2.0872 acc_val: 0.7800 time: 0.0680s\n",
            "Epoch: 0008 loss_train: 2.1174 acc_train: 0.7800 loss_val: 2.0546 acc_val: 0.7800 time: 0.0776s\n",
            "Epoch: 0009 loss_train: 2.1195 acc_train: 0.7800 loss_val: 2.0255 acc_val: 0.7800 time: 0.0867s\n",
            "Epoch: 0010 loss_train: 1.9981 acc_train: 0.7800 loss_val: 1.9989 acc_val: 0.7800 time: 0.0972s\n",
            "Epoch: 0011 loss_train: 1.9913 acc_train: 0.7800 loss_val: 1.9741 acc_val: 0.7800 time: 0.1084s\n",
            "Epoch: 0012 loss_train: 1.9983 acc_train: 0.7800 loss_val: 1.9506 acc_val: 0.7800 time: 0.1193s\n",
            "Epoch: 0013 loss_train: 1.9319 acc_train: 0.7800 loss_val: 1.9278 acc_val: 0.7800 time: 0.1268s\n",
            "Epoch: 0014 loss_train: 1.8640 acc_train: 0.7800 loss_val: 1.9054 acc_val: 0.7800 time: 0.1337s\n",
            "Epoch: 0015 loss_train: 1.8337 acc_train: 0.7800 loss_val: 1.8831 acc_val: 0.7800 time: 0.1402s\n",
            "Epoch: 0016 loss_train: 1.7966 acc_train: 0.7800 loss_val: 1.8610 acc_val: 0.7800 time: 0.1493s\n",
            "Epoch: 0017 loss_train: 1.7259 acc_train: 0.7800 loss_val: 1.8390 acc_val: 0.7800 time: 0.1579s\n",
            "Epoch: 0018 loss_train: 1.7052 acc_train: 0.7800 loss_val: 1.8174 acc_val: 0.7800 time: 0.1646s\n",
            "Epoch: 0019 loss_train: 1.7072 acc_train: 0.7600 loss_val: 1.7963 acc_val: 0.7800 time: 0.1731s\n",
            "Epoch: 0020 loss_train: 1.6581 acc_train: 0.7800 loss_val: 1.7756 acc_val: 0.7800 time: 0.1803s\n",
            "Epoch: 0021 loss_train: 1.6188 acc_train: 0.7800 loss_val: 1.7557 acc_val: 0.7800 time: 0.1880s\n",
            "Epoch: 0022 loss_train: 1.5488 acc_train: 0.7800 loss_val: 1.7364 acc_val: 0.7800 time: 0.1954s\n",
            "Epoch: 0023 loss_train: 1.5440 acc_train: 0.7800 loss_val: 1.7180 acc_val: 0.7800 time: 0.2020s\n",
            "Epoch: 0024 loss_train: 1.5030 acc_train: 0.7800 loss_val: 1.7006 acc_val: 0.7800 time: 0.2105s\n",
            "Epoch: 0025 loss_train: 1.4877 acc_train: 0.7800 loss_val: 1.6841 acc_val: 0.7800 time: 0.2203s\n",
            "Epoch: 0026 loss_train: 1.4696 acc_train: 0.7800 loss_val: 1.6686 acc_val: 0.7800 time: 0.2274s\n",
            "Epoch: 0027 loss_train: 1.4194 acc_train: 0.7800 loss_val: 1.6540 acc_val: 0.7800 time: 0.2341s\n",
            "Epoch: 0028 loss_train: 1.4582 acc_train: 0.7800 loss_val: 1.6404 acc_val: 0.7800 time: 0.2407s\n",
            "Epoch: 0029 loss_train: 1.4292 acc_train: 0.7800 loss_val: 1.6277 acc_val: 0.7800 time: 0.2487s\n",
            "Epoch: 0030 loss_train: 1.3914 acc_train: 0.7800 loss_val: 1.6158 acc_val: 0.7800 time: 0.2575s\n",
            "Epoch: 0031 loss_train: 1.4073 acc_train: 0.7800 loss_val: 1.6051 acc_val: 0.7800 time: 0.2651s\n",
            "Epoch: 0032 loss_train: 1.3701 acc_train: 0.7800 loss_val: 1.5951 acc_val: 0.7800 time: 0.2722s\n",
            "Epoch: 0033 loss_train: 1.3870 acc_train: 0.7800 loss_val: 1.5859 acc_val: 0.7800 time: 0.2790s\n",
            "Epoch: 0034 loss_train: 1.3099 acc_train: 0.7800 loss_val: 1.5773 acc_val: 0.7800 time: 0.2863s\n",
            "Epoch: 0035 loss_train: 1.3836 acc_train: 0.7800 loss_val: 1.5696 acc_val: 0.7800 time: 0.2939s\n",
            "Epoch: 0036 loss_train: 1.3066 acc_train: 0.7800 loss_val: 1.5624 acc_val: 0.7800 time: 0.3018s\n",
            "Epoch: 0037 loss_train: 1.3474 acc_train: 0.7800 loss_val: 1.5559 acc_val: 0.7800 time: 0.3098s\n",
            "Epoch: 0038 loss_train: 1.3128 acc_train: 0.7800 loss_val: 1.5499 acc_val: 0.7800 time: 0.3188s\n",
            "Epoch: 0039 loss_train: 1.3252 acc_train: 0.7800 loss_val: 1.5443 acc_val: 0.7800 time: 0.3264s\n",
            "Epoch: 0040 loss_train: 1.3051 acc_train: 0.7800 loss_val: 1.5389 acc_val: 0.7800 time: 0.3338s\n",
            "Epoch: 0041 loss_train: 1.3198 acc_train: 0.7800 loss_val: 1.5338 acc_val: 0.7800 time: 0.3408s\n",
            "Epoch: 0042 loss_train: 1.3014 acc_train: 0.7800 loss_val: 1.5289 acc_val: 0.7800 time: 0.3481s\n",
            "Epoch: 0043 loss_train: 1.2813 acc_train: 0.7800 loss_val: 1.5241 acc_val: 0.7800 time: 0.3546s\n",
            "Epoch: 0044 loss_train: 1.2501 acc_train: 0.7800 loss_val: 1.5194 acc_val: 0.7800 time: 0.3624s\n",
            "Epoch: 0045 loss_train: 1.2632 acc_train: 0.7800 loss_val: 1.5149 acc_val: 0.7800 time: 0.3694s\n",
            "Epoch: 0046 loss_train: 1.2225 acc_train: 0.7800 loss_val: 1.5104 acc_val: 0.7800 time: 0.3778s\n",
            "Epoch: 0047 loss_train: 1.2489 acc_train: 0.7800 loss_val: 1.5061 acc_val: 0.7800 time: 0.3846s\n",
            "Epoch: 0048 loss_train: 1.2539 acc_train: 0.7800 loss_val: 1.5018 acc_val: 0.7800 time: 0.3915s\n",
            "Epoch: 0049 loss_train: 1.2556 acc_train: 0.7800 loss_val: 1.4975 acc_val: 0.7800 time: 0.3983s\n",
            "Epoch: 0050 loss_train: 1.2100 acc_train: 0.7800 loss_val: 1.4934 acc_val: 0.7800 time: 0.4061s\n",
            "Epoch: 0051 loss_train: 1.2223 acc_train: 0.7800 loss_val: 1.4892 acc_val: 0.7800 time: 0.4142s\n",
            "Epoch: 0052 loss_train: 1.2492 acc_train: 0.7800 loss_val: 1.4850 acc_val: 0.7800 time: 0.4272s\n",
            "Epoch: 0053 loss_train: 1.2534 acc_train: 0.7800 loss_val: 1.4808 acc_val: 0.7800 time: 0.4340s\n",
            "Epoch: 0054 loss_train: 1.1929 acc_train: 0.7800 loss_val: 1.4765 acc_val: 0.7800 time: 0.4413s\n",
            "Epoch: 0055 loss_train: 1.1733 acc_train: 0.7800 loss_val: 1.4722 acc_val: 0.7800 time: 0.4489s\n",
            "Epoch: 0056 loss_train: 1.2026 acc_train: 0.7800 loss_val: 1.4679 acc_val: 0.7800 time: 0.4565s\n",
            "Epoch: 0057 loss_train: 1.2081 acc_train: 0.7800 loss_val: 1.4635 acc_val: 0.7800 time: 0.4642s\n",
            "Epoch: 0058 loss_train: 1.2078 acc_train: 0.7800 loss_val: 1.4592 acc_val: 0.7800 time: 0.4709s\n",
            "Epoch: 0059 loss_train: 1.1764 acc_train: 0.7800 loss_val: 1.4549 acc_val: 0.7800 time: 0.4786s\n",
            "Epoch: 0060 loss_train: 1.1576 acc_train: 0.7800 loss_val: 1.4505 acc_val: 0.7800 time: 0.4856s\n",
            "Epoch: 0061 loss_train: 1.1411 acc_train: 0.7800 loss_val: 1.4461 acc_val: 0.7800 time: 0.4931s\n",
            "Epoch: 0062 loss_train: 1.1383 acc_train: 0.7800 loss_val: 1.4419 acc_val: 0.7800 time: 0.4998s\n",
            "Epoch: 0063 loss_train: 1.1521 acc_train: 0.7800 loss_val: 1.4375 acc_val: 0.7800 time: 0.5079s\n",
            "Epoch: 0064 loss_train: 1.1621 acc_train: 0.7800 loss_val: 1.4332 acc_val: 0.7800 time: 0.5147s\n",
            "Epoch: 0065 loss_train: 1.1509 acc_train: 0.7800 loss_val: 1.4290 acc_val: 0.7800 time: 0.5228s\n",
            "Epoch: 0066 loss_train: 1.1369 acc_train: 0.7800 loss_val: 1.4247 acc_val: 0.7800 time: 0.5306s\n",
            "Epoch: 0067 loss_train: 1.1193 acc_train: 0.7800 loss_val: 1.4206 acc_val: 0.7800 time: 0.5377s\n",
            "Epoch: 0068 loss_train: 1.1193 acc_train: 0.7800 loss_val: 1.4164 acc_val: 0.7800 time: 0.5445s\n",
            "Epoch: 0069 loss_train: 1.0959 acc_train: 0.7800 loss_val: 1.4123 acc_val: 0.7800 time: 0.5516s\n",
            "Epoch: 0070 loss_train: 1.0912 acc_train: 0.7800 loss_val: 1.4082 acc_val: 0.7800 time: 0.5591s\n",
            "Epoch: 0071 loss_train: 1.1223 acc_train: 0.7800 loss_val: 1.4042 acc_val: 0.7800 time: 0.5663s\n",
            "Epoch: 0072 loss_train: 1.1111 acc_train: 0.7800 loss_val: 1.4001 acc_val: 0.7800 time: 0.5757s\n",
            "Epoch: 0073 loss_train: 1.1084 acc_train: 0.7800 loss_val: 1.3961 acc_val: 0.7800 time: 0.5835s\n",
            "Epoch: 0074 loss_train: 1.1049 acc_train: 0.7800 loss_val: 1.3921 acc_val: 0.7800 time: 0.5913s\n",
            "Epoch: 0075 loss_train: 1.1191 acc_train: 0.7800 loss_val: 1.3880 acc_val: 0.7800 time: 0.5990s\n",
            "Epoch: 0076 loss_train: 1.1068 acc_train: 0.7800 loss_val: 1.3839 acc_val: 0.7800 time: 0.6105s\n",
            "Epoch: 0077 loss_train: 1.0843 acc_train: 0.7800 loss_val: 1.3797 acc_val: 0.7800 time: 0.6172s\n",
            "Epoch: 0078 loss_train: 1.0767 acc_train: 0.7800 loss_val: 1.3757 acc_val: 0.7800 time: 0.6270s\n",
            "Epoch: 0079 loss_train: 1.0930 acc_train: 0.7800 loss_val: 1.3716 acc_val: 0.7800 time: 0.6345s\n",
            "Epoch: 0080 loss_train: 1.0616 acc_train: 0.7800 loss_val: 1.3678 acc_val: 0.7800 time: 0.6461s\n",
            "Epoch: 0081 loss_train: 1.0639 acc_train: 0.7800 loss_val: 1.3639 acc_val: 0.7800 time: 0.6547s\n",
            "Epoch: 0082 loss_train: 1.0585 acc_train: 0.7800 loss_val: 1.3601 acc_val: 0.7800 time: 0.6627s\n",
            "Epoch: 0083 loss_train: 1.0729 acc_train: 0.7800 loss_val: 1.3563 acc_val: 0.7800 time: 0.6708s\n",
            "Epoch: 0084 loss_train: 1.0435 acc_train: 0.7800 loss_val: 1.3525 acc_val: 0.7800 time: 0.6790s\n",
            "Epoch: 0085 loss_train: 1.0504 acc_train: 0.7800 loss_val: 1.3489 acc_val: 0.7800 time: 0.6878s\n",
            "Epoch: 0086 loss_train: 1.0392 acc_train: 0.7800 loss_val: 1.3457 acc_val: 0.7800 time: 0.6958s\n",
            "Epoch: 0087 loss_train: 1.0434 acc_train: 0.7800 loss_val: 1.3423 acc_val: 0.7800 time: 0.7042s\n",
            "Epoch: 0088 loss_train: 1.0381 acc_train: 0.7800 loss_val: 1.3390 acc_val: 0.7800 time: 0.7125s\n",
            "Epoch: 0089 loss_train: 1.0709 acc_train: 0.7800 loss_val: 1.3355 acc_val: 0.7800 time: 0.7195s\n",
            "Epoch: 0090 loss_train: 1.0332 acc_train: 0.7800 loss_val: 1.3321 acc_val: 0.7800 time: 0.7265s\n",
            "Epoch: 0091 loss_train: 1.0425 acc_train: 0.7800 loss_val: 1.3286 acc_val: 0.7800 time: 0.7364s\n",
            "Epoch: 0092 loss_train: 1.0284 acc_train: 0.7800 loss_val: 1.3253 acc_val: 0.7800 time: 0.7438s\n",
            "Epoch: 0093 loss_train: 1.0123 acc_train: 0.7800 loss_val: 1.3223 acc_val: 0.7800 time: 0.7517s\n",
            "Epoch: 0094 loss_train: 1.0308 acc_train: 0.7800 loss_val: 1.3193 acc_val: 0.7800 time: 0.7623s\n",
            "Epoch: 0095 loss_train: 1.0239 acc_train: 0.7800 loss_val: 1.3163 acc_val: 0.7800 time: 0.7707s\n",
            "Epoch: 0096 loss_train: 1.0161 acc_train: 0.7800 loss_val: 1.3133 acc_val: 0.7800 time: 0.7779s\n",
            "Epoch: 0097 loss_train: 1.0386 acc_train: 0.7800 loss_val: 1.3100 acc_val: 0.7800 time: 0.7849s\n",
            "Epoch: 0098 loss_train: 1.0111 acc_train: 0.7800 loss_val: 1.3067 acc_val: 0.7800 time: 0.7932s\n",
            "Epoch: 0099 loss_train: 1.0075 acc_train: 0.7800 loss_val: 1.3037 acc_val: 0.7800 time: 0.8002s\n",
            "Epoch: 0100 loss_train: 0.9979 acc_train: 0.7800 loss_val: 1.3006 acc_val: 0.7800 time: 0.8084s\n",
            "Epoch: 0101 loss_train: 1.0286 acc_train: 0.7800 loss_val: 1.2975 acc_val: 0.7800 time: 0.8153s\n",
            "Epoch: 0102 loss_train: 0.9896 acc_train: 0.7800 loss_val: 1.2948 acc_val: 0.7800 time: 0.8232s\n",
            "Epoch: 0103 loss_train: 0.9969 acc_train: 0.7800 loss_val: 1.2921 acc_val: 0.7800 time: 0.8319s\n",
            "Epoch: 0104 loss_train: 0.9889 acc_train: 0.7800 loss_val: 1.2892 acc_val: 0.7800 time: 0.8427s\n",
            "Epoch: 0105 loss_train: 1.0069 acc_train: 0.7800 loss_val: 1.2860 acc_val: 0.7800 time: 0.8490s\n",
            "Epoch: 0106 loss_train: 0.9926 acc_train: 0.7800 loss_val: 1.2823 acc_val: 0.7800 time: 0.8590s\n",
            "Epoch: 0107 loss_train: 0.9995 acc_train: 0.7800 loss_val: 1.2783 acc_val: 0.7800 time: 0.8657s\n",
            "Epoch: 0108 loss_train: 0.9869 acc_train: 0.7800 loss_val: 1.2744 acc_val: 0.7800 time: 0.8725s\n",
            "Epoch: 0109 loss_train: 0.9776 acc_train: 0.7800 loss_val: 1.2710 acc_val: 0.7800 time: 0.8801s\n",
            "Epoch: 0110 loss_train: 0.9612 acc_train: 0.7800 loss_val: 1.2681 acc_val: 0.7800 time: 0.8869s\n",
            "Epoch: 0111 loss_train: 0.9634 acc_train: 0.7800 loss_val: 1.2659 acc_val: 0.7800 time: 0.8936s\n",
            "Epoch: 0112 loss_train: 0.9573 acc_train: 0.7800 loss_val: 1.2643 acc_val: 0.7800 time: 0.9010s\n",
            "Epoch: 0113 loss_train: 0.9684 acc_train: 0.7800 loss_val: 1.2625 acc_val: 0.7800 time: 0.9091s\n",
            "Epoch: 0114 loss_train: 0.9657 acc_train: 0.7800 loss_val: 1.2606 acc_val: 0.7800 time: 0.9158s\n",
            "Epoch: 0115 loss_train: 0.9473 acc_train: 0.7800 loss_val: 1.2591 acc_val: 0.7800 time: 0.9227s\n",
            "Epoch: 0116 loss_train: 0.9764 acc_train: 0.7800 loss_val: 1.2569 acc_val: 0.7800 time: 0.9314s\n",
            "Epoch: 0117 loss_train: 0.9608 acc_train: 0.7800 loss_val: 1.2546 acc_val: 0.7800 time: 0.9408s\n",
            "Epoch: 0118 loss_train: 0.9415 acc_train: 0.7800 loss_val: 1.2517 acc_val: 0.7800 time: 0.9476s\n",
            "Epoch: 0119 loss_train: 0.9339 acc_train: 0.7800 loss_val: 1.2488 acc_val: 0.7800 time: 0.9551s\n",
            "Epoch: 0120 loss_train: 0.9429 acc_train: 0.7800 loss_val: 1.2454 acc_val: 0.7800 time: 0.9633s\n",
            "Epoch: 0121 loss_train: 0.9757 acc_train: 0.7800 loss_val: 1.2420 acc_val: 0.7800 time: 0.9713s\n",
            "Epoch: 0122 loss_train: 0.9594 acc_train: 0.7800 loss_val: 1.2381 acc_val: 0.7800 time: 0.9797s\n",
            "Epoch: 0123 loss_train: 0.9738 acc_train: 0.7800 loss_val: 1.2345 acc_val: 0.7800 time: 0.9871s\n",
            "Epoch: 0124 loss_train: 0.9386 acc_train: 0.7800 loss_val: 1.2306 acc_val: 0.7800 time: 0.9936s\n",
            "Epoch: 0125 loss_train: 0.9087 acc_train: 0.8000 loss_val: 1.2272 acc_val: 0.7800 time: 0.9997s\n",
            "Epoch: 0126 loss_train: 0.9405 acc_train: 0.8000 loss_val: 1.2242 acc_val: 0.7800 time: 1.0058s\n",
            "Epoch: 0127 loss_train: 0.9106 acc_train: 0.8000 loss_val: 1.2217 acc_val: 0.7800 time: 1.0128s\n",
            "Epoch: 0128 loss_train: 0.9555 acc_train: 0.7800 loss_val: 1.2198 acc_val: 0.7800 time: 1.0191s\n",
            "Epoch: 0129 loss_train: 0.9592 acc_train: 0.7800 loss_val: 1.2180 acc_val: 0.7800 time: 1.0250s\n",
            "Epoch: 0130 loss_train: 0.8980 acc_train: 0.8000 loss_val: 1.2165 acc_val: 0.7800 time: 1.0313s\n",
            "Epoch: 0131 loss_train: 0.9493 acc_train: 0.7800 loss_val: 1.2143 acc_val: 0.7800 time: 1.0372s\n",
            "Epoch: 0132 loss_train: 0.8997 acc_train: 0.8000 loss_val: 1.2130 acc_val: 0.7800 time: 1.0775s\n",
            "Epoch: 0133 loss_train: 0.9404 acc_train: 0.7800 loss_val: 1.2109 acc_val: 0.7800 time: 1.0876s\n",
            "Epoch: 0134 loss_train: 0.9043 acc_train: 0.8000 loss_val: 1.2092 acc_val: 0.7800 time: 1.0969s\n",
            "Epoch: 0135 loss_train: 0.9326 acc_train: 0.7800 loss_val: 1.2076 acc_val: 0.7800 time: 1.1059s\n",
            "Epoch: 0136 loss_train: 0.8872 acc_train: 0.8000 loss_val: 1.2071 acc_val: 0.7800 time: 1.1140s\n",
            "Epoch: 0137 loss_train: 0.9165 acc_train: 0.7800 loss_val: 1.2063 acc_val: 0.7800 time: 1.1219s\n",
            "Epoch: 0138 loss_train: 0.9520 acc_train: 0.7800 loss_val: 1.2051 acc_val: 0.7800 time: 1.1292s\n",
            "Epoch: 0139 loss_train: 0.9111 acc_train: 0.8000 loss_val: 1.2034 acc_val: 0.7800 time: 1.1385s\n",
            "Epoch: 0140 loss_train: 0.9119 acc_train: 0.7800 loss_val: 1.2017 acc_val: 0.7800 time: 1.1475s\n",
            "Epoch: 0141 loss_train: 0.9276 acc_train: 0.7800 loss_val: 1.1997 acc_val: 0.7800 time: 1.1560s\n",
            "Epoch: 0142 loss_train: 0.9016 acc_train: 0.7800 loss_val: 1.1979 acc_val: 0.7800 time: 1.1656s\n",
            "Epoch: 0143 loss_train: 0.9033 acc_train: 0.7800 loss_val: 1.1963 acc_val: 0.7800 time: 1.1710s\n",
            "Epoch: 0144 loss_train: 0.9269 acc_train: 0.7800 loss_val: 1.1943 acc_val: 0.7800 time: 1.1771s\n",
            "Epoch: 0145 loss_train: 0.9020 acc_train: 0.8000 loss_val: 1.1929 acc_val: 0.7800 time: 1.1829s\n",
            "Epoch: 0146 loss_train: 0.8955 acc_train: 0.8000 loss_val: 1.1921 acc_val: 0.7800 time: 1.1900s\n",
            "Epoch: 0147 loss_train: 0.8685 acc_train: 0.8000 loss_val: 1.1923 acc_val: 0.7800 time: 1.1976s\n",
            "Epoch: 0148 loss_train: 0.9090 acc_train: 0.7800 loss_val: 1.1917 acc_val: 0.7800 time: 1.2049s\n",
            "Epoch: 0149 loss_train: 0.9312 acc_train: 0.7800 loss_val: 1.1903 acc_val: 0.7800 time: 1.2146s\n",
            "Epoch: 0150 loss_train: 0.9273 acc_train: 0.7800 loss_val: 1.1885 acc_val: 0.7800 time: 1.2236s\n",
            "Epoch: 0151 loss_train: 0.9301 acc_train: 0.7800 loss_val: 1.1860 acc_val: 0.7800 time: 1.2314s\n",
            "Epoch: 0152 loss_train: 0.9087 acc_train: 0.7800 loss_val: 1.1827 acc_val: 0.7800 time: 1.2391s\n",
            "Epoch: 0153 loss_train: 0.8820 acc_train: 0.8000 loss_val: 1.1797 acc_val: 0.7800 time: 1.2462s\n",
            "Epoch: 0154 loss_train: 0.8943 acc_train: 0.7800 loss_val: 1.1756 acc_val: 0.7800 time: 1.2549s\n",
            "Epoch: 0155 loss_train: 0.9092 acc_train: 0.7800 loss_val: 1.1750 acc_val: 0.7800 time: 1.2631s\n",
            "Epoch: 0156 loss_train: 0.9187 acc_train: 0.7800 loss_val: 1.1741 acc_val: 0.7800 time: 1.2708s\n",
            "Epoch: 0157 loss_train: 0.9179 acc_train: 0.7800 loss_val: 1.1730 acc_val: 0.7800 time: 1.2786s\n",
            "Epoch: 0158 loss_train: 0.9073 acc_train: 0.7800 loss_val: 1.1718 acc_val: 0.7800 time: 1.2888s\n",
            "Epoch: 0159 loss_train: 0.9102 acc_train: 0.7800 loss_val: 1.1712 acc_val: 0.7800 time: 1.3001s\n",
            "Epoch: 0160 loss_train: 0.9135 acc_train: 0.7800 loss_val: 1.1696 acc_val: 0.7800 time: 1.3087s\n",
            "Epoch: 0161 loss_train: 0.8941 acc_train: 0.7800 loss_val: 1.1680 acc_val: 0.7800 time: 1.3158s\n",
            "Epoch: 0162 loss_train: 0.8619 acc_train: 0.8000 loss_val: 1.1662 acc_val: 0.7800 time: 1.3235s\n",
            "Epoch: 0163 loss_train: 0.8597 acc_train: 0.7800 loss_val: 1.1645 acc_val: 0.7800 time: 1.3416s\n",
            "Epoch: 0164 loss_train: 0.8865 acc_train: 0.7800 loss_val: 1.1631 acc_val: 0.7800 time: 1.3502s\n",
            "Epoch: 0165 loss_train: 0.8751 acc_train: 0.7800 loss_val: 1.1618 acc_val: 0.7800 time: 1.3598s\n",
            "Epoch: 0166 loss_train: 0.9144 acc_train: 0.7800 loss_val: 1.1606 acc_val: 0.7800 time: 1.3675s\n",
            "Epoch: 0167 loss_train: 0.8702 acc_train: 0.7800 loss_val: 1.1595 acc_val: 0.7800 time: 1.3760s\n",
            "Epoch: 0168 loss_train: 0.8612 acc_train: 0.8000 loss_val: 1.1586 acc_val: 0.7800 time: 1.3864s\n",
            "Epoch: 0169 loss_train: 0.8653 acc_train: 0.8000 loss_val: 1.1582 acc_val: 0.7800 time: 1.3948s\n",
            "Epoch: 0170 loss_train: 0.8531 acc_train: 0.8000 loss_val: 1.1579 acc_val: 0.7800 time: 1.4019s\n",
            "Epoch: 0171 loss_train: 0.8761 acc_train: 0.8000 loss_val: 1.1577 acc_val: 0.7800 time: 1.4104s\n",
            "Epoch: 0172 loss_train: 0.8920 acc_train: 0.7800 loss_val: 1.1584 acc_val: 0.7800 time: 1.4179s\n",
            "Epoch: 0173 loss_train: 0.9029 acc_train: 0.7800 loss_val: 1.1585 acc_val: 0.7800 time: 1.4257s\n",
            "Epoch: 0174 loss_train: 0.8526 acc_train: 0.8000 loss_val: 1.1584 acc_val: 0.7800 time: 1.4348s\n",
            "Epoch: 0175 loss_train: 0.8791 acc_train: 0.7800 loss_val: 1.1594 acc_val: 0.7800 time: 1.4428s\n",
            "Epoch: 0176 loss_train: 0.8513 acc_train: 0.8000 loss_val: 1.1604 acc_val: 0.7800 time: 1.4510s\n",
            "Epoch: 0177 loss_train: 0.8890 acc_train: 0.7800 loss_val: 1.1623 acc_val: 0.7800 time: 1.4588s\n",
            "Epoch: 0178 loss_train: 0.8958 acc_train: 0.7800 loss_val: 1.1644 acc_val: 0.7800 time: 1.4668s\n",
            "Epoch: 0179 loss_train: 0.8355 acc_train: 0.8000 loss_val: 1.1657 acc_val: 0.7800 time: 1.4753s\n",
            "Epoch: 0180 loss_train: 0.8520 acc_train: 0.8000 loss_val: 1.1671 acc_val: 0.7800 time: 1.4840s\n",
            "Epoch: 0181 loss_train: 0.8798 acc_train: 0.8000 loss_val: 1.1672 acc_val: 0.7800 time: 1.4928s\n",
            "Epoch: 0182 loss_train: 0.8965 acc_train: 0.7800 loss_val: 1.1659 acc_val: 0.7800 time: 1.5060s\n",
            "Epoch: 0183 loss_train: 0.8868 acc_train: 0.7800 loss_val: 1.1621 acc_val: 0.7800 time: 1.5136s\n",
            "Epoch: 0184 loss_train: 0.8744 acc_train: 0.8000 loss_val: 1.1579 acc_val: 0.7800 time: 1.5212s\n",
            "Epoch: 0185 loss_train: 0.8593 acc_train: 0.8000 loss_val: 1.1541 acc_val: 0.7800 time: 1.5283s\n",
            "Epoch: 0186 loss_train: 0.8443 acc_train: 0.8000 loss_val: 1.1523 acc_val: 0.7800 time: 1.5352s\n",
            "Epoch: 0187 loss_train: 0.8390 acc_train: 0.8000 loss_val: 1.1500 acc_val: 0.7800 time: 1.5429s\n",
            "Epoch: 0188 loss_train: 0.8907 acc_train: 0.7800 loss_val: 1.1476 acc_val: 0.7800 time: 1.5497s\n",
            "Epoch: 0189 loss_train: 0.8837 acc_train: 0.7800 loss_val: 1.1448 acc_val: 0.7800 time: 1.5577s\n",
            "Epoch: 0190 loss_train: 0.8554 acc_train: 0.8000 loss_val: 1.1427 acc_val: 0.7800 time: 1.5652s\n",
            "Epoch: 0191 loss_train: 0.8650 acc_train: 0.7800 loss_val: 1.1402 acc_val: 0.7800 time: 1.5719s\n",
            "Epoch: 0192 loss_train: 0.8476 acc_train: 0.8000 loss_val: 1.1391 acc_val: 0.7800 time: 1.5801s\n",
            "Epoch: 0193 loss_train: 0.8790 acc_train: 0.7800 loss_val: 1.1378 acc_val: 0.7800 time: 1.5887s\n",
            "Epoch: 0194 loss_train: 0.8300 acc_train: 0.8000 loss_val: 1.1362 acc_val: 0.7800 time: 1.5955s\n",
            "Epoch: 0195 loss_train: 0.8399 acc_train: 0.8000 loss_val: 1.1341 acc_val: 0.7800 time: 1.6044s\n",
            "Epoch: 0196 loss_train: 0.8810 acc_train: 0.7800 loss_val: 1.1314 acc_val: 0.7800 time: 1.6121s\n",
            "Epoch: 0197 loss_train: 0.8302 acc_train: 0.8000 loss_val: 1.1303 acc_val: 0.7800 time: 1.6192s\n",
            "Epoch: 0198 loss_train: 0.8163 acc_train: 0.8000 loss_val: 1.1292 acc_val: 0.7800 time: 1.6262s\n",
            "Epoch: 0199 loss_train: 0.8735 acc_train: 0.7800 loss_val: 1.1286 acc_val: 0.7800 time: 1.6334s\n",
            "Epoch: 0200 loss_train: 0.8687 acc_train: 0.7800 loss_val: 1.1294 acc_val: 0.7800 time: 1.6407s\n",
            "Epoch: 0201 loss_train: 0.8752 acc_train: 0.7800 loss_val: 1.1304 acc_val: 0.7800 time: 1.6471s\n",
            "Epoch: 0202 loss_train: 0.8388 acc_train: 0.8000 loss_val: 1.1307 acc_val: 0.7800 time: 1.6541s\n",
            "Epoch: 0203 loss_train: 0.8221 acc_train: 0.8000 loss_val: 1.1314 acc_val: 0.7800 time: 1.6619s\n",
            "Epoch: 0204 loss_train: 0.8190 acc_train: 0.8000 loss_val: 1.1342 acc_val: 0.7800 time: 1.6698s\n",
            "Epoch: 0205 loss_train: 0.8663 acc_train: 0.7800 loss_val: 1.1375 acc_val: 0.7800 time: 1.6775s\n",
            "Epoch: 0206 loss_train: 0.8730 acc_train: 0.7800 loss_val: 1.1388 acc_val: 0.7800 time: 1.6847s\n",
            "Epoch: 0207 loss_train: 0.8182 acc_train: 0.8000 loss_val: 1.1391 acc_val: 0.7800 time: 1.6924s\n",
            "Epoch: 0208 loss_train: 0.8720 acc_train: 0.7800 loss_val: 1.1379 acc_val: 0.7800 time: 1.6997s\n",
            "Epoch: 0209 loss_train: 0.8410 acc_train: 0.7800 loss_val: 1.1353 acc_val: 0.7800 time: 1.7124s\n",
            "Epoch: 0210 loss_train: 0.8239 acc_train: 0.8000 loss_val: 1.1322 acc_val: 0.7800 time: 1.7199s\n",
            "Epoch: 0211 loss_train: 0.8911 acc_train: 0.7800 loss_val: 1.1277 acc_val: 0.7800 time: 1.7264s\n",
            "Epoch: 0212 loss_train: 0.8764 acc_train: 0.7800 loss_val: 1.1249 acc_val: 0.7800 time: 1.7340s\n",
            "Epoch: 0213 loss_train: 0.8878 acc_train: 0.7800 loss_val: 1.1258 acc_val: 0.7800 time: 1.7413s\n",
            "Epoch: 0214 loss_train: 0.8133 acc_train: 0.8000 loss_val: 1.1283 acc_val: 0.7800 time: 1.7485s\n",
            "Epoch: 0215 loss_train: 0.8422 acc_train: 0.7800 loss_val: 1.1313 acc_val: 0.7800 time: 1.7569s\n",
            "Epoch: 0216 loss_train: 0.8254 acc_train: 0.8000 loss_val: 1.1331 acc_val: 0.7800 time: 1.7687s\n",
            "Epoch: 0217 loss_train: 0.8586 acc_train: 0.7800 loss_val: 1.1343 acc_val: 0.7800 time: 1.7770s\n",
            "Epoch: 0218 loss_train: 0.8325 acc_train: 0.7800 loss_val: 1.1360 acc_val: 0.7800 time: 1.7841s\n",
            "Epoch: 0219 loss_train: 0.8956 acc_train: 0.7800 loss_val: 1.1367 acc_val: 0.7800 time: 1.7932s\n",
            "Epoch: 0220 loss_train: 0.8445 acc_train: 0.7800 loss_val: 1.1379 acc_val: 0.7800 time: 1.8022s\n",
            "Epoch: 0221 loss_train: 0.8739 acc_train: 0.7800 loss_val: 1.1390 acc_val: 0.7800 time: 1.8129s\n",
            "Epoch: 0222 loss_train: 0.8571 acc_train: 0.7800 loss_val: 1.1384 acc_val: 0.7800 time: 1.8208s\n",
            "Epoch: 0223 loss_train: 0.8660 acc_train: 0.7800 loss_val: 1.1369 acc_val: 0.7800 time: 1.8282s\n",
            "Epoch: 0224 loss_train: 0.8452 acc_train: 0.7800 loss_val: 1.1337 acc_val: 0.7800 time: 1.8368s\n",
            "Epoch: 0225 loss_train: 0.8313 acc_train: 0.8000 loss_val: 1.1304 acc_val: 0.7800 time: 1.8446s\n",
            "Epoch: 0226 loss_train: 0.8517 acc_train: 0.7800 loss_val: 1.1270 acc_val: 0.7800 time: 1.8515s\n",
            "Epoch: 0227 loss_train: 0.8384 acc_train: 0.8000 loss_val: 1.1223 acc_val: 0.7800 time: 1.8637s\n",
            "Epoch: 0228 loss_train: 0.8613 acc_train: 0.7800 loss_val: 1.1184 acc_val: 0.7800 time: 1.8707s\n",
            "Epoch: 0229 loss_train: 0.8103 acc_train: 0.8000 loss_val: 1.1155 acc_val: 0.7800 time: 1.8785s\n",
            "Epoch: 0230 loss_train: 0.8414 acc_train: 0.8000 loss_val: 1.1127 acc_val: 0.7800 time: 1.8864s\n",
            "Epoch: 0231 loss_train: 0.8310 acc_train: 0.8000 loss_val: 1.1104 acc_val: 0.7800 time: 1.8942s\n",
            "Epoch: 0232 loss_train: 0.8212 acc_train: 0.8000 loss_val: 1.1086 acc_val: 0.7800 time: 1.9017s\n",
            "Epoch: 0233 loss_train: 0.8822 acc_train: 0.7800 loss_val: 1.1068 acc_val: 0.7800 time: 1.9198s\n",
            "Epoch: 0234 loss_train: 0.8650 acc_train: 0.7800 loss_val: 1.1073 acc_val: 0.7800 time: 1.9272s\n",
            "Epoch: 0235 loss_train: 0.8098 acc_train: 0.8000 loss_val: 1.1089 acc_val: 0.7800 time: 1.9350s\n",
            "Epoch: 0236 loss_train: 0.8500 acc_train: 0.7800 loss_val: 1.1115 acc_val: 0.7800 time: 1.9431s\n",
            "Epoch: 0237 loss_train: 0.8322 acc_train: 0.7800 loss_val: 1.1156 acc_val: 0.7800 time: 1.9563s\n",
            "Epoch: 0238 loss_train: 0.8121 acc_train: 0.8000 loss_val: 1.1194 acc_val: 0.7800 time: 1.9664s\n",
            "Epoch: 0239 loss_train: 0.8398 acc_train: 0.7800 loss_val: 1.1216 acc_val: 0.7800 time: 1.9739s\n",
            "Epoch: 0240 loss_train: 0.8215 acc_train: 0.8000 loss_val: 1.1237 acc_val: 0.7800 time: 1.9821s\n",
            "Epoch: 0241 loss_train: 0.8036 acc_train: 0.8000 loss_val: 1.1256 acc_val: 0.7800 time: 1.9903s\n",
            "Epoch: 0242 loss_train: 0.8832 acc_train: 0.7800 loss_val: 1.1269 acc_val: 0.7800 time: 1.9981s\n",
            "Epoch: 0243 loss_train: 0.8275 acc_train: 0.8000 loss_val: 1.1261 acc_val: 0.7800 time: 2.0096s\n",
            "Epoch: 0244 loss_train: 0.8022 acc_train: 0.8000 loss_val: 1.1260 acc_val: 0.7800 time: 2.0166s\n",
            "Epoch: 0245 loss_train: 0.8073 acc_train: 0.8000 loss_val: 1.1254 acc_val: 0.7800 time: 2.0250s\n",
            "Epoch: 0246 loss_train: 0.7763 acc_train: 0.8000 loss_val: 1.1255 acc_val: 0.7800 time: 2.0331s\n",
            "Epoch: 0247 loss_train: 0.8064 acc_train: 0.8000 loss_val: 1.1251 acc_val: 0.7800 time: 2.0411s\n",
            "Epoch: 0248 loss_train: 0.8366 acc_train: 0.7800 loss_val: 1.1239 acc_val: 0.7800 time: 2.0495s\n",
            "Epoch: 0249 loss_train: 0.9010 acc_train: 0.7800 loss_val: 1.1209 acc_val: 0.7800 time: 2.0620s\n",
            "Epoch: 0250 loss_train: 0.8414 acc_train: 0.7800 loss_val: 1.1188 acc_val: 0.7800 time: 2.0733s\n",
            "Epoch: 0251 loss_train: 0.8088 acc_train: 0.8000 loss_val: 1.1191 acc_val: 0.7800 time: 2.0911s\n",
            "Epoch: 0252 loss_train: 0.8478 acc_train: 0.7800 loss_val: 1.1192 acc_val: 0.7800 time: 2.1002s\n",
            "Epoch: 0253 loss_train: 0.7977 acc_train: 0.8000 loss_val: 1.1189 acc_val: 0.7800 time: 2.1130s\n",
            "Epoch: 0254 loss_train: 0.8495 acc_train: 0.7800 loss_val: 1.1178 acc_val: 0.7800 time: 2.1186s\n",
            "Epoch: 0255 loss_train: 0.8128 acc_train: 0.8000 loss_val: 1.1164 acc_val: 0.7800 time: 2.1279s\n",
            "Epoch: 0256 loss_train: 0.8655 acc_train: 0.7800 loss_val: 1.1143 acc_val: 0.7800 time: 2.1393s\n",
            "Epoch: 0257 loss_train: 0.7721 acc_train: 0.8000 loss_val: 1.1122 acc_val: 0.7800 time: 2.1517s\n",
            "Epoch: 0258 loss_train: 0.8404 acc_train: 0.7800 loss_val: 1.1103 acc_val: 0.7800 time: 2.1633s\n",
            "Epoch: 0259 loss_train: 0.8685 acc_train: 0.7800 loss_val: 1.1086 acc_val: 0.7800 time: 2.1715s\n",
            "Epoch: 0260 loss_train: 0.8056 acc_train: 0.8000 loss_val: 1.1075 acc_val: 0.7800 time: 2.1821s\n",
            "Epoch: 0261 loss_train: 0.7934 acc_train: 0.8000 loss_val: 1.1083 acc_val: 0.7800 time: 2.1912s\n",
            "Epoch: 0262 loss_train: 0.8653 acc_train: 0.7800 loss_val: 1.1108 acc_val: 0.7800 time: 2.2032s\n",
            "Epoch: 0263 loss_train: 0.7951 acc_train: 0.8000 loss_val: 1.1124 acc_val: 0.7800 time: 2.2207s\n",
            "Epoch: 0264 loss_train: 0.8220 acc_train: 0.7800 loss_val: 1.1135 acc_val: 0.7800 time: 2.2322s\n",
            "Epoch: 0265 loss_train: 0.8595 acc_train: 0.7800 loss_val: 1.1146 acc_val: 0.7800 time: 2.2414s\n",
            "Epoch: 0266 loss_train: 0.8037 acc_train: 0.8000 loss_val: 1.1146 acc_val: 0.7800 time: 2.2526s\n",
            "Epoch: 0267 loss_train: 0.7958 acc_train: 0.8000 loss_val: 1.1139 acc_val: 0.7800 time: 2.2580s\n",
            "Epoch: 0268 loss_train: 0.7885 acc_train: 0.8000 loss_val: 1.1125 acc_val: 0.7800 time: 2.2629s\n",
            "Epoch: 0269 loss_train: 0.8383 acc_train: 0.7800 loss_val: 1.1113 acc_val: 0.7800 time: 2.2678s\n",
            "Epoch: 0270 loss_train: 0.7768 acc_train: 0.8000 loss_val: 1.1104 acc_val: 0.7800 time: 2.2728s\n",
            "Epoch: 0271 loss_train: 0.7922 acc_train: 0.8000 loss_val: 1.1091 acc_val: 0.7800 time: 2.2788s\n",
            "Epoch: 0272 loss_train: 0.7979 acc_train: 0.8000 loss_val: 1.1076 acc_val: 0.7800 time: 2.2865s\n",
            "Epoch: 0273 loss_train: 0.7899 acc_train: 0.8000 loss_val: 1.1072 acc_val: 0.7800 time: 2.2985s\n",
            "Epoch: 0274 loss_train: 0.8327 acc_train: 0.7800 loss_val: 1.1079 acc_val: 0.7800 time: 2.3057s\n",
            "Epoch: 0275 loss_train: 0.8032 acc_train: 0.8000 loss_val: 1.1093 acc_val: 0.7800 time: 2.3135s\n",
            "Epoch: 0276 loss_train: 0.8054 acc_train: 0.8000 loss_val: 1.1112 acc_val: 0.7800 time: 2.3207s\n",
            "Epoch: 0277 loss_train: 0.7822 acc_train: 0.8000 loss_val: 1.1135 acc_val: 0.7800 time: 2.3281s\n",
            "Epoch: 0278 loss_train: 0.8233 acc_train: 0.7800 loss_val: 1.1166 acc_val: 0.7800 time: 2.3513s\n",
            "Epoch: 0279 loss_train: 0.8535 acc_train: 0.7800 loss_val: 1.1191 acc_val: 0.7800 time: 2.3587s\n",
            "Epoch: 0280 loss_train: 0.8434 acc_train: 0.7800 loss_val: 1.1204 acc_val: 0.7800 time: 2.3651s\n",
            "Epoch: 0281 loss_train: 0.7933 acc_train: 0.7800 loss_val: 1.1208 acc_val: 0.7800 time: 2.3717s\n",
            "Epoch: 0282 loss_train: 0.8076 acc_train: 0.8000 loss_val: 1.1203 acc_val: 0.7800 time: 2.3825s\n",
            "Epoch: 0283 loss_train: 0.8036 acc_train: 0.8000 loss_val: 1.1194 acc_val: 0.7800 time: 2.3992s\n",
            "Epoch: 0284 loss_train: 0.7971 acc_train: 0.8000 loss_val: 1.1184 acc_val: 0.7800 time: 2.4075s\n",
            "Epoch: 0285 loss_train: 0.8015 acc_train: 0.8000 loss_val: 1.1158 acc_val: 0.7800 time: 2.4150s\n",
            "Epoch: 0286 loss_train: 0.8669 acc_train: 0.7800 loss_val: 1.1116 acc_val: 0.7800 time: 2.4222s\n",
            "Epoch: 0287 loss_train: 0.7891 acc_train: 0.8000 loss_val: 1.1086 acc_val: 0.7800 time: 2.4291s\n",
            "Epoch: 0288 loss_train: 0.7794 acc_train: 0.8000 loss_val: 1.1066 acc_val: 0.7800 time: 2.4377s\n",
            "Epoch: 0289 loss_train: 0.8043 acc_train: 0.7800 loss_val: 1.1038 acc_val: 0.7800 time: 2.4512s\n",
            "Epoch: 0290 loss_train: 0.8462 acc_train: 0.7800 loss_val: 1.1010 acc_val: 0.7800 time: 2.4584s\n",
            "Epoch: 0291 loss_train: 0.7969 acc_train: 0.8000 loss_val: 1.0991 acc_val: 0.7800 time: 2.4653s\n",
            "Epoch: 0292 loss_train: 0.8226 acc_train: 0.7800 loss_val: 1.0971 acc_val: 0.7800 time: 2.4732s\n",
            "Epoch: 0293 loss_train: 0.7965 acc_train: 0.8000 loss_val: 1.0966 acc_val: 0.7800 time: 2.4894s\n",
            "Epoch: 0294 loss_train: 0.8400 acc_train: 0.7800 loss_val: 1.0971 acc_val: 0.7800 time: 2.5001s\n",
            "Epoch: 0295 loss_train: 0.7723 acc_train: 0.8000 loss_val: 1.0974 acc_val: 0.7800 time: 2.5075s\n",
            "Epoch: 0296 loss_train: 0.8447 acc_train: 0.7800 loss_val: 1.0992 acc_val: 0.7800 time: 2.5166s\n",
            "Epoch: 0297 loss_train: 0.7976 acc_train: 0.7800 loss_val: 1.1020 acc_val: 0.7800 time: 2.5248s\n",
            "Epoch: 0298 loss_train: 0.7429 acc_train: 0.8000 loss_val: 1.1049 acc_val: 0.7800 time: 2.5414s\n",
            "Epoch: 0299 loss_train: 0.7788 acc_train: 0.8000 loss_val: 1.1074 acc_val: 0.7800 time: 2.5494s\n",
            "Epoch: 0300 loss_train: 0.8237 acc_train: 0.7800 loss_val: 1.1108 acc_val: 0.7800 time: 2.5598s\n",
            "Epoch: 0301 loss_train: 0.7947 acc_train: 0.7800 loss_val: 1.1141 acc_val: 0.7800 time: 2.5734s\n",
            "Epoch: 0302 loss_train: 0.8225 acc_train: 0.7800 loss_val: 1.1155 acc_val: 0.7800 time: 2.5836s\n",
            "Epoch: 0303 loss_train: 0.7731 acc_train: 0.8000 loss_val: 1.1160 acc_val: 0.7800 time: 2.5970s\n",
            "Epoch: 0304 loss_train: 0.8261 acc_train: 0.7800 loss_val: 1.1157 acc_val: 0.7800 time: 2.6045s\n",
            "Epoch: 0305 loss_train: 0.8051 acc_train: 0.7800 loss_val: 1.1146 acc_val: 0.7800 time: 2.6119s\n",
            "Epoch: 0306 loss_train: 0.7430 acc_train: 0.8000 loss_val: 1.1134 acc_val: 0.7800 time: 2.6274s\n",
            "Epoch: 0307 loss_train: 0.7042 acc_train: 0.8000 loss_val: 1.1159 acc_val: 0.7800 time: 2.6366s\n",
            "Epoch: 0308 loss_train: 0.8125 acc_train: 0.7800 loss_val: 1.1171 acc_val: 0.7800 time: 2.6440s\n",
            "Epoch: 0309 loss_train: 0.8135 acc_train: 0.7800 loss_val: 1.1205 acc_val: 0.7800 time: 2.6511s\n",
            "Epoch: 0310 loss_train: 0.7715 acc_train: 0.8000 loss_val: 1.1246 acc_val: 0.7800 time: 2.6581s\n",
            "Epoch: 0311 loss_train: 0.7862 acc_train: 0.7800 loss_val: 1.1269 acc_val: 0.7800 time: 2.6727s\n",
            "Epoch: 0312 loss_train: 0.7756 acc_train: 0.8000 loss_val: 1.1274 acc_val: 0.7800 time: 2.6820s\n",
            "Epoch: 0313 loss_train: 0.8122 acc_train: 0.7800 loss_val: 1.1258 acc_val: 0.7800 time: 2.6897s\n",
            "Epoch: 0314 loss_train: 0.8248 acc_train: 0.7800 loss_val: 1.1225 acc_val: 0.7800 time: 2.6990s\n",
            "Epoch: 0315 loss_train: 0.7843 acc_train: 0.8000 loss_val: 1.1180 acc_val: 0.7800 time: 2.7060s\n",
            "Epoch: 0316 loss_train: 0.8536 acc_train: 0.7800 loss_val: 1.1128 acc_val: 0.7800 time: 2.7133s\n",
            "Epoch: 0317 loss_train: 0.7648 acc_train: 0.8000 loss_val: 1.1091 acc_val: 0.7800 time: 2.7216s\n",
            "Epoch: 0318 loss_train: 0.8425 acc_train: 0.7800 loss_val: 1.1058 acc_val: 0.7800 time: 2.7323s\n",
            "Epoch: 0319 loss_train: 0.7524 acc_train: 0.8000 loss_val: 1.1041 acc_val: 0.7800 time: 2.7418s\n",
            "Epoch: 0320 loss_train: 0.7323 acc_train: 0.8000 loss_val: 1.1032 acc_val: 0.7800 time: 2.7498s\n",
            "Epoch: 0321 loss_train: 0.7929 acc_train: 0.7800 loss_val: 1.1035 acc_val: 0.7800 time: 2.7581s\n",
            "Epoch: 0322 loss_train: 0.7833 acc_train: 0.8000 loss_val: 1.1041 acc_val: 0.7800 time: 2.7775s\n",
            "Epoch: 0323 loss_train: 0.8395 acc_train: 0.7800 loss_val: 1.1039 acc_val: 0.7800 time: 2.7867s\n",
            "Epoch: 0324 loss_train: 0.7668 acc_train: 0.8000 loss_val: 1.1037 acc_val: 0.7800 time: 2.7923s\n",
            "Epoch: 0325 loss_train: 0.8404 acc_train: 0.7800 loss_val: 1.1050 acc_val: 0.7800 time: 2.7981s\n",
            "Epoch: 0326 loss_train: 0.7091 acc_train: 0.8000 loss_val: 1.1067 acc_val: 0.7800 time: 2.8056s\n",
            "Epoch: 0327 loss_train: 0.7736 acc_train: 0.8000 loss_val: 1.1096 acc_val: 0.7800 time: 2.8151s\n",
            "Epoch: 0328 loss_train: 0.7885 acc_train: 0.7800 loss_val: 1.1122 acc_val: 0.7800 time: 2.8224s\n",
            "Epoch: 0329 loss_train: 0.7603 acc_train: 0.8000 loss_val: 1.1147 acc_val: 0.7800 time: 2.8299s\n",
            "Epoch: 0330 loss_train: 0.8021 acc_train: 0.7800 loss_val: 1.1177 acc_val: 0.7800 time: 2.8370s\n",
            "Epoch: 0331 loss_train: 0.7895 acc_train: 0.7800 loss_val: 1.1209 acc_val: 0.7800 time: 2.8437s\n",
            "Epoch: 0332 loss_train: 0.7268 acc_train: 0.8200 loss_val: 1.1231 acc_val: 0.7800 time: 2.8512s\n",
            "Epoch: 0333 loss_train: 0.7420 acc_train: 0.8200 loss_val: 1.1232 acc_val: 0.7800 time: 2.8603s\n",
            "Epoch: 0334 loss_train: 0.7755 acc_train: 0.8000 loss_val: 1.1211 acc_val: 0.7800 time: 2.8679s\n",
            "Epoch: 0335 loss_train: 0.8591 acc_train: 0.7800 loss_val: 1.1160 acc_val: 0.7800 time: 2.8750s\n",
            "Epoch: 0336 loss_train: 0.7927 acc_train: 0.8000 loss_val: 1.1106 acc_val: 0.7800 time: 2.8815s\n",
            "Epoch: 0337 loss_train: 0.7856 acc_train: 0.8000 loss_val: 1.1063 acc_val: 0.7800 time: 2.8880s\n",
            "Epoch: 0338 loss_train: 0.7782 acc_train: 0.8000 loss_val: 1.1035 acc_val: 0.7800 time: 2.8945s\n",
            "Epoch: 0339 loss_train: 0.7809 acc_train: 0.8200 loss_val: 1.1017 acc_val: 0.7800 time: 2.9012s\n",
            "Epoch: 0340 loss_train: 0.8031 acc_train: 0.7800 loss_val: 1.1014 acc_val: 0.7800 time: 2.9076s\n",
            "Epoch: 0341 loss_train: 0.8047 acc_train: 0.7800 loss_val: 1.1026 acc_val: 0.7800 time: 2.9142s\n",
            "Epoch: 0342 loss_train: 0.7682 acc_train: 0.8000 loss_val: 1.1050 acc_val: 0.7800 time: 2.9247s\n",
            "Epoch: 0343 loss_train: 0.8133 acc_train: 0.7800 loss_val: 1.1101 acc_val: 0.7800 time: 2.9351s\n",
            "Epoch: 0344 loss_train: 0.7307 acc_train: 0.8200 loss_val: 1.1156 acc_val: 0.7800 time: 2.9426s\n",
            "Epoch: 0345 loss_train: 0.7887 acc_train: 0.7800 loss_val: 1.1196 acc_val: 0.7800 time: 2.9495s\n",
            "Epoch: 0346 loss_train: 0.7968 acc_train: 0.8000 loss_val: 1.1231 acc_val: 0.7800 time: 2.9578s\n",
            "Epoch: 0347 loss_train: 0.7844 acc_train: 0.7800 loss_val: 1.1267 acc_val: 0.7800 time: 2.9660s\n",
            "Epoch: 0348 loss_train: 0.7539 acc_train: 0.7800 loss_val: 1.1313 acc_val: 0.7800 time: 2.9752s\n",
            "Epoch: 0349 loss_train: 0.7534 acc_train: 0.8000 loss_val: 1.1358 acc_val: 0.7800 time: 2.9858s\n",
            "Epoch: 0350 loss_train: 0.7534 acc_train: 0.8000 loss_val: 1.1378 acc_val: 0.7800 time: 2.9917s\n",
            "Epoch: 0351 loss_train: 0.7271 acc_train: 0.8000 loss_val: 1.1379 acc_val: 0.7800 time: 3.0037s\n",
            "Epoch: 0352 loss_train: 0.8406 acc_train: 0.7800 loss_val: 1.1390 acc_val: 0.7800 time: 3.0114s\n",
            "Epoch: 0353 loss_train: 0.8274 acc_train: 0.7800 loss_val: 1.1395 acc_val: 0.7800 time: 3.0188s\n",
            "Epoch: 0354 loss_train: 0.7778 acc_train: 0.7800 loss_val: 1.1384 acc_val: 0.7800 time: 3.0257s\n",
            "Epoch: 0355 loss_train: 0.7915 acc_train: 0.7800 loss_val: 1.1364 acc_val: 0.7800 time: 3.0389s\n",
            "Epoch: 0356 loss_train: 0.7484 acc_train: 0.8000 loss_val: 1.1332 acc_val: 0.7800 time: 3.0466s\n",
            "Epoch: 0357 loss_train: 0.7506 acc_train: 0.7800 loss_val: 1.1276 acc_val: 0.7800 time: 3.0548s\n",
            "Epoch: 0358 loss_train: 0.7957 acc_train: 0.7800 loss_val: 1.1230 acc_val: 0.7800 time: 3.0621s\n",
            "Epoch: 0359 loss_train: 0.7506 acc_train: 0.8200 loss_val: 1.1185 acc_val: 0.7800 time: 3.0692s\n",
            "Epoch: 0360 loss_train: 0.7679 acc_train: 0.8000 loss_val: 1.1132 acc_val: 0.7800 time: 3.0777s\n",
            "Epoch: 0361 loss_train: 0.7253 acc_train: 0.8000 loss_val: 1.1085 acc_val: 0.7800 time: 3.0850s\n",
            "Epoch: 0362 loss_train: 0.7644 acc_train: 0.7800 loss_val: 1.1038 acc_val: 0.7800 time: 3.0932s\n",
            "Epoch: 0363 loss_train: 0.7244 acc_train: 0.8200 loss_val: 1.1013 acc_val: 0.7800 time: 3.1023s\n",
            "Epoch: 0364 loss_train: 0.7733 acc_train: 0.8000 loss_val: 1.1046 acc_val: 0.7800 time: 3.1127s\n",
            "Epoch: 0365 loss_train: 0.8042 acc_train: 0.8000 loss_val: 1.1072 acc_val: 0.7800 time: 3.1184s\n",
            "Epoch: 0366 loss_train: 0.7716 acc_train: 0.8000 loss_val: 1.1096 acc_val: 0.7800 time: 3.1240s\n",
            "Epoch: 0367 loss_train: 0.7190 acc_train: 0.8200 loss_val: 1.1117 acc_val: 0.7800 time: 3.1317s\n",
            "Epoch: 0368 loss_train: 0.6909 acc_train: 0.8200 loss_val: 1.1119 acc_val: 0.7800 time: 3.1396s\n",
            "Epoch: 0369 loss_train: 0.7285 acc_train: 0.8200 loss_val: 1.1108 acc_val: 0.7800 time: 3.1477s\n",
            "Epoch: 0370 loss_train: 0.7617 acc_train: 0.7800 loss_val: 1.1099 acc_val: 0.7800 time: 3.1546s\n",
            "Epoch: 0371 loss_train: 0.7268 acc_train: 0.8000 loss_val: 1.1092 acc_val: 0.7800 time: 3.1623s\n",
            "Epoch: 0372 loss_train: 0.6827 acc_train: 0.8000 loss_val: 1.1091 acc_val: 0.7800 time: 3.1691s\n",
            "Epoch: 0373 loss_train: 0.7398 acc_train: 0.8000 loss_val: 1.1093 acc_val: 0.7800 time: 3.1757s\n",
            "Epoch: 0374 loss_train: 0.7462 acc_train: 0.8200 loss_val: 1.1086 acc_val: 0.7800 time: 3.1824s\n",
            "Epoch: 0375 loss_train: 0.7274 acc_train: 0.7800 loss_val: 1.1093 acc_val: 0.7800 time: 3.1912s\n",
            "Epoch: 0376 loss_train: 0.7910 acc_train: 0.8000 loss_val: 1.1097 acc_val: 0.7800 time: 3.2058s\n",
            "Epoch: 0377 loss_train: 0.7198 acc_train: 0.8000 loss_val: 1.1104 acc_val: 0.7800 time: 3.2131s\n",
            "Epoch: 0378 loss_train: 0.7429 acc_train: 0.8200 loss_val: 1.1106 acc_val: 0.7800 time: 3.2199s\n",
            "Epoch: 0379 loss_train: 0.8192 acc_train: 0.7800 loss_val: 1.1102 acc_val: 0.7800 time: 3.2282s\n",
            "Epoch: 0380 loss_train: 0.7320 acc_train: 0.8200 loss_val: 1.1105 acc_val: 0.7800 time: 3.2356s\n",
            "Epoch: 0381 loss_train: 0.7186 acc_train: 0.8000 loss_val: 1.1110 acc_val: 0.7800 time: 3.2431s\n",
            "Epoch: 0382 loss_train: 0.6907 acc_train: 0.8200 loss_val: 1.1108 acc_val: 0.7800 time: 3.2516s\n",
            "Epoch: 0383 loss_train: 0.6944 acc_train: 0.8200 loss_val: 1.1107 acc_val: 0.7800 time: 3.2585s\n",
            "Epoch: 0384 loss_train: 0.7432 acc_train: 0.7800 loss_val: 1.1099 acc_val: 0.7800 time: 3.2653s\n",
            "Epoch: 0385 loss_train: 0.7632 acc_train: 0.8000 loss_val: 1.1115 acc_val: 0.7800 time: 3.2721s\n",
            "Epoch: 0386 loss_train: 0.7834 acc_train: 0.8200 loss_val: 1.1153 acc_val: 0.7800 time: 3.2793s\n",
            "Epoch: 0387 loss_train: 0.7365 acc_train: 0.8000 loss_val: 1.1179 acc_val: 0.7800 time: 3.2859s\n",
            "Epoch: 0388 loss_train: 0.6765 acc_train: 0.8200 loss_val: 1.1201 acc_val: 0.7800 time: 3.2939s\n",
            "Epoch: 0389 loss_train: 0.6943 acc_train: 0.8400 loss_val: 1.1214 acc_val: 0.7800 time: 3.3043s\n",
            "Epoch: 0390 loss_train: 0.7601 acc_train: 0.8200 loss_val: 1.1221 acc_val: 0.7800 time: 3.3121s\n",
            "Epoch: 0391 loss_train: 0.7218 acc_train: 0.8200 loss_val: 1.1218 acc_val: 0.7800 time: 3.3245s\n",
            "Epoch: 0392 loss_train: 0.8010 acc_train: 0.7800 loss_val: 1.1208 acc_val: 0.7800 time: 3.3332s\n",
            "Epoch: 0393 loss_train: 0.7680 acc_train: 0.8000 loss_val: 1.1199 acc_val: 0.7800 time: 3.3410s\n",
            "Epoch: 0394 loss_train: 0.7874 acc_train: 0.8000 loss_val: 1.1181 acc_val: 0.7800 time: 3.3479s\n",
            "Epoch: 0395 loss_train: 0.7945 acc_train: 0.7800 loss_val: 1.1154 acc_val: 0.7800 time: 3.3544s\n",
            "Epoch: 0396 loss_train: 0.7225 acc_train: 0.8000 loss_val: 1.1117 acc_val: 0.7800 time: 3.3614s\n",
            "Epoch: 0397 loss_train: 0.7276 acc_train: 0.8000 loss_val: 1.1100 acc_val: 0.7800 time: 3.3683s\n",
            "Epoch: 0398 loss_train: 0.8101 acc_train: 0.7800 loss_val: 1.1073 acc_val: 0.7800 time: 3.3754s\n",
            "Epoch: 0399 loss_train: 0.7132 acc_train: 0.8200 loss_val: 1.1034 acc_val: 0.7800 time: 3.3834s\n",
            "Epoch: 0400 loss_train: 0.7569 acc_train: 0.7800 loss_val: 1.1010 acc_val: 0.7800 time: 3.3903s\n",
            "Epoch: 0401 loss_train: 0.7234 acc_train: 0.8000 loss_val: 1.1016 acc_val: 0.7800 time: 3.3977s\n",
            "Epoch: 0402 loss_train: 0.7519 acc_train: 0.8000 loss_val: 1.1021 acc_val: 0.7800 time: 3.4054s\n",
            "Epoch: 0403 loss_train: 0.7313 acc_train: 0.8000 loss_val: 1.1038 acc_val: 0.7800 time: 3.4197s\n",
            "Epoch: 0404 loss_train: 0.7420 acc_train: 0.8000 loss_val: 1.1094 acc_val: 0.7800 time: 3.4288s\n",
            "Epoch: 0405 loss_train: 0.7072 acc_train: 0.8000 loss_val: 1.1147 acc_val: 0.7800 time: 3.4361s\n",
            "Epoch: 0406 loss_train: 0.6891 acc_train: 0.8200 loss_val: 1.1183 acc_val: 0.7800 time: 3.4436s\n",
            "Epoch: 0407 loss_train: 0.8266 acc_train: 0.7800 loss_val: 1.1212 acc_val: 0.7800 time: 3.4511s\n",
            "Epoch: 0408 loss_train: 0.7243 acc_train: 0.8000 loss_val: 1.1213 acc_val: 0.7800 time: 3.4585s\n",
            "Epoch: 0409 loss_train: 0.7512 acc_train: 0.8000 loss_val: 1.1200 acc_val: 0.7800 time: 3.4660s\n",
            "Epoch: 0410 loss_train: 0.8163 acc_train: 0.7800 loss_val: 1.1167 acc_val: 0.7800 time: 3.4736s\n",
            "Epoch: 0411 loss_train: 0.7235 acc_train: 0.7800 loss_val: 1.1135 acc_val: 0.7800 time: 3.4813s\n",
            "Epoch: 0412 loss_train: 0.6694 acc_train: 0.8200 loss_val: 1.1102 acc_val: 0.7800 time: 3.4891s\n",
            "Epoch: 0413 loss_train: 0.7179 acc_train: 0.8200 loss_val: 1.1084 acc_val: 0.7800 time: 3.4988s\n",
            "Epoch: 0414 loss_train: 0.7903 acc_train: 0.7800 loss_val: 1.1075 acc_val: 0.7800 time: 3.5088s\n",
            "Epoch: 0415 loss_train: 0.7326 acc_train: 0.8200 loss_val: 1.1063 acc_val: 0.7800 time: 3.5170s\n",
            "Epoch: 0416 loss_train: 0.7720 acc_train: 0.7800 loss_val: 1.1091 acc_val: 0.7800 time: 3.5259s\n",
            "Epoch: 0417 loss_train: 0.7452 acc_train: 0.8000 loss_val: 1.1124 acc_val: 0.7800 time: 3.5335s\n",
            "Epoch: 0418 loss_train: 0.7670 acc_train: 0.8000 loss_val: 1.1151 acc_val: 0.7800 time: 3.5427s\n",
            "Epoch: 0419 loss_train: 0.6822 acc_train: 0.8200 loss_val: 1.1166 acc_val: 0.7800 time: 3.5498s\n",
            "Epoch: 0420 loss_train: 0.7465 acc_train: 0.8000 loss_val: 1.1168 acc_val: 0.7800 time: 3.5569s\n",
            "Epoch: 0421 loss_train: 0.7578 acc_train: 0.7800 loss_val: 1.1160 acc_val: 0.7800 time: 3.5642s\n",
            "Epoch: 0422 loss_train: 0.7261 acc_train: 0.8000 loss_val: 1.1153 acc_val: 0.7800 time: 3.5723s\n",
            "Epoch: 0423 loss_train: 0.7467 acc_train: 0.8000 loss_val: 1.1152 acc_val: 0.7800 time: 3.5804s\n",
            "Epoch: 0424 loss_train: 0.7379 acc_train: 0.7800 loss_val: 1.1136 acc_val: 0.7800 time: 3.5881s\n",
            "Epoch: 0425 loss_train: 0.7055 acc_train: 0.8200 loss_val: 1.1115 acc_val: 0.7800 time: 3.5951s\n",
            "Epoch: 0426 loss_train: 0.7331 acc_train: 0.8000 loss_val: 1.1089 acc_val: 0.7800 time: 3.6029s\n",
            "Epoch: 0427 loss_train: 0.6926 acc_train: 0.8400 loss_val: 1.1045 acc_val: 0.7800 time: 3.6095s\n",
            "Epoch: 0428 loss_train: 0.6871 acc_train: 0.8400 loss_val: 1.1016 acc_val: 0.7800 time: 3.6178s\n",
            "Epoch: 0429 loss_train: 0.7232 acc_train: 0.8400 loss_val: 1.1002 acc_val: 0.7800 time: 3.6317s\n",
            "Epoch: 0430 loss_train: 0.7314 acc_train: 0.8000 loss_val: 1.1001 acc_val: 0.7800 time: 3.6395s\n",
            "Epoch: 0431 loss_train: 0.7479 acc_train: 0.7800 loss_val: 1.1017 acc_val: 0.7800 time: 3.6461s\n",
            "Epoch: 0432 loss_train: 0.7172 acc_train: 0.8000 loss_val: 1.1014 acc_val: 0.7800 time: 3.6538s\n",
            "Epoch: 0433 loss_train: 0.7801 acc_train: 0.7800 loss_val: 1.1005 acc_val: 0.7800 time: 3.6616s\n",
            "Epoch: 0434 loss_train: 0.6842 acc_train: 0.8200 loss_val: 1.1007 acc_val: 0.7800 time: 3.6687s\n",
            "Epoch: 0435 loss_train: 0.6616 acc_train: 0.8200 loss_val: 1.1005 acc_val: 0.7800 time: 3.6752s\n",
            "Epoch: 0436 loss_train: 0.6832 acc_train: 0.8000 loss_val: 1.1008 acc_val: 0.7800 time: 3.6816s\n",
            "Epoch: 0437 loss_train: 0.7986 acc_train: 0.7800 loss_val: 1.1013 acc_val: 0.7800 time: 3.6886s\n",
            "Epoch: 0438 loss_train: 0.7669 acc_train: 0.7800 loss_val: 1.1018 acc_val: 0.7800 time: 3.6952s\n",
            "Epoch: 0439 loss_train: 0.7630 acc_train: 0.7800 loss_val: 1.1030 acc_val: 0.7800 time: 3.7021s\n",
            "Epoch: 0440 loss_train: 0.6733 acc_train: 0.8200 loss_val: 1.1063 acc_val: 0.7800 time: 3.7100s\n",
            "Epoch: 0441 loss_train: 0.7073 acc_train: 0.8000 loss_val: 1.1100 acc_val: 0.7800 time: 3.7166s\n",
            "Epoch: 0442 loss_train: 0.7033 acc_train: 0.8000 loss_val: 1.1131 acc_val: 0.7800 time: 3.7282s\n",
            "Epoch: 0443 loss_train: 0.7669 acc_train: 0.8000 loss_val: 1.1143 acc_val: 0.7800 time: 3.7359s\n",
            "Epoch: 0444 loss_train: 0.6726 acc_train: 0.8200 loss_val: 1.1154 acc_val: 0.7800 time: 3.7431s\n",
            "Epoch: 0445 loss_train: 0.7122 acc_train: 0.8000 loss_val: 1.1158 acc_val: 0.7800 time: 3.7512s\n",
            "Epoch: 0446 loss_train: 0.6788 acc_train: 0.8400 loss_val: 1.1171 acc_val: 0.7800 time: 3.7607s\n",
            "Epoch: 0447 loss_train: 0.7510 acc_train: 0.7800 loss_val: 1.1165 acc_val: 0.7800 time: 3.7682s\n",
            "Epoch: 0448 loss_train: 0.7417 acc_train: 0.8000 loss_val: 1.1155 acc_val: 0.7800 time: 3.7775s\n",
            "Epoch: 0449 loss_train: 0.6860 acc_train: 0.8000 loss_val: 1.1137 acc_val: 0.7800 time: 3.7847s\n",
            "Epoch: 0450 loss_train: 0.7256 acc_train: 0.8200 loss_val: 1.1103 acc_val: 0.7800 time: 3.7914s\n",
            "Epoch: 0451 loss_train: 0.7645 acc_train: 0.8000 loss_val: 1.1057 acc_val: 0.7800 time: 3.7980s\n",
            "Epoch: 0452 loss_train: 0.7650 acc_train: 0.8000 loss_val: 1.1049 acc_val: 0.7800 time: 3.8049s\n",
            "Epoch: 0453 loss_train: 0.6991 acc_train: 0.8200 loss_val: 1.1057 acc_val: 0.7800 time: 3.8127s\n",
            "Epoch: 0454 loss_train: 0.7739 acc_train: 0.7800 loss_val: 1.1054 acc_val: 0.7800 time: 3.8211s\n",
            "Epoch: 0455 loss_train: 0.7187 acc_train: 0.8000 loss_val: 1.1050 acc_val: 0.7800 time: 3.8278s\n",
            "Epoch: 0456 loss_train: 0.6749 acc_train: 0.8200 loss_val: 1.1038 acc_val: 0.7800 time: 3.8372s\n",
            "Epoch: 0457 loss_train: 0.7602 acc_train: 0.7800 loss_val: 1.1027 acc_val: 0.7800 time: 3.8480s\n",
            "Epoch: 0458 loss_train: 0.7075 acc_train: 0.8000 loss_val: 1.1022 acc_val: 0.7800 time: 3.8550s\n",
            "Epoch: 0459 loss_train: 0.6932 acc_train: 0.8000 loss_val: 1.1031 acc_val: 0.7800 time: 3.8616s\n",
            "Epoch: 0460 loss_train: 0.7585 acc_train: 0.7800 loss_val: 1.1032 acc_val: 0.7800 time: 3.8687s\n",
            "Epoch: 0461 loss_train: 0.6765 acc_train: 0.8200 loss_val: 1.1046 acc_val: 0.7800 time: 3.8757s\n",
            "Epoch: 0462 loss_train: 0.7146 acc_train: 0.8000 loss_val: 1.1051 acc_val: 0.7800 time: 3.8827s\n",
            "Epoch: 0463 loss_train: 0.7386 acc_train: 0.8000 loss_val: 1.1053 acc_val: 0.7800 time: 3.8890s\n",
            "Epoch: 0464 loss_train: 0.6803 acc_train: 0.8000 loss_val: 1.1055 acc_val: 0.7800 time: 3.8953s\n",
            "Epoch: 0465 loss_train: 0.7069 acc_train: 0.8200 loss_val: 1.1058 acc_val: 0.7800 time: 3.9019s\n",
            "Epoch: 0466 loss_train: 0.7083 acc_train: 0.8000 loss_val: 1.1063 acc_val: 0.7800 time: 3.9092s\n",
            "Epoch: 0467 loss_train: 0.6841 acc_train: 0.8400 loss_val: 1.1075 acc_val: 0.7800 time: 3.9158s\n",
            "Epoch: 0468 loss_train: 0.7571 acc_train: 0.7800 loss_val: 1.1086 acc_val: 0.7800 time: 3.9256s\n",
            "Epoch: 0469 loss_train: 0.7415 acc_train: 0.8400 loss_val: 1.1100 acc_val: 0.7800 time: 3.9343s\n",
            "Epoch: 0470 loss_train: 0.7364 acc_train: 0.8000 loss_val: 1.1094 acc_val: 0.7800 time: 3.9454s\n",
            "Epoch: 0471 loss_train: 0.7118 acc_train: 0.8000 loss_val: 1.1070 acc_val: 0.7800 time: 3.9519s\n",
            "Epoch: 0472 loss_train: 0.7149 acc_train: 0.8000 loss_val: 1.1054 acc_val: 0.7800 time: 3.9583s\n",
            "Epoch: 0473 loss_train: 0.6998 acc_train: 0.8400 loss_val: 1.1031 acc_val: 0.7800 time: 3.9667s\n",
            "Epoch: 0474 loss_train: 0.7120 acc_train: 0.8400 loss_val: 1.1018 acc_val: 0.7800 time: 3.9753s\n",
            "Epoch: 0475 loss_train: 0.6792 acc_train: 0.8400 loss_val: 1.1001 acc_val: 0.7800 time: 3.9829s\n",
            "Epoch: 0476 loss_train: 0.7194 acc_train: 0.7800 loss_val: 1.0990 acc_val: 0.7800 time: 3.9917s\n",
            "Epoch: 0477 loss_train: 0.7057 acc_train: 0.8200 loss_val: 1.0986 acc_val: 0.7800 time: 3.9991s\n",
            "Epoch: 0478 loss_train: 0.7149 acc_train: 0.8200 loss_val: 1.0977 acc_val: 0.7800 time: 4.0063s\n",
            "Epoch: 0479 loss_train: 0.8078 acc_train: 0.8000 loss_val: 1.0970 acc_val: 0.7800 time: 4.0133s\n",
            "Epoch: 0480 loss_train: 0.7669 acc_train: 0.7800 loss_val: 1.0978 acc_val: 0.7800 time: 4.0200s\n",
            "Epoch: 0481 loss_train: 0.7023 acc_train: 0.8000 loss_val: 1.0988 acc_val: 0.7800 time: 4.0273s\n",
            "Epoch: 0482 loss_train: 0.6663 acc_train: 0.8200 loss_val: 1.1010 acc_val: 0.7800 time: 4.0345s\n",
            "Epoch: 0483 loss_train: 0.7090 acc_train: 0.8000 loss_val: 1.1023 acc_val: 0.7800 time: 4.0446s\n",
            "Epoch: 0484 loss_train: 0.7334 acc_train: 0.7800 loss_val: 1.1036 acc_val: 0.7800 time: 4.0591s\n",
            "Epoch: 0485 loss_train: 0.7816 acc_train: 0.8000 loss_val: 1.1044 acc_val: 0.7800 time: 4.0690s\n",
            "Epoch: 0486 loss_train: 0.7006 acc_train: 0.8400 loss_val: 1.1053 acc_val: 0.7800 time: 4.0762s\n",
            "Epoch: 0487 loss_train: 0.7758 acc_train: 0.7800 loss_val: 1.1047 acc_val: 0.7800 time: 4.0831s\n",
            "Epoch: 0488 loss_train: 0.7207 acc_train: 0.8000 loss_val: 1.1044 acc_val: 0.7800 time: 4.0913s\n",
            "Epoch: 0489 loss_train: 0.6676 acc_train: 0.8200 loss_val: 1.1044 acc_val: 0.7800 time: 4.0990s\n",
            "Epoch: 0490 loss_train: 0.6652 acc_train: 0.8400 loss_val: 1.1034 acc_val: 0.7800 time: 4.1072s\n",
            "Epoch: 0491 loss_train: 0.7368 acc_train: 0.8000 loss_val: 1.1045 acc_val: 0.7800 time: 4.1193s\n",
            "Epoch: 0492 loss_train: 0.7659 acc_train: 0.8000 loss_val: 1.1065 acc_val: 0.7800 time: 4.1282s\n",
            "Epoch: 0493 loss_train: 0.7615 acc_train: 0.8000 loss_val: 1.1067 acc_val: 0.7800 time: 4.1355s\n",
            "Epoch: 0494 loss_train: 0.7133 acc_train: 0.8000 loss_val: 1.1083 acc_val: 0.7800 time: 4.1452s\n",
            "Epoch: 0495 loss_train: 0.6732 acc_train: 0.8000 loss_val: 1.1092 acc_val: 0.7800 time: 4.1540s\n",
            "Epoch: 0496 loss_train: 0.6635 acc_train: 0.8400 loss_val: 1.1075 acc_val: 0.7800 time: 4.1605s\n",
            "Epoch: 0497 loss_train: 0.7467 acc_train: 0.7800 loss_val: 1.1019 acc_val: 0.7800 time: 4.1670s\n",
            "Epoch: 0498 loss_train: 0.7164 acc_train: 0.8000 loss_val: 1.0982 acc_val: 0.7800 time: 4.1746s\n",
            "Epoch: 0499 loss_train: 0.7023 acc_train: 0.7800 loss_val: 1.0944 acc_val: 0.7800 time: 4.1822s\n",
            "Epoch: 0500 loss_train: 0.6561 acc_train: 0.8200 loss_val: 1.0920 acc_val: 0.7800 time: 4.1896s\n",
            "Epoch: 0501 loss_train: 0.7115 acc_train: 0.8000 loss_val: 1.0915 acc_val: 0.7800 time: 4.1972s\n",
            "Epoch: 0502 loss_train: 0.6692 acc_train: 0.8200 loss_val: 1.0929 acc_val: 0.7800 time: 4.2040s\n",
            "Epoch: 0503 loss_train: 0.6630 acc_train: 0.8200 loss_val: 1.0957 acc_val: 0.7800 time: 4.2104s\n",
            "Epoch: 0504 loss_train: 0.8267 acc_train: 0.7600 loss_val: 1.0936 acc_val: 0.7800 time: 4.2168s\n",
            "Epoch: 0505 loss_train: 0.6969 acc_train: 0.8000 loss_val: 1.0936 acc_val: 0.7800 time: 4.2239s\n",
            "Epoch: 0506 loss_train: 0.7861 acc_train: 0.8000 loss_val: 1.0928 acc_val: 0.7800 time: 4.2308s\n",
            "Epoch: 0507 loss_train: 0.7269 acc_train: 0.8000 loss_val: 1.0923 acc_val: 0.7800 time: 4.2371s\n",
            "Epoch: 0508 loss_train: 0.6996 acc_train: 0.8200 loss_val: 1.0923 acc_val: 0.7800 time: 4.2439s\n",
            "Epoch: 0509 loss_train: 0.7023 acc_train: 0.8000 loss_val: 1.0922 acc_val: 0.7800 time: 4.2504s\n",
            "Epoch: 0510 loss_train: 0.6890 acc_train: 0.8000 loss_val: 1.0923 acc_val: 0.7800 time: 4.2607s\n",
            "Epoch: 0511 loss_train: 0.6726 acc_train: 0.8000 loss_val: 1.0933 acc_val: 0.7800 time: 4.2689s\n",
            "Epoch: 0512 loss_train: 0.6897 acc_train: 0.8200 loss_val: 1.0948 acc_val: 0.7800 time: 4.2755s\n",
            "Epoch: 0513 loss_train: 0.6880 acc_train: 0.7800 loss_val: 1.0971 acc_val: 0.7800 time: 4.2825s\n",
            "Epoch: 0514 loss_train: 0.6788 acc_train: 0.8000 loss_val: 1.1000 acc_val: 0.7800 time: 4.2892s\n",
            "Epoch: 0515 loss_train: 0.7016 acc_train: 0.8200 loss_val: 1.1032 acc_val: 0.7800 time: 4.2958s\n",
            "Epoch: 0516 loss_train: 0.6794 acc_train: 0.8400 loss_val: 1.1085 acc_val: 0.7800 time: 4.3026s\n",
            "Epoch: 0517 loss_train: 0.7148 acc_train: 0.8000 loss_val: 1.1128 acc_val: 0.7800 time: 4.3093s\n",
            "Epoch: 0518 loss_train: 0.7052 acc_train: 0.8000 loss_val: 1.1160 acc_val: 0.7800 time: 4.3162s\n",
            "Epoch: 0519 loss_train: 0.7054 acc_train: 0.8200 loss_val: 1.1162 acc_val: 0.7800 time: 4.3246s\n",
            "Epoch: 0520 loss_train: 0.6860 acc_train: 0.8200 loss_val: 1.1144 acc_val: 0.7800 time: 4.3328s\n",
            "Epoch: 0521 loss_train: 0.7610 acc_train: 0.7800 loss_val: 1.1112 acc_val: 0.7800 time: 4.3407s\n",
            "Epoch: 0522 loss_train: 0.6726 acc_train: 0.8400 loss_val: 1.1068 acc_val: 0.7800 time: 4.3499s\n",
            "Epoch: 0523 loss_train: 0.7396 acc_train: 0.8000 loss_val: 1.1033 acc_val: 0.7800 time: 4.3573s\n",
            "Epoch: 0524 loss_train: 0.7571 acc_train: 0.7800 loss_val: 1.0988 acc_val: 0.7800 time: 4.3643s\n",
            "Epoch: 0525 loss_train: 0.6751 acc_train: 0.8000 loss_val: 1.0958 acc_val: 0.7800 time: 4.3710s\n",
            "Epoch: 0526 loss_train: 0.7255 acc_train: 0.7800 loss_val: 1.0954 acc_val: 0.7800 time: 4.3789s\n",
            "Epoch: 0527 loss_train: 0.7684 acc_train: 0.8000 loss_val: 1.0972 acc_val: 0.7800 time: 4.3862s\n",
            "Epoch: 0528 loss_train: 0.7457 acc_train: 0.8000 loss_val: 1.1002 acc_val: 0.7800 time: 4.3935s\n",
            "Epoch: 0529 loss_train: 0.6788 acc_train: 0.8400 loss_val: 1.1037 acc_val: 0.7800 time: 4.4004s\n",
            "Epoch: 0530 loss_train: 0.6913 acc_train: 0.8000 loss_val: 1.1082 acc_val: 0.7800 time: 4.4076s\n",
            "Epoch: 0531 loss_train: 0.6590 acc_train: 0.8200 loss_val: 1.1135 acc_val: 0.7800 time: 4.4142s\n",
            "Epoch: 0532 loss_train: 0.6970 acc_train: 0.8000 loss_val: 1.1176 acc_val: 0.7800 time: 4.4223s\n",
            "Epoch: 0533 loss_train: 0.7308 acc_train: 0.8000 loss_val: 1.1200 acc_val: 0.7800 time: 4.4291s\n",
            "Epoch: 0534 loss_train: 0.6332 acc_train: 0.8200 loss_val: 1.1204 acc_val: 0.7800 time: 4.4353s\n",
            "Epoch: 0535 loss_train: 0.7105 acc_train: 0.8400 loss_val: 1.1189 acc_val: 0.7800 time: 4.4417s\n",
            "Epoch: 0536 loss_train: 0.6863 acc_train: 0.8000 loss_val: 1.1169 acc_val: 0.7800 time: 4.4490s\n",
            "Epoch: 0537 loss_train: 0.6547 acc_train: 0.8200 loss_val: 1.1134 acc_val: 0.7800 time: 4.4557s\n",
            "Epoch: 0538 loss_train: 0.7289 acc_train: 0.8200 loss_val: 1.1098 acc_val: 0.7800 time: 4.4625s\n",
            "Epoch: 0539 loss_train: 0.6992 acc_train: 0.8200 loss_val: 1.1070 acc_val: 0.7800 time: 4.4719s\n",
            "Epoch: 0540 loss_train: 0.6285 acc_train: 0.8600 loss_val: 1.1063 acc_val: 0.7800 time: 4.4818s\n",
            "Epoch: 0541 loss_train: 0.6416 acc_train: 0.8200 loss_val: 1.1073 acc_val: 0.7800 time: 4.4890s\n",
            "Epoch: 0542 loss_train: 0.7316 acc_train: 0.8000 loss_val: 1.1076 acc_val: 0.7800 time: 4.4961s\n",
            "Epoch: 0543 loss_train: 0.6476 acc_train: 0.8200 loss_val: 1.1065 acc_val: 0.7800 time: 4.5039s\n",
            "Epoch: 0544 loss_train: 0.6519 acc_train: 0.8200 loss_val: 1.1065 acc_val: 0.7800 time: 4.5106s\n",
            "Epoch: 0545 loss_train: 0.6698 acc_train: 0.8200 loss_val: 1.1063 acc_val: 0.7800 time: 4.5182s\n",
            "Epoch: 0546 loss_train: 0.6881 acc_train: 0.8200 loss_val: 1.1057 acc_val: 0.7800 time: 4.5290s\n",
            "Epoch: 0547 loss_train: 0.5811 acc_train: 0.8600 loss_val: 1.1050 acc_val: 0.7800 time: 4.5371s\n",
            "Epoch: 0548 loss_train: 0.6828 acc_train: 0.8000 loss_val: 1.1058 acc_val: 0.7800 time: 4.5445s\n",
            "Epoch: 0549 loss_train: 0.6889 acc_train: 0.8200 loss_val: 1.1064 acc_val: 0.7800 time: 4.5525s\n",
            "Epoch: 0550 loss_train: 0.7386 acc_train: 0.8000 loss_val: 1.1068 acc_val: 0.7800 time: 4.5598s\n",
            "Epoch: 0551 loss_train: 0.6642 acc_train: 0.8000 loss_val: 1.1063 acc_val: 0.7800 time: 4.5672s\n",
            "Epoch: 0552 loss_train: 0.6328 acc_train: 0.8400 loss_val: 1.1070 acc_val: 0.7800 time: 4.5721s\n",
            "Epoch: 0553 loss_train: 0.6751 acc_train: 0.8000 loss_val: 1.1078 acc_val: 0.7800 time: 4.5780s\n",
            "Epoch: 0554 loss_train: 0.6895 acc_train: 0.8000 loss_val: 1.1075 acc_val: 0.7800 time: 4.5856s\n",
            "Epoch: 0555 loss_train: 0.6842 acc_train: 0.8000 loss_val: 1.1053 acc_val: 0.7800 time: 4.5944s\n",
            "Epoch: 0556 loss_train: 0.7735 acc_train: 0.8000 loss_val: 1.1026 acc_val: 0.7800 time: 4.6021s\n",
            "Epoch: 0557 loss_train: 0.6755 acc_train: 0.8000 loss_val: 1.1011 acc_val: 0.7800 time: 4.6101s\n",
            "Epoch: 0558 loss_train: 0.7421 acc_train: 0.8200 loss_val: 1.1003 acc_val: 0.7800 time: 4.6179s\n",
            "Epoch: 0559 loss_train: 0.6677 acc_train: 0.8200 loss_val: 1.1009 acc_val: 0.7800 time: 4.6258s\n",
            "Epoch: 0560 loss_train: 0.7185 acc_train: 0.8200 loss_val: 1.1011 acc_val: 0.7800 time: 4.6326s\n",
            "Epoch: 0561 loss_train: 0.7363 acc_train: 0.8200 loss_val: 1.0945 acc_val: 0.7800 time: 4.6390s\n",
            "Epoch: 0562 loss_train: 0.7648 acc_train: 0.7800 loss_val: 1.0938 acc_val: 0.7800 time: 4.6455s\n",
            "Epoch: 0563 loss_train: 0.5973 acc_train: 0.8600 loss_val: 1.0969 acc_val: 0.7800 time: 4.6519s\n",
            "Epoch: 0564 loss_train: 0.6960 acc_train: 0.8000 loss_val: 1.1014 acc_val: 0.7800 time: 4.6610s\n",
            "Epoch: 0565 loss_train: 0.7065 acc_train: 0.8200 loss_val: 1.1054 acc_val: 0.7800 time: 4.6693s\n",
            "Epoch: 0566 loss_train: 0.6440 acc_train: 0.8400 loss_val: 1.1098 acc_val: 0.7800 time: 4.6787s\n",
            "Epoch: 0567 loss_train: 0.6298 acc_train: 0.8200 loss_val: 1.1133 acc_val: 0.7800 time: 4.6879s\n",
            "Epoch: 0568 loss_train: 0.7005 acc_train: 0.8200 loss_val: 1.1126 acc_val: 0.7800 time: 4.6941s\n",
            "Epoch: 0569 loss_train: 0.8067 acc_train: 0.7800 loss_val: 1.1091 acc_val: 0.7800 time: 4.7013s\n",
            "Epoch: 0570 loss_train: 0.6539 acc_train: 0.8200 loss_val: 1.1046 acc_val: 0.7800 time: 4.7088s\n",
            "Epoch: 0571 loss_train: 0.7111 acc_train: 0.8200 loss_val: 1.0994 acc_val: 0.7800 time: 4.7165s\n",
            "Epoch: 0572 loss_train: 0.6219 acc_train: 0.8600 loss_val: 1.0959 acc_val: 0.7800 time: 4.7245s\n",
            "Epoch: 0573 loss_train: 0.7205 acc_train: 0.7800 loss_val: 1.0946 acc_val: 0.7800 time: 4.7321s\n",
            "Epoch: 0574 loss_train: 0.7067 acc_train: 0.8000 loss_val: 1.0950 acc_val: 0.7800 time: 4.7399s\n",
            "Epoch: 0575 loss_train: 0.7090 acc_train: 0.8000 loss_val: 1.0929 acc_val: 0.7800 time: 4.7462s\n",
            "Epoch: 0576 loss_train: 0.6615 acc_train: 0.8400 loss_val: 1.0917 acc_val: 0.7800 time: 4.7536s\n",
            "Epoch: 0577 loss_train: 0.6532 acc_train: 0.8200 loss_val: 1.0913 acc_val: 0.7800 time: 4.7612s\n",
            "Epoch: 0578 loss_train: 0.7210 acc_train: 0.8200 loss_val: 1.0920 acc_val: 0.7800 time: 4.7687s\n",
            "Epoch: 0579 loss_train: 0.7078 acc_train: 0.7800 loss_val: 1.0954 acc_val: 0.7800 time: 4.7794s\n",
            "Epoch: 0580 loss_train: 0.6901 acc_train: 0.8000 loss_val: 1.0999 acc_val: 0.7800 time: 4.7873s\n",
            "Epoch: 0581 loss_train: 0.6893 acc_train: 0.8200 loss_val: 1.1046 acc_val: 0.7800 time: 4.7940s\n",
            "Epoch: 0582 loss_train: 0.6662 acc_train: 0.8400 loss_val: 1.1096 acc_val: 0.7800 time: 4.8011s\n",
            "Epoch: 0583 loss_train: 0.7420 acc_train: 0.7800 loss_val: 1.1124 acc_val: 0.7800 time: 4.8085s\n",
            "Epoch: 0584 loss_train: 0.6892 acc_train: 0.8200 loss_val: 1.1150 acc_val: 0.7800 time: 4.8160s\n",
            "Epoch: 0585 loss_train: 0.7782 acc_train: 0.8000 loss_val: 1.1152 acc_val: 0.7800 time: 4.8232s\n",
            "Epoch: 0586 loss_train: 0.7320 acc_train: 0.7800 loss_val: 1.1142 acc_val: 0.7800 time: 4.8309s\n",
            "Epoch: 0587 loss_train: 0.6633 acc_train: 0.8400 loss_val: 1.1131 acc_val: 0.7800 time: 4.8361s\n",
            "Epoch: 0588 loss_train: 0.7181 acc_train: 0.8000 loss_val: 1.1103 acc_val: 0.7800 time: 4.8428s\n",
            "Epoch: 0589 loss_train: 0.6808 acc_train: 0.8200 loss_val: 1.1071 acc_val: 0.7800 time: 4.8493s\n",
            "Epoch: 0590 loss_train: 0.7056 acc_train: 0.8000 loss_val: 1.1038 acc_val: 0.7800 time: 4.8562s\n",
            "Epoch: 0591 loss_train: 0.6637 acc_train: 0.8200 loss_val: 1.1015 acc_val: 0.7800 time: 4.8627s\n",
            "Epoch: 0592 loss_train: 0.7159 acc_train: 0.8000 loss_val: 1.0997 acc_val: 0.7800 time: 4.8694s\n",
            "Epoch: 0593 loss_train: 0.8070 acc_train: 0.7800 loss_val: 1.0991 acc_val: 0.7800 time: 4.8759s\n",
            "Epoch: 0594 loss_train: 0.6531 acc_train: 0.8200 loss_val: 1.0993 acc_val: 0.7800 time: 4.8866s\n",
            "Epoch: 0595 loss_train: 0.6681 acc_train: 0.8200 loss_val: 1.1004 acc_val: 0.7800 time: 4.8963s\n",
            "Epoch: 0596 loss_train: 0.6448 acc_train: 0.8000 loss_val: 1.1024 acc_val: 0.7800 time: 4.9038s\n",
            "Epoch: 0597 loss_train: 0.6459 acc_train: 0.8600 loss_val: 1.1048 acc_val: 0.7800 time: 4.9110s\n",
            "Epoch: 0598 loss_train: 0.7149 acc_train: 0.8200 loss_val: 1.1057 acc_val: 0.7800 time: 4.9180s\n",
            "Epoch: 0599 loss_train: 0.6833 acc_train: 0.8200 loss_val: 1.1055 acc_val: 0.7800 time: 4.9252s\n",
            "Epoch: 0600 loss_train: 0.7005 acc_train: 0.8200 loss_val: 1.1050 acc_val: 0.7800 time: 4.9325s\n",
            "Epoch: 0601 loss_train: 0.6367 acc_train: 0.8600 loss_val: 1.1045 acc_val: 0.7800 time: 4.9398s\n",
            "Epoch: 0602 loss_train: 0.7249 acc_train: 0.7800 loss_val: 1.1044 acc_val: 0.7800 time: 4.9470s\n",
            "Epoch: 0603 loss_train: 0.6745 acc_train: 0.8200 loss_val: 1.1047 acc_val: 0.7800 time: 4.9541s\n",
            "Epoch: 0604 loss_train: 0.6427 acc_train: 0.8200 loss_val: 1.1065 acc_val: 0.7800 time: 4.9613s\n",
            "Epoch: 0605 loss_train: 0.6963 acc_train: 0.8200 loss_val: 1.1083 acc_val: 0.7800 time: 4.9687s\n",
            "Epoch: 0606 loss_train: 0.6854 acc_train: 0.8200 loss_val: 1.1116 acc_val: 0.7800 time: 4.9773s\n",
            "Epoch: 0607 loss_train: 0.6410 acc_train: 0.8400 loss_val: 1.1154 acc_val: 0.7800 time: 4.9866s\n",
            "Epoch: 0608 loss_train: 0.6547 acc_train: 0.8400 loss_val: 1.1204 acc_val: 0.7600 time: 4.9934s\n",
            "Epoch: 0609 loss_train: 0.6882 acc_train: 0.8000 loss_val: 1.1233 acc_val: 0.7600 time: 5.0009s\n",
            "Epoch: 0610 loss_train: 0.6869 acc_train: 0.8400 loss_val: 1.1296 acc_val: 0.7600 time: 5.0094s\n",
            "Epoch: 0611 loss_train: 0.6469 acc_train: 0.8200 loss_val: 1.1356 acc_val: 0.7600 time: 5.0163s\n",
            "Epoch: 0612 loss_train: 0.7037 acc_train: 0.7800 loss_val: 1.1377 acc_val: 0.7600 time: 5.0232s\n",
            "Epoch: 0613 loss_train: 0.6366 acc_train: 0.8200 loss_val: 1.1415 acc_val: 0.7600 time: 5.0297s\n",
            "Epoch: 0614 loss_train: 0.6509 acc_train: 0.8400 loss_val: 1.1335 acc_val: 0.7600 time: 5.0362s\n",
            "Epoch: 0615 loss_train: 0.7474 acc_train: 0.8000 loss_val: 1.1275 acc_val: 0.7800 time: 5.0426s\n",
            "Epoch: 0616 loss_train: 0.6568 acc_train: 0.8000 loss_val: 1.1249 acc_val: 0.7800 time: 5.0489s\n",
            "Epoch: 0617 loss_train: 0.6565 acc_train: 0.8200 loss_val: 1.1196 acc_val: 0.7800 time: 5.0560s\n",
            "Epoch: 0618 loss_train: 0.6287 acc_train: 0.8200 loss_val: 1.1176 acc_val: 0.7800 time: 5.0641s\n",
            "Epoch: 0619 loss_train: 0.7445 acc_train: 0.7800 loss_val: 1.1154 acc_val: 0.7800 time: 5.0715s\n",
            "Epoch: 0620 loss_train: 0.6685 acc_train: 0.8200 loss_val: 1.1135 acc_val: 0.7800 time: 5.0786s\n",
            "Epoch: 0621 loss_train: 0.6325 acc_train: 0.8600 loss_val: 1.1123 acc_val: 0.7800 time: 5.0860s\n",
            "Epoch: 0622 loss_train: 0.6760 acc_train: 0.8200 loss_val: 1.1102 acc_val: 0.7800 time: 5.0994s\n",
            "Epoch: 0623 loss_train: 0.6144 acc_train: 0.8600 loss_val: 1.1087 acc_val: 0.7800 time: 5.1067s\n",
            "Epoch: 0624 loss_train: 0.6284 acc_train: 0.8400 loss_val: 1.1073 acc_val: 0.7800 time: 5.1131s\n",
            "Epoch: 0625 loss_train: 0.6841 acc_train: 0.8000 loss_val: 1.1071 acc_val: 0.7800 time: 5.1196s\n",
            "Epoch: 0626 loss_train: 0.7185 acc_train: 0.8200 loss_val: 1.1070 acc_val: 0.7800 time: 5.1267s\n",
            "Epoch: 0627 loss_train: 0.7675 acc_train: 0.8000 loss_val: 1.1076 acc_val: 0.7800 time: 5.1385s\n",
            "Epoch: 0628 loss_train: 0.7135 acc_train: 0.8200 loss_val: 1.1081 acc_val: 0.7800 time: 5.1499s\n",
            "Epoch: 0629 loss_train: 0.5972 acc_train: 0.8400 loss_val: 1.1101 acc_val: 0.7800 time: 5.1572s\n",
            "Epoch: 0630 loss_train: 0.7189 acc_train: 0.8000 loss_val: 1.1113 acc_val: 0.7800 time: 5.1643s\n",
            "Epoch: 0631 loss_train: 0.6308 acc_train: 0.8600 loss_val: 1.1120 acc_val: 0.7800 time: 5.1720s\n",
            "Epoch: 0632 loss_train: 0.6789 acc_train: 0.8200 loss_val: 1.1104 acc_val: 0.7800 time: 5.1799s\n",
            "Epoch: 0633 loss_train: 0.6405 acc_train: 0.8200 loss_val: 1.1094 acc_val: 0.7800 time: 5.1883s\n",
            "Epoch: 0634 loss_train: 0.6663 acc_train: 0.8200 loss_val: 1.1104 acc_val: 0.7800 time: 5.1962s\n",
            "Epoch: 0635 loss_train: 0.6997 acc_train: 0.8200 loss_val: 1.1148 acc_val: 0.7800 time: 5.2035s\n",
            "Epoch: 0636 loss_train: 0.6518 acc_train: 0.7800 loss_val: 1.1199 acc_val: 0.7600 time: 5.2115s\n",
            "Epoch: 0637 loss_train: 0.6727 acc_train: 0.8200 loss_val: 1.1318 acc_val: 0.7600 time: 5.2190s\n",
            "Epoch: 0638 loss_train: 0.6384 acc_train: 0.8200 loss_val: 1.1519 acc_val: 0.7400 time: 5.2263s\n",
            "Epoch: 0639 loss_train: 0.6249 acc_train: 0.8200 loss_val: 1.1771 acc_val: 0.7400 time: 5.2339s\n",
            "Epoch: 0640 loss_train: 0.6375 acc_train: 0.8200 loss_val: 1.2076 acc_val: 0.7400 time: 5.2393s\n",
            "Epoch: 0641 loss_train: 0.6636 acc_train: 0.8200 loss_val: 1.1682 acc_val: 0.7400 time: 5.2464s\n",
            "Epoch: 0642 loss_train: 0.7157 acc_train: 0.8000 loss_val: 1.1400 acc_val: 0.7600 time: 5.2543s\n",
            "Epoch: 0643 loss_train: 0.6737 acc_train: 0.8000 loss_val: 1.1256 acc_val: 0.7600 time: 5.2618s\n",
            "Epoch: 0644 loss_train: 0.6382 acc_train: 0.8400 loss_val: 1.1182 acc_val: 0.7800 time: 5.2689s\n",
            "Epoch: 0645 loss_train: 0.6683 acc_train: 0.8200 loss_val: 1.1154 acc_val: 0.7800 time: 5.2768s\n",
            "Epoch: 0646 loss_train: 0.6181 acc_train: 0.8400 loss_val: 1.1154 acc_val: 0.7800 time: 5.2850s\n",
            "Epoch: 0647 loss_train: 0.5766 acc_train: 0.8600 loss_val: 1.1168 acc_val: 0.7800 time: 5.2956s\n",
            "Epoch: 0648 loss_train: 0.6017 acc_train: 0.8400 loss_val: 1.1201 acc_val: 0.7800 time: 5.3059s\n",
            "Epoch: 0649 loss_train: 0.6913 acc_train: 0.8400 loss_val: 1.1234 acc_val: 0.7800 time: 5.3157s\n",
            "Epoch: 0650 loss_train: 0.6908 acc_train: 0.8200 loss_val: 1.1245 acc_val: 0.7800 time: 5.3228s\n",
            "Epoch: 0651 loss_train: 0.6498 acc_train: 0.8200 loss_val: 1.1242 acc_val: 0.7800 time: 5.3296s\n",
            "Epoch: 0652 loss_train: 0.6133 acc_train: 0.8400 loss_val: 1.1248 acc_val: 0.7600 time: 5.3365s\n",
            "Epoch: 0653 loss_train: 0.6442 acc_train: 0.8400 loss_val: 1.1250 acc_val: 0.7600 time: 5.3440s\n",
            "Epoch: 0654 loss_train: 0.6192 acc_train: 0.8400 loss_val: 1.1225 acc_val: 0.7600 time: 5.3512s\n",
            "Epoch: 0655 loss_train: 0.6207 acc_train: 0.8200 loss_val: 1.1211 acc_val: 0.7800 time: 5.3595s\n",
            "Epoch: 0656 loss_train: 0.6799 acc_train: 0.8400 loss_val: 1.1204 acc_val: 0.7800 time: 5.3675s\n",
            "Epoch: 0657 loss_train: 0.6587 acc_train: 0.8200 loss_val: 1.1208 acc_val: 0.7800 time: 5.3751s\n",
            "Epoch: 0658 loss_train: 0.7323 acc_train: 0.8000 loss_val: 1.1201 acc_val: 0.7800 time: 5.3828s\n",
            "Epoch: 0659 loss_train: 0.6339 acc_train: 0.8600 loss_val: 1.1204 acc_val: 0.7800 time: 5.3906s\n",
            "Epoch: 0660 loss_train: 0.7274 acc_train: 0.8000 loss_val: 1.1206 acc_val: 0.7800 time: 5.4007s\n",
            "Epoch: 0661 loss_train: 0.6702 acc_train: 0.8200 loss_val: 1.1198 acc_val: 0.7800 time: 5.4084s\n",
            "Epoch: 0662 loss_train: 0.7149 acc_train: 0.7800 loss_val: 1.1182 acc_val: 0.7800 time: 5.4159s\n",
            "Epoch: 0663 loss_train: 0.6434 acc_train: 0.8200 loss_val: 1.1165 acc_val: 0.7800 time: 5.4255s\n",
            "Epoch: 0664 loss_train: 0.6102 acc_train: 0.8200 loss_val: 1.1147 acc_val: 0.7800 time: 5.4340s\n",
            "Epoch: 0665 loss_train: 0.7084 acc_train: 0.8000 loss_val: 1.1132 acc_val: 0.7800 time: 5.4425s\n",
            "Epoch: 0666 loss_train: 0.6440 acc_train: 0.8400 loss_val: 1.1116 acc_val: 0.7800 time: 5.4522s\n",
            "Epoch: 0667 loss_train: 0.7783 acc_train: 0.7800 loss_val: 1.1106 acc_val: 0.7800 time: 5.4597s\n",
            "Epoch: 0668 loss_train: 0.6361 acc_train: 0.8200 loss_val: 1.1116 acc_val: 0.7800 time: 5.4685s\n",
            "Epoch: 0669 loss_train: 0.6305 acc_train: 0.8400 loss_val: 1.1164 acc_val: 0.7800 time: 5.4755s\n",
            "Epoch: 0670 loss_train: 0.6144 acc_train: 0.8400 loss_val: 1.1213 acc_val: 0.7600 time: 5.4836s\n",
            "Epoch: 0671 loss_train: 0.6726 acc_train: 0.8400 loss_val: 1.1249 acc_val: 0.7600 time: 5.4909s\n",
            "Epoch: 0672 loss_train: 0.6815 acc_train: 0.8000 loss_val: 1.1262 acc_val: 0.7600 time: 5.4991s\n",
            "Epoch: 0673 loss_train: 0.6244 acc_train: 0.8200 loss_val: 1.1263 acc_val: 0.7600 time: 5.5072s\n",
            "Epoch: 0674 loss_train: 0.7564 acc_train: 0.7800 loss_val: 1.1301 acc_val: 0.7600 time: 5.5169s\n",
            "Epoch: 0675 loss_train: 0.6651 acc_train: 0.8000 loss_val: 1.1341 acc_val: 0.7600 time: 5.5234s\n",
            "Epoch: 0676 loss_train: 0.6886 acc_train: 0.8200 loss_val: 1.1375 acc_val: 0.7600 time: 5.5355s\n",
            "Epoch: 0677 loss_train: 0.6174 acc_train: 0.8400 loss_val: 1.1427 acc_val: 0.7600 time: 5.5438s\n",
            "Epoch: 0678 loss_train: 0.6279 acc_train: 0.8400 loss_val: 1.1555 acc_val: 0.7400 time: 5.5518s\n",
            "Epoch: 0679 loss_train: 0.6101 acc_train: 0.8600 loss_val: 1.1678 acc_val: 0.7400 time: 5.5591s\n",
            "Epoch: 0680 loss_train: 0.7056 acc_train: 0.8200 loss_val: 1.1790 acc_val: 0.7400 time: 5.5674s\n",
            "Epoch: 0681 loss_train: 0.6459 acc_train: 0.8400 loss_val: 1.1736 acc_val: 0.7400 time: 5.5748s\n",
            "Epoch: 0682 loss_train: 0.7258 acc_train: 0.7800 loss_val: 1.1652 acc_val: 0.7400 time: 5.5817s\n",
            "Epoch: 0683 loss_train: 0.6356 acc_train: 0.8200 loss_val: 1.1565 acc_val: 0.7400 time: 5.5903s\n",
            "Epoch: 0684 loss_train: 0.6988 acc_train: 0.8000 loss_val: 1.1491 acc_val: 0.7400 time: 5.5988s\n",
            "Epoch: 0685 loss_train: 0.6600 acc_train: 0.8200 loss_val: 1.1130 acc_val: 0.7800 time: 5.6100s\n",
            "Epoch: 0686 loss_train: 0.6312 acc_train: 0.8400 loss_val: 1.1159 acc_val: 0.7800 time: 5.6193s\n",
            "Epoch: 0687 loss_train: 0.6689 acc_train: 0.8200 loss_val: 1.1189 acc_val: 0.7800 time: 5.6273s\n",
            "Epoch: 0688 loss_train: 0.6918 acc_train: 0.8000 loss_val: 1.1215 acc_val: 0.7800 time: 5.6371s\n",
            "Epoch: 0689 loss_train: 0.7040 acc_train: 0.8200 loss_val: 1.1215 acc_val: 0.7800 time: 5.6443s\n",
            "Epoch: 0690 loss_train: 0.6015 acc_train: 0.8200 loss_val: 1.1229 acc_val: 0.7800 time: 5.6529s\n",
            "Epoch: 0691 loss_train: 0.7023 acc_train: 0.7800 loss_val: 1.1244 acc_val: 0.7800 time: 5.6596s\n",
            "Epoch: 0692 loss_train: 0.6422 acc_train: 0.8200 loss_val: 1.1239 acc_val: 0.7800 time: 5.6665s\n",
            "Epoch: 0693 loss_train: 0.6709 acc_train: 0.8400 loss_val: 1.1214 acc_val: 0.7800 time: 5.6745s\n",
            "Epoch: 0694 loss_train: 0.6986 acc_train: 0.8000 loss_val: 1.1160 acc_val: 0.7800 time: 5.6816s\n",
            "Epoch: 0695 loss_train: 0.6767 acc_train: 0.8200 loss_val: 1.1123 acc_val: 0.7800 time: 5.6890s\n",
            "Epoch: 0696 loss_train: 0.6772 acc_train: 0.8000 loss_val: 1.1092 acc_val: 0.7800 time: 5.6962s\n",
            "Epoch: 0697 loss_train: 0.6106 acc_train: 0.8400 loss_val: 1.1075 acc_val: 0.7800 time: 5.7028s\n",
            "Epoch: 0698 loss_train: 0.6776 acc_train: 0.8200 loss_val: 1.1046 acc_val: 0.7800 time: 5.7096s\n",
            "Epoch: 0699 loss_train: 0.6060 acc_train: 0.8400 loss_val: 1.1048 acc_val: 0.7800 time: 5.7169s\n",
            "Epoch: 0700 loss_train: 0.6140 acc_train: 0.8400 loss_val: 1.1094 acc_val: 0.7800 time: 5.7277s\n",
            "Epoch: 0701 loss_train: 0.6966 acc_train: 0.8200 loss_val: 1.1216 acc_val: 0.7600 time: 5.7407s\n",
            "Epoch: 0702 loss_train: 0.7062 acc_train: 0.8000 loss_val: 1.1403 acc_val: 0.7600 time: 5.7480s\n",
            "Epoch: 0703 loss_train: 0.6230 acc_train: 0.8200 loss_val: 1.1456 acc_val: 0.7600 time: 5.7581s\n",
            "Epoch: 0704 loss_train: 0.6538 acc_train: 0.8400 loss_val: 1.1450 acc_val: 0.7600 time: 5.7671s\n",
            "Epoch: 0705 loss_train: 0.6383 acc_train: 0.8400 loss_val: 1.1379 acc_val: 0.7600 time: 5.7755s\n",
            "Epoch: 0706 loss_train: 0.6190 acc_train: 0.8400 loss_val: 1.1353 acc_val: 0.7600 time: 5.7860s\n",
            "Epoch: 0707 loss_train: 0.6060 acc_train: 0.8400 loss_val: 1.1338 acc_val: 0.7600 time: 5.7928s\n",
            "Epoch: 0708 loss_train: 0.6191 acc_train: 0.8200 loss_val: 1.1309 acc_val: 0.7600 time: 5.7996s\n",
            "Epoch: 0709 loss_train: 0.6978 acc_train: 0.8000 loss_val: 1.1301 acc_val: 0.7600 time: 5.8061s\n",
            "Epoch: 0710 loss_train: 0.6724 acc_train: 0.8200 loss_val: 1.1290 acc_val: 0.7600 time: 5.8126s\n",
            "Epoch: 0711 loss_train: 0.6535 acc_train: 0.8000 loss_val: 1.1250 acc_val: 0.7800 time: 5.8217s\n",
            "Epoch: 0712 loss_train: 0.6155 acc_train: 0.8600 loss_val: 1.1237 acc_val: 0.7800 time: 5.8293s\n",
            "Epoch: 0713 loss_train: 0.6263 acc_train: 0.8400 loss_val: 1.1239 acc_val: 0.7800 time: 5.8375s\n",
            "Epoch: 0714 loss_train: 0.6697 acc_train: 0.8200 loss_val: 1.1239 acc_val: 0.7800 time: 5.8452s\n",
            "Epoch: 0715 loss_train: 0.6305 acc_train: 0.8600 loss_val: 1.1238 acc_val: 0.7800 time: 5.8523s\n",
            "Epoch: 0716 loss_train: 0.6241 acc_train: 0.8400 loss_val: 1.1239 acc_val: 0.7800 time: 5.8593s\n",
            "Epoch: 0717 loss_train: 0.6492 acc_train: 0.8200 loss_val: 1.1247 acc_val: 0.7800 time: 5.8658s\n",
            "Epoch: 0718 loss_train: 0.6439 acc_train: 0.8000 loss_val: 1.1260 acc_val: 0.7800 time: 5.8723s\n",
            "Epoch: 0719 loss_train: 0.6149 acc_train: 0.8200 loss_val: 1.1275 acc_val: 0.7800 time: 5.8801s\n",
            "Epoch: 0720 loss_train: 0.6057 acc_train: 0.8400 loss_val: 1.1293 acc_val: 0.7800 time: 5.8880s\n",
            "Epoch: 0721 loss_train: 0.6289 acc_train: 0.8400 loss_val: 1.1327 acc_val: 0.7800 time: 5.8946s\n",
            "Epoch: 0722 loss_train: 0.7639 acc_train: 0.8000 loss_val: 1.1365 acc_val: 0.7800 time: 5.9021s\n",
            "Epoch: 0723 loss_train: 0.6577 acc_train: 0.8400 loss_val: 1.1403 acc_val: 0.7600 time: 5.9087s\n",
            "Epoch: 0724 loss_train: 0.6375 acc_train: 0.8400 loss_val: 1.1443 acc_val: 0.7600 time: 5.9215s\n",
            "Epoch: 0725 loss_train: 0.6417 acc_train: 0.7800 loss_val: 1.1532 acc_val: 0.7600 time: 5.9290s\n",
            "Epoch: 0726 loss_train: 0.6647 acc_train: 0.8600 loss_val: 1.1564 acc_val: 0.7600 time: 5.9363s\n",
            "Epoch: 0727 loss_train: 0.6041 acc_train: 0.8400 loss_val: 1.1646 acc_val: 0.7400 time: 5.9447s\n",
            "Epoch: 0728 loss_train: 0.6489 acc_train: 0.8200 loss_val: 1.1821 acc_val: 0.7400 time: 5.9551s\n",
            "Epoch: 0729 loss_train: 0.6123 acc_train: 0.8400 loss_val: 1.1956 acc_val: 0.7400 time: 5.9639s\n",
            "Epoch: 0730 loss_train: 0.6959 acc_train: 0.8000 loss_val: 1.2063 acc_val: 0.7400 time: 5.9710s\n",
            "Epoch: 0731 loss_train: 0.6006 acc_train: 0.8800 loss_val: 1.1854 acc_val: 0.7400 time: 5.9790s\n",
            "Epoch: 0732 loss_train: 0.6478 acc_train: 0.8000 loss_val: 1.1688 acc_val: 0.7400 time: 5.9883s\n",
            "Epoch: 0733 loss_train: 0.6642 acc_train: 0.8200 loss_val: 1.1512 acc_val: 0.7600 time: 5.9962s\n",
            "Epoch: 0734 loss_train: 0.6288 acc_train: 0.8200 loss_val: 1.1364 acc_val: 0.7600 time: 6.0045s\n",
            "Epoch: 0735 loss_train: 0.6144 acc_train: 0.8400 loss_val: 1.1315 acc_val: 0.7600 time: 6.0122s\n",
            "Epoch: 0736 loss_train: 0.6410 acc_train: 0.8200 loss_val: 1.1299 acc_val: 0.7600 time: 6.0206s\n",
            "Epoch: 0737 loss_train: 0.6560 acc_train: 0.8200 loss_val: 1.1279 acc_val: 0.7600 time: 6.0293s\n",
            "Epoch: 0738 loss_train: 0.6115 acc_train: 0.8400 loss_val: 1.1303 acc_val: 0.7600 time: 6.0420s\n",
            "Epoch: 0739 loss_train: 0.5992 acc_train: 0.8400 loss_val: 1.1340 acc_val: 0.7600 time: 6.0501s\n",
            "Epoch: 0740 loss_train: 0.5992 acc_train: 0.8400 loss_val: 1.1439 acc_val: 0.7600 time: 6.0580s\n",
            "Epoch: 0741 loss_train: 0.6592 acc_train: 0.8000 loss_val: 1.1568 acc_val: 0.7600 time: 6.0661s\n",
            "Epoch: 0742 loss_train: 0.6181 acc_train: 0.8400 loss_val: 1.1737 acc_val: 0.7400 time: 6.0738s\n",
            "Epoch: 0743 loss_train: 0.6320 acc_train: 0.8200 loss_val: 1.1843 acc_val: 0.7400 time: 6.0812s\n",
            "Epoch: 0744 loss_train: 0.6862 acc_train: 0.8200 loss_val: 1.1865 acc_val: 0.7400 time: 6.0891s\n",
            "Epoch: 0745 loss_train: 0.6580 acc_train: 0.8200 loss_val: 1.1921 acc_val: 0.7400 time: 6.0962s\n",
            "Epoch: 0746 loss_train: 0.5921 acc_train: 0.8600 loss_val: 1.2012 acc_val: 0.7400 time: 6.1040s\n",
            "Epoch: 0747 loss_train: 0.6005 acc_train: 0.8400 loss_val: 1.1970 acc_val: 0.7400 time: 6.1111s\n",
            "Epoch: 0748 loss_train: 0.6808 acc_train: 0.8000 loss_val: 1.1923 acc_val: 0.7400 time: 6.1200s\n",
            "Epoch: 0749 loss_train: 0.6943 acc_train: 0.7800 loss_val: 1.1812 acc_val: 0.7400 time: 6.1255s\n",
            "Epoch: 0750 loss_train: 0.6984 acc_train: 0.8000 loss_val: 1.1750 acc_val: 0.7400 time: 6.1316s\n",
            "Epoch: 0751 loss_train: 0.7006 acc_train: 0.8000 loss_val: 1.1479 acc_val: 0.7600 time: 6.1370s\n",
            "Epoch: 0752 loss_train: 0.6526 acc_train: 0.8200 loss_val: 1.1349 acc_val: 0.7800 time: 6.1446s\n",
            "Epoch: 0753 loss_train: 0.5682 acc_train: 0.8600 loss_val: 1.1315 acc_val: 0.7800 time: 6.1575s\n",
            "Epoch: 0754 loss_train: 0.6733 acc_train: 0.8000 loss_val: 1.1305 acc_val: 0.7800 time: 6.1639s\n",
            "Epoch: 0755 loss_train: 0.6334 acc_train: 0.8000 loss_val: 1.1313 acc_val: 0.7800 time: 6.1815s\n",
            "Epoch: 0756 loss_train: 0.5612 acc_train: 0.8400 loss_val: 1.1365 acc_val: 0.7800 time: 6.1890s\n",
            "Epoch: 0757 loss_train: 0.6722 acc_train: 0.8000 loss_val: 1.1435 acc_val: 0.7600 time: 6.1964s\n",
            "Epoch: 0758 loss_train: 0.7220 acc_train: 0.7800 loss_val: 1.1566 acc_val: 0.7400 time: 6.2030s\n",
            "Epoch: 0759 loss_train: 0.5303 acc_train: 0.8600 loss_val: 1.1774 acc_val: 0.7400 time: 6.2095s\n",
            "Epoch: 0760 loss_train: 0.5951 acc_train: 0.8400 loss_val: 1.2098 acc_val: 0.7400 time: 6.2166s\n",
            "Epoch: 0761 loss_train: 0.6236 acc_train: 0.8400 loss_val: 1.2356 acc_val: 0.7400 time: 6.2243s\n",
            "Epoch: 0762 loss_train: 0.5891 acc_train: 0.8400 loss_val: 1.2661 acc_val: 0.7400 time: 6.2311s\n",
            "Epoch: 0763 loss_train: 0.6686 acc_train: 0.8400 loss_val: 1.2803 acc_val: 0.7400 time: 6.2390s\n",
            "Epoch: 0764 loss_train: 0.6210 acc_train: 0.8200 loss_val: 1.2743 acc_val: 0.7400 time: 6.2478s\n",
            "Epoch: 0765 loss_train: 0.6262 acc_train: 0.8400 loss_val: 1.2462 acc_val: 0.7400 time: 6.2553s\n",
            "Epoch: 0766 loss_train: 0.6572 acc_train: 0.8000 loss_val: 1.2292 acc_val: 0.7400 time: 6.2658s\n",
            "Epoch: 0767 loss_train: 0.6038 acc_train: 0.8400 loss_val: 1.2155 acc_val: 0.7400 time: 6.2728s\n",
            "Epoch: 0768 loss_train: 0.5933 acc_train: 0.8600 loss_val: 1.2104 acc_val: 0.7400 time: 6.2794s\n",
            "Epoch: 0769 loss_train: 0.6066 acc_train: 0.8600 loss_val: 1.2092 acc_val: 0.7400 time: 6.2870s\n",
            "Epoch: 0770 loss_train: 0.5897 acc_train: 0.8400 loss_val: 1.1968 acc_val: 0.7400 time: 6.2947s\n",
            "Epoch: 0771 loss_train: 0.7345 acc_train: 0.7800 loss_val: 1.1935 acc_val: 0.7400 time: 6.3014s\n",
            "Epoch: 0772 loss_train: 0.6499 acc_train: 0.8400 loss_val: 1.2002 acc_val: 0.7400 time: 6.3088s\n",
            "Epoch: 0773 loss_train: 0.6444 acc_train: 0.8200 loss_val: 1.2109 acc_val: 0.7400 time: 6.3162s\n",
            "Epoch: 0774 loss_train: 0.6103 acc_train: 0.8200 loss_val: 1.2297 acc_val: 0.7400 time: 6.3240s\n",
            "Epoch: 0775 loss_train: 0.7079 acc_train: 0.7600 loss_val: 1.2530 acc_val: 0.7400 time: 6.3315s\n",
            "Epoch: 0776 loss_train: 0.7689 acc_train: 0.7800 loss_val: 1.2771 acc_val: 0.7400 time: 6.3415s\n",
            "Epoch: 0777 loss_train: 0.6739 acc_train: 0.8000 loss_val: 1.2658 acc_val: 0.7400 time: 6.3483s\n",
            "Epoch: 0778 loss_train: 0.6040 acc_train: 0.8600 loss_val: 1.2420 acc_val: 0.7400 time: 6.3558s\n",
            "Epoch: 0779 loss_train: 0.5845 acc_train: 0.8400 loss_val: 1.2050 acc_val: 0.7400 time: 6.3633s\n",
            "Epoch: 0780 loss_train: 0.6080 acc_train: 0.8400 loss_val: 1.1850 acc_val: 0.7400 time: 6.3701s\n",
            "Epoch: 0781 loss_train: 0.6185 acc_train: 0.8600 loss_val: 1.1690 acc_val: 0.7400 time: 6.3785s\n",
            "Epoch: 0782 loss_train: 0.5700 acc_train: 0.8600 loss_val: 1.1617 acc_val: 0.7400 time: 6.3840s\n",
            "Epoch: 0783 loss_train: 0.6636 acc_train: 0.8000 loss_val: 1.1566 acc_val: 0.7400 time: 6.3958s\n",
            "Epoch: 0784 loss_train: 0.6154 acc_train: 0.8400 loss_val: 1.1546 acc_val: 0.7400 time: 6.4028s\n",
            "Epoch: 0785 loss_train: 0.5814 acc_train: 0.8400 loss_val: 1.1614 acc_val: 0.7400 time: 6.4095s\n",
            "Epoch: 0786 loss_train: 0.5968 acc_train: 0.8400 loss_val: 1.1714 acc_val: 0.7400 time: 6.4164s\n",
            "Epoch: 0787 loss_train: 0.6561 acc_train: 0.8200 loss_val: 1.1793 acc_val: 0.7400 time: 6.4235s\n",
            "Epoch: 0788 loss_train: 0.7151 acc_train: 0.8000 loss_val: 1.1866 acc_val: 0.7400 time: 6.4306s\n",
            "Epoch: 0789 loss_train: 0.6620 acc_train: 0.8200 loss_val: 1.1950 acc_val: 0.7400 time: 6.4376s\n",
            "Epoch: 0790 loss_train: 0.6469 acc_train: 0.8200 loss_val: 1.2140 acc_val: 0.7400 time: 6.4454s\n",
            "Epoch: 0791 loss_train: 0.5478 acc_train: 0.8800 loss_val: 1.2356 acc_val: 0.7400 time: 6.4535s\n",
            "Epoch: 0792 loss_train: 0.6482 acc_train: 0.8400 loss_val: 1.2569 acc_val: 0.7400 time: 6.4604s\n",
            "Epoch: 0793 loss_train: 0.6893 acc_train: 0.8200 loss_val: 1.2908 acc_val: 0.7400 time: 6.4682s\n",
            "Epoch: 0794 loss_train: 0.5572 acc_train: 0.8600 loss_val: 1.3201 acc_val: 0.7400 time: 6.4767s\n",
            "Epoch: 0795 loss_train: 0.6456 acc_train: 0.8000 loss_val: 1.3479 acc_val: 0.7400 time: 6.4851s\n",
            "Epoch: 0796 loss_train: 0.5726 acc_train: 0.8400 loss_val: 1.3780 acc_val: 0.7400 time: 6.4928s\n",
            "Epoch: 0797 loss_train: 0.6267 acc_train: 0.8600 loss_val: 1.3961 acc_val: 0.7400 time: 6.5006s\n",
            "Epoch: 0798 loss_train: 0.6251 acc_train: 0.8400 loss_val: 1.3236 acc_val: 0.7400 time: 6.5076s\n",
            "Epoch: 0799 loss_train: 0.6489 acc_train: 0.8200 loss_val: 1.2585 acc_val: 0.7400 time: 6.5195s\n",
            "Epoch: 0800 loss_train: 0.6432 acc_train: 0.8400 loss_val: 1.2111 acc_val: 0.7400 time: 6.5282s\n",
            "Epoch: 0801 loss_train: 0.7588 acc_train: 0.8000 loss_val: 1.1648 acc_val: 0.7600 time: 6.5353s\n",
            "Epoch: 0802 loss_train: 0.5776 acc_train: 0.8400 loss_val: 1.1410 acc_val: 0.7600 time: 6.5430s\n",
            "Epoch: 0803 loss_train: 0.6623 acc_train: 0.8200 loss_val: 1.1317 acc_val: 0.7800 time: 6.5500s\n",
            "Epoch: 0804 loss_train: 0.6978 acc_train: 0.8200 loss_val: 1.1253 acc_val: 0.7800 time: 6.5578s\n",
            "Epoch: 0805 loss_train: 0.5502 acc_train: 0.8600 loss_val: 1.1237 acc_val: 0.7800 time: 6.5648s\n",
            "Epoch: 0806 loss_train: 0.6645 acc_train: 0.8200 loss_val: 1.1239 acc_val: 0.7800 time: 6.5716s\n",
            "Epoch: 0807 loss_train: 0.6166 acc_train: 0.8600 loss_val: 1.1255 acc_val: 0.7800 time: 6.5786s\n",
            "Epoch: 0808 loss_train: 0.6001 acc_train: 0.8400 loss_val: 1.1277 acc_val: 0.7800 time: 6.5853s\n",
            "Epoch: 0809 loss_train: 0.6269 acc_train: 0.8200 loss_val: 1.1338 acc_val: 0.7600 time: 6.5950s\n",
            "Epoch: 0810 loss_train: 0.6250 acc_train: 0.8200 loss_val: 1.1375 acc_val: 0.7600 time: 6.6055s\n",
            "Epoch: 0811 loss_train: 0.7137 acc_train: 0.7800 loss_val: 1.1224 acc_val: 0.7800 time: 6.6135s\n",
            "Epoch: 0812 loss_train: 0.5791 acc_train: 0.8400 loss_val: 1.1209 acc_val: 0.7800 time: 6.6205s\n",
            "Epoch: 0813 loss_train: 0.6062 acc_train: 0.8400 loss_val: 1.1207 acc_val: 0.7800 time: 6.6281s\n",
            "Epoch: 0814 loss_train: 0.6379 acc_train: 0.8000 loss_val: 1.1205 acc_val: 0.7800 time: 6.6347s\n",
            "Epoch: 0815 loss_train: 0.5739 acc_train: 0.8400 loss_val: 1.1242 acc_val: 0.7600 time: 6.6421s\n",
            "Epoch: 0816 loss_train: 0.6940 acc_train: 0.8000 loss_val: 1.1304 acc_val: 0.7600 time: 6.6485s\n",
            "Epoch: 0817 loss_train: 0.6521 acc_train: 0.8400 loss_val: 1.1406 acc_val: 0.7600 time: 6.6550s\n",
            "Epoch: 0818 loss_train: 0.5753 acc_train: 0.8400 loss_val: 1.1607 acc_val: 0.7600 time: 6.6626s\n",
            "Epoch: 0819 loss_train: 0.6835 acc_train: 0.8200 loss_val: 1.1863 acc_val: 0.7400 time: 6.6702s\n",
            "Epoch: 0820 loss_train: 0.6392 acc_train: 0.8000 loss_val: 1.2085 acc_val: 0.7400 time: 6.6780s\n",
            "Epoch: 0821 loss_train: 0.6430 acc_train: 0.8200 loss_val: 1.2315 acc_val: 0.7400 time: 6.6868s\n",
            "Epoch: 0822 loss_train: 0.6558 acc_train: 0.7800 loss_val: 1.1734 acc_val: 0.7600 time: 6.6949s\n",
            "Epoch: 0823 loss_train: 0.6238 acc_train: 0.8200 loss_val: 1.1479 acc_val: 0.7600 time: 6.7012s\n",
            "Epoch: 0824 loss_train: 0.6985 acc_train: 0.8000 loss_val: 1.1352 acc_val: 0.7600 time: 6.7084s\n",
            "Epoch: 0825 loss_train: 0.5651 acc_train: 0.8400 loss_val: 1.1412 acc_val: 0.7600 time: 6.7148s\n",
            "Epoch: 0826 loss_train: 0.6422 acc_train: 0.8400 loss_val: 1.1485 acc_val: 0.7600 time: 6.7212s\n",
            "Epoch: 0827 loss_train: 0.6392 acc_train: 0.8200 loss_val: 1.1614 acc_val: 0.7400 time: 6.7277s\n",
            "Epoch: 0828 loss_train: 0.6708 acc_train: 0.8200 loss_val: 1.1726 acc_val: 0.7400 time: 6.7356s\n",
            "Epoch: 0829 loss_train: 0.6116 acc_train: 0.8400 loss_val: 1.1699 acc_val: 0.7400 time: 6.7427s\n",
            "Epoch: 0830 loss_train: 0.6204 acc_train: 0.8400 loss_val: 1.1670 acc_val: 0.7400 time: 6.7506s\n",
            "Epoch: 0831 loss_train: 0.6263 acc_train: 0.8400 loss_val: 1.1621 acc_val: 0.7400 time: 6.7592s\n",
            "Epoch: 0832 loss_train: 0.5785 acc_train: 0.8600 loss_val: 1.1603 acc_val: 0.7400 time: 6.7684s\n",
            "Epoch: 0833 loss_train: 0.6745 acc_train: 0.8000 loss_val: 1.1530 acc_val: 0.7400 time: 6.7752s\n",
            "Epoch: 0834 loss_train: 0.5266 acc_train: 0.9000 loss_val: 1.1527 acc_val: 0.7400 time: 6.7821s\n",
            "Epoch: 0835 loss_train: 0.6975 acc_train: 0.8200 loss_val: 1.1499 acc_val: 0.7600 time: 6.7896s\n",
            "Epoch: 0836 loss_train: 0.5710 acc_train: 0.8600 loss_val: 1.1478 acc_val: 0.7600 time: 6.7975s\n",
            "Epoch: 0837 loss_train: 0.6544 acc_train: 0.8400 loss_val: 1.1435 acc_val: 0.7600 time: 6.8119s\n",
            "Epoch: 0838 loss_train: 0.5967 acc_train: 0.8600 loss_val: 1.1414 acc_val: 0.7600 time: 6.8188s\n",
            "Epoch: 0839 loss_train: 0.6066 acc_train: 0.8400 loss_val: 1.1458 acc_val: 0.7600 time: 6.8266s\n",
            "Epoch: 0840 loss_train: 0.6400 acc_train: 0.8600 loss_val: 1.1551 acc_val: 0.7600 time: 6.8341s\n",
            "Epoch: 0841 loss_train: 0.6652 acc_train: 0.8000 loss_val: 1.1653 acc_val: 0.7600 time: 6.8408s\n",
            "Epoch: 0842 loss_train: 0.6251 acc_train: 0.8000 loss_val: 1.1703 acc_val: 0.7600 time: 6.8491s\n",
            "Epoch: 0843 loss_train: 0.5694 acc_train: 0.8600 loss_val: 1.1801 acc_val: 0.7600 time: 6.8590s\n",
            "Epoch: 0844 loss_train: 0.6929 acc_train: 0.8200 loss_val: 1.1917 acc_val: 0.7600 time: 6.8672s\n",
            "Epoch: 0845 loss_train: 0.6015 acc_train: 0.8600 loss_val: 1.2036 acc_val: 0.7600 time: 6.8744s\n",
            "Epoch: 0846 loss_train: 0.6692 acc_train: 0.8000 loss_val: 1.2207 acc_val: 0.7400 time: 6.8813s\n",
            "Epoch: 0847 loss_train: 0.6986 acc_train: 0.8000 loss_val: 1.2378 acc_val: 0.7400 time: 6.8883s\n",
            "Epoch: 0848 loss_train: 0.6097 acc_train: 0.8600 loss_val: 1.2454 acc_val: 0.7400 time: 6.8970s\n",
            "Epoch: 0849 loss_train: 0.6409 acc_train: 0.8400 loss_val: 1.2415 acc_val: 0.7400 time: 6.9063s\n",
            "Epoch: 0850 loss_train: 0.5666 acc_train: 0.8600 loss_val: 1.2432 acc_val: 0.7400 time: 6.9139s\n",
            "Epoch: 0851 loss_train: 0.6292 acc_train: 0.8400 loss_val: 1.2363 acc_val: 0.7400 time: 6.9220s\n",
            "Epoch: 0852 loss_train: 0.6318 acc_train: 0.8200 loss_val: 1.2179 acc_val: 0.7400 time: 6.9291s\n",
            "Epoch: 0853 loss_train: 0.6780 acc_train: 0.8400 loss_val: 1.1968 acc_val: 0.7400 time: 6.9394s\n",
            "Epoch: 0854 loss_train: 0.7338 acc_train: 0.8000 loss_val: 1.1794 acc_val: 0.7600 time: 6.9501s\n",
            "Epoch: 0855 loss_train: 0.6388 acc_train: 0.8200 loss_val: 1.1616 acc_val: 0.7600 time: 6.9584s\n",
            "Epoch: 0856 loss_train: 0.5590 acc_train: 0.8600 loss_val: 1.1538 acc_val: 0.7600 time: 6.9660s\n",
            "Epoch: 0857 loss_train: 0.5969 acc_train: 0.8400 loss_val: 1.1591 acc_val: 0.7600 time: 6.9753s\n",
            "Epoch: 0858 loss_train: 0.6408 acc_train: 0.8000 loss_val: 1.1802 acc_val: 0.7400 time: 6.9845s\n",
            "Epoch: 0859 loss_train: 0.6766 acc_train: 0.8200 loss_val: 1.2176 acc_val: 0.7400 time: 6.9922s\n",
            "Epoch: 0860 loss_train: 0.6026 acc_train: 0.8200 loss_val: 1.2217 acc_val: 0.7400 time: 7.0015s\n",
            "Epoch: 0861 loss_train: 0.6124 acc_train: 0.8400 loss_val: 1.2042 acc_val: 0.7400 time: 7.0099s\n",
            "Epoch: 0862 loss_train: 0.6144 acc_train: 0.8600 loss_val: 1.1805 acc_val: 0.7400 time: 7.0215s\n",
            "Epoch: 0863 loss_train: 0.6093 acc_train: 0.8400 loss_val: 1.1612 acc_val: 0.7600 time: 7.0312s\n",
            "Epoch: 0864 loss_train: 0.5902 acc_train: 0.8400 loss_val: 1.1506 acc_val: 0.7600 time: 7.0396s\n",
            "Epoch: 0865 loss_train: 0.5437 acc_train: 0.8600 loss_val: 1.1459 acc_val: 0.7600 time: 7.0481s\n",
            "Epoch: 0866 loss_train: 0.5394 acc_train: 0.8600 loss_val: 1.1476 acc_val: 0.7600 time: 7.0558s\n",
            "Epoch: 0867 loss_train: 0.5776 acc_train: 0.8600 loss_val: 1.1497 acc_val: 0.7600 time: 7.0642s\n",
            "Epoch: 0868 loss_train: 0.6263 acc_train: 0.8000 loss_val: 1.1525 acc_val: 0.7600 time: 7.0723s\n",
            "Epoch: 0869 loss_train: 0.6610 acc_train: 0.8200 loss_val: 1.1529 acc_val: 0.7600 time: 7.0793s\n",
            "Epoch: 0870 loss_train: 0.6137 acc_train: 0.8400 loss_val: 1.1553 acc_val: 0.7600 time: 7.0861s\n",
            "Epoch: 0871 loss_train: 0.6316 acc_train: 0.8400 loss_val: 1.1610 acc_val: 0.7600 time: 7.0936s\n",
            "Epoch: 0872 loss_train: 0.6534 acc_train: 0.8400 loss_val: 1.1667 acc_val: 0.7600 time: 7.1008s\n",
            "Epoch: 0873 loss_train: 0.6103 acc_train: 0.8000 loss_val: 1.1750 acc_val: 0.7600 time: 7.1086s\n",
            "Epoch: 0874 loss_train: 0.5795 acc_train: 0.8400 loss_val: 1.1917 acc_val: 0.7400 time: 7.1189s\n",
            "Epoch: 0875 loss_train: 0.6479 acc_train: 0.8400 loss_val: 1.2047 acc_val: 0.7400 time: 7.1266s\n",
            "Epoch: 0876 loss_train: 0.6890 acc_train: 0.8000 loss_val: 1.1846 acc_val: 0.7400 time: 7.1349s\n",
            "Epoch: 0877 loss_train: 0.6020 acc_train: 0.8200 loss_val: 1.1788 acc_val: 0.7400 time: 7.1423s\n",
            "Epoch: 0878 loss_train: 0.6297 acc_train: 0.8200 loss_val: 1.1756 acc_val: 0.7400 time: 7.1493s\n",
            "Epoch: 0879 loss_train: 0.6308 acc_train: 0.8600 loss_val: 1.1764 acc_val: 0.7400 time: 7.1556s\n",
            "Epoch: 0880 loss_train: 0.6366 acc_train: 0.8400 loss_val: 1.1824 acc_val: 0.7400 time: 7.1626s\n",
            "Epoch: 0881 loss_train: 0.6229 acc_train: 0.8000 loss_val: 1.2094 acc_val: 0.7400 time: 7.1691s\n",
            "Epoch: 0882 loss_train: 0.5529 acc_train: 0.8600 loss_val: 1.2395 acc_val: 0.7400 time: 7.1755s\n",
            "Epoch: 0883 loss_train: 0.6936 acc_train: 0.8000 loss_val: 1.2764 acc_val: 0.7400 time: 7.1824s\n",
            "Epoch: 0884 loss_train: 0.5942 acc_train: 0.8400 loss_val: 1.3150 acc_val: 0.7400 time: 7.1918s\n",
            "Epoch: 0885 loss_train: 0.6438 acc_train: 0.8400 loss_val: 1.3468 acc_val: 0.7400 time: 7.1990s\n",
            "Epoch: 0886 loss_train: 0.6722 acc_train: 0.8200 loss_val: 1.3719 acc_val: 0.7400 time: 7.2068s\n",
            "Epoch: 0887 loss_train: 0.6642 acc_train: 0.8200 loss_val: 1.3793 acc_val: 0.7400 time: 7.2136s\n",
            "Epoch: 0888 loss_train: 0.5755 acc_train: 0.8800 loss_val: 1.3026 acc_val: 0.7400 time: 7.2200s\n",
            "Epoch: 0889 loss_train: 0.6541 acc_train: 0.8200 loss_val: 1.2330 acc_val: 0.7400 time: 7.2303s\n",
            "Epoch: 0890 loss_train: 0.5975 acc_train: 0.8400 loss_val: 1.1708 acc_val: 0.7400 time: 7.2421s\n",
            "Epoch: 0891 loss_train: 0.5900 acc_train: 0.8400 loss_val: 1.1440 acc_val: 0.7600 time: 7.2502s\n",
            "Epoch: 0892 loss_train: 0.6258 acc_train: 0.8200 loss_val: 1.1336 acc_val: 0.7800 time: 7.2575s\n",
            "Epoch: 0893 loss_train: 0.6795 acc_train: 0.8000 loss_val: 1.1303 acc_val: 0.7800 time: 7.2656s\n",
            "Epoch: 0894 loss_train: 0.6924 acc_train: 0.8200 loss_val: 1.1301 acc_val: 0.7800 time: 7.2725s\n",
            "Epoch: 0895 loss_train: 0.6836 acc_train: 0.8000 loss_val: 1.1354 acc_val: 0.7600 time: 7.2795s\n",
            "Epoch: 0896 loss_train: 0.6457 acc_train: 0.8200 loss_val: 1.1515 acc_val: 0.7400 time: 7.2867s\n",
            "Epoch: 0897 loss_train: 0.6588 acc_train: 0.8200 loss_val: 1.1900 acc_val: 0.7400 time: 7.2938s\n",
            "Epoch: 0898 loss_train: 0.6201 acc_train: 0.8200 loss_val: 1.2353 acc_val: 0.7400 time: 7.3007s\n",
            "Epoch: 0899 loss_train: 0.6494 acc_train: 0.8200 loss_val: 1.2646 acc_val: 0.7400 time: 7.3073s\n",
            "Epoch: 0900 loss_train: 0.5734 acc_train: 0.8600 loss_val: 1.2981 acc_val: 0.7400 time: 7.3141s\n",
            "Epoch: 0901 loss_train: 0.6020 acc_train: 0.8600 loss_val: 1.3322 acc_val: 0.7400 time: 7.3230s\n",
            "Epoch: 0902 loss_train: 0.5707 acc_train: 0.8600 loss_val: 1.3424 acc_val: 0.7400 time: 7.3331s\n",
            "Epoch: 0903 loss_train: 0.5446 acc_train: 0.8600 loss_val: 1.3067 acc_val: 0.7400 time: 7.3398s\n",
            "Epoch: 0904 loss_train: 0.6511 acc_train: 0.8200 loss_val: 1.2798 acc_val: 0.7400 time: 7.3483s\n",
            "Epoch: 0905 loss_train: 0.5826 acc_train: 0.8400 loss_val: 1.2653 acc_val: 0.7400 time: 7.3567s\n",
            "Epoch: 0906 loss_train: 0.5940 acc_train: 0.8400 loss_val: 1.2445 acc_val: 0.7400 time: 7.3648s\n",
            "Epoch: 0907 loss_train: 0.6594 acc_train: 0.8600 loss_val: 1.1828 acc_val: 0.7400 time: 7.3738s\n",
            "Epoch: 0908 loss_train: 0.6715 acc_train: 0.8000 loss_val: 1.1537 acc_val: 0.7600 time: 7.3806s\n",
            "Epoch: 0909 loss_train: 0.5965 acc_train: 0.8400 loss_val: 1.1436 acc_val: 0.7800 time: 7.3889s\n",
            "Epoch: 0910 loss_train: 0.6730 acc_train: 0.8200 loss_val: 1.1381 acc_val: 0.7800 time: 7.3955s\n",
            "Epoch: 0911 loss_train: 0.5425 acc_train: 0.8800 loss_val: 1.1348 acc_val: 0.7800 time: 7.4028s\n",
            "Epoch: 0912 loss_train: 0.6116 acc_train: 0.8400 loss_val: 1.1334 acc_val: 0.7800 time: 7.4104s\n",
            "Epoch: 0913 loss_train: 0.6500 acc_train: 0.8200 loss_val: 1.1332 acc_val: 0.7800 time: 7.4174s\n",
            "Epoch: 0914 loss_train: 0.6386 acc_train: 0.8400 loss_val: 1.1374 acc_val: 0.7800 time: 7.4249s\n",
            "Epoch: 0915 loss_train: 0.5789 acc_train: 0.8400 loss_val: 1.1538 acc_val: 0.7600 time: 7.4326s\n",
            "Epoch: 0916 loss_train: 0.6255 acc_train: 0.8200 loss_val: 1.1847 acc_val: 0.7400 time: 7.4412s\n",
            "Epoch: 0917 loss_train: 0.6572 acc_train: 0.8400 loss_val: 1.2333 acc_val: 0.7400 time: 7.4480s\n",
            "Epoch: 0918 loss_train: 0.6170 acc_train: 0.8400 loss_val: 1.2650 acc_val: 0.7400 time: 7.4534s\n",
            "Epoch: 0919 loss_train: 0.5798 acc_train: 0.8600 loss_val: 1.2952 acc_val: 0.7400 time: 7.4687s\n",
            "Epoch: 0920 loss_train: 0.5825 acc_train: 0.8800 loss_val: 1.3215 acc_val: 0.7400 time: 7.4763s\n",
            "Epoch: 0921 loss_train: 0.6156 acc_train: 0.8600 loss_val: 1.3441 acc_val: 0.7400 time: 7.4850s\n",
            "Epoch: 0922 loss_train: 0.5983 acc_train: 0.8400 loss_val: 1.3352 acc_val: 0.7400 time: 7.4932s\n",
            "Epoch: 0923 loss_train: 0.5756 acc_train: 0.8600 loss_val: 1.3307 acc_val: 0.7400 time: 7.5020s\n",
            "Epoch: 0924 loss_train: 0.5945 acc_train: 0.8600 loss_val: 1.3171 acc_val: 0.7400 time: 7.5089s\n",
            "Epoch: 0925 loss_train: 0.5561 acc_train: 0.8600 loss_val: 1.3080 acc_val: 0.7400 time: 7.5170s\n",
            "Epoch: 0926 loss_train: 0.6191 acc_train: 0.8400 loss_val: 1.2830 acc_val: 0.7400 time: 7.5249s\n",
            "Epoch: 0927 loss_train: 0.6275 acc_train: 0.8400 loss_val: 1.2712 acc_val: 0.7400 time: 7.5328s\n",
            "Epoch: 0928 loss_train: 0.7399 acc_train: 0.8000 loss_val: 1.2635 acc_val: 0.7400 time: 7.5437s\n",
            "Epoch: 0929 loss_train: 0.5923 acc_train: 0.8400 loss_val: 1.2538 acc_val: 0.7400 time: 7.5540s\n",
            "Epoch: 0930 loss_train: 0.6035 acc_train: 0.8200 loss_val: 1.2574 acc_val: 0.7400 time: 7.5638s\n",
            "Epoch: 0931 loss_train: 0.6160 acc_train: 0.8000 loss_val: 1.2544 acc_val: 0.7400 time: 7.5714s\n",
            "Epoch: 0932 loss_train: 0.6271 acc_train: 0.8400 loss_val: 1.2566 acc_val: 0.7400 time: 7.5767s\n",
            "Epoch: 0933 loss_train: 0.6618 acc_train: 0.8200 loss_val: 1.2446 acc_val: 0.7400 time: 7.5839s\n",
            "Epoch: 0934 loss_train: 0.7230 acc_train: 0.7800 loss_val: 1.2333 acc_val: 0.7400 time: 7.5960s\n",
            "Epoch: 0935 loss_train: 0.6025 acc_train: 0.8400 loss_val: 1.2198 acc_val: 0.7400 time: 7.6040s\n",
            "Epoch: 0936 loss_train: 0.5857 acc_train: 0.8400 loss_val: 1.2095 acc_val: 0.7400 time: 7.6120s\n",
            "Epoch: 0937 loss_train: 0.6017 acc_train: 0.8600 loss_val: 1.2110 acc_val: 0.7400 time: 7.6205s\n",
            "Epoch: 0938 loss_train: 0.6446 acc_train: 0.8200 loss_val: 1.2095 acc_val: 0.7400 time: 7.6276s\n",
            "Epoch: 0939 loss_train: 0.5817 acc_train: 0.8600 loss_val: 1.2164 acc_val: 0.7400 time: 7.6364s\n",
            "Epoch: 0940 loss_train: 0.6826 acc_train: 0.8400 loss_val: 1.2167 acc_val: 0.7400 time: 7.6438s\n",
            "Epoch: 0941 loss_train: 0.6413 acc_train: 0.8000 loss_val: 1.2313 acc_val: 0.7400 time: 7.6506s\n",
            "Epoch: 0942 loss_train: 0.6665 acc_train: 0.8400 loss_val: 1.2471 acc_val: 0.7400 time: 7.6575s\n",
            "Epoch: 0943 loss_train: 0.6543 acc_train: 0.8000 loss_val: 1.2505 acc_val: 0.7400 time: 7.6640s\n",
            "Epoch: 0944 loss_train: 0.5836 acc_train: 0.8200 loss_val: 1.2716 acc_val: 0.7400 time: 7.6792s\n",
            "Epoch: 0945 loss_train: 0.5596 acc_train: 0.8600 loss_val: 1.2853 acc_val: 0.7400 time: 7.6895s\n",
            "Epoch: 0946 loss_train: 0.6315 acc_train: 0.8400 loss_val: 1.2924 acc_val: 0.7400 time: 7.6974s\n",
            "Epoch: 0947 loss_train: 0.6260 acc_train: 0.8400 loss_val: 1.2673 acc_val: 0.7400 time: 7.7052s\n",
            "Epoch: 0948 loss_train: 0.7104 acc_train: 0.8200 loss_val: 1.2605 acc_val: 0.7400 time: 7.7128s\n",
            "Epoch: 0949 loss_train: 0.6536 acc_train: 0.8200 loss_val: 1.2417 acc_val: 0.7400 time: 7.7201s\n",
            "Epoch: 0950 loss_train: 0.5849 acc_train: 0.8400 loss_val: 1.2444 acc_val: 0.7400 time: 7.7274s\n",
            "Epoch: 0951 loss_train: 0.6636 acc_train: 0.8200 loss_val: 1.2599 acc_val: 0.7400 time: 7.7347s\n",
            "Epoch: 0952 loss_train: 0.6580 acc_train: 0.7800 loss_val: 1.2788 acc_val: 0.7400 time: 7.7420s\n",
            "Epoch: 0953 loss_train: 0.6279 acc_train: 0.8400 loss_val: 1.3012 acc_val: 0.7400 time: 7.7494s\n",
            "Epoch: 0954 loss_train: 0.5546 acc_train: 0.8600 loss_val: 1.3247 acc_val: 0.7400 time: 7.7577s\n",
            "Epoch: 0955 loss_train: 0.6979 acc_train: 0.8200 loss_val: 1.3534 acc_val: 0.7400 time: 7.7678s\n",
            "Epoch: 0956 loss_train: 0.5813 acc_train: 0.8600 loss_val: 1.3642 acc_val: 0.7400 time: 7.7747s\n",
            "Epoch: 0957 loss_train: 0.6336 acc_train: 0.8400 loss_val: 1.3846 acc_val: 0.7400 time: 7.7818s\n",
            "Epoch: 0958 loss_train: 0.6415 acc_train: 0.8200 loss_val: 1.2283 acc_val: 0.7400 time: 7.7894s\n",
            "Epoch: 0959 loss_train: 0.6432 acc_train: 0.8400 loss_val: 1.1412 acc_val: 0.7600 time: 7.7971s\n",
            "Epoch: 0960 loss_train: 0.6789 acc_train: 0.8400 loss_val: 1.1228 acc_val: 0.7800 time: 7.8049s\n",
            "Epoch: 0961 loss_train: 0.6785 acc_train: 0.8600 loss_val: 1.1219 acc_val: 0.7800 time: 7.8132s\n",
            "Epoch: 0962 loss_train: 0.6293 acc_train: 0.8400 loss_val: 1.1217 acc_val: 0.7800 time: 7.8200s\n",
            "Epoch: 0963 loss_train: 0.6158 acc_train: 0.8400 loss_val: 1.1210 acc_val: 0.7800 time: 7.8265s\n",
            "Epoch: 0964 loss_train: 0.6389 acc_train: 0.8200 loss_val: 1.1206 acc_val: 0.7800 time: 7.8329s\n",
            "Epoch: 0965 loss_train: 0.7101 acc_train: 0.8200 loss_val: 1.1198 acc_val: 0.7800 time: 7.8401s\n",
            "Epoch: 0966 loss_train: 0.7009 acc_train: 0.8200 loss_val: 1.1198 acc_val: 0.7800 time: 7.8472s\n",
            "Epoch: 0967 loss_train: 0.6395 acc_train: 0.8400 loss_val: 1.1229 acc_val: 0.7800 time: 7.8540s\n",
            "Epoch: 0968 loss_train: 0.6147 acc_train: 0.8200 loss_val: 1.1364 acc_val: 0.7600 time: 7.8608s\n",
            "Epoch: 0969 loss_train: 0.5866 acc_train: 0.8600 loss_val: 1.1660 acc_val: 0.7600 time: 7.8672s\n",
            "Epoch: 0970 loss_train: 0.6751 acc_train: 0.8000 loss_val: 1.2239 acc_val: 0.7400 time: 7.8796s\n",
            "Epoch: 0971 loss_train: 0.5888 acc_train: 0.8600 loss_val: 1.2628 acc_val: 0.7400 time: 7.8881s\n",
            "Epoch: 0972 loss_train: 0.5769 acc_train: 0.8600 loss_val: 1.2898 acc_val: 0.7400 time: 7.8953s\n",
            "Epoch: 0973 loss_train: 0.6031 acc_train: 0.8400 loss_val: 1.3051 acc_val: 0.7400 time: 7.9020s\n",
            "Epoch: 0974 loss_train: 0.6038 acc_train: 0.8600 loss_val: 1.2570 acc_val: 0.7400 time: 7.9093s\n",
            "Epoch: 0975 loss_train: 0.6332 acc_train: 0.8400 loss_val: 1.2080 acc_val: 0.7400 time: 7.9159s\n",
            "Epoch: 0976 loss_train: 0.6252 acc_train: 0.8400 loss_val: 1.1756 acc_val: 0.7600 time: 7.9239s\n",
            "Epoch: 0977 loss_train: 0.6061 acc_train: 0.8000 loss_val: 1.1602 acc_val: 0.7600 time: 7.9316s\n",
            "Epoch: 0978 loss_train: 0.6734 acc_train: 0.8200 loss_val: 1.1504 acc_val: 0.7600 time: 7.9383s\n",
            "Epoch: 0979 loss_train: 0.5767 acc_train: 0.8600 loss_val: 1.1477 acc_val: 0.7600 time: 7.9462s\n",
            "Epoch: 0980 loss_train: 0.5652 acc_train: 0.8600 loss_val: 1.1477 acc_val: 0.7600 time: 7.9540s\n",
            "Epoch: 0981 loss_train: 0.6424 acc_train: 0.8400 loss_val: 1.1483 acc_val: 0.7600 time: 7.9620s\n",
            "Epoch: 0982 loss_train: 0.6448 acc_train: 0.8200 loss_val: 1.1521 acc_val: 0.7600 time: 7.9722s\n",
            "Epoch: 0983 loss_train: 0.6084 acc_train: 0.8400 loss_val: 1.1612 acc_val: 0.7600 time: 7.9795s\n",
            "Epoch: 0984 loss_train: 0.5575 acc_train: 0.8400 loss_val: 1.1840 acc_val: 0.7600 time: 7.9862s\n",
            "Epoch: 0985 loss_train: 0.5944 acc_train: 0.8600 loss_val: 1.2069 acc_val: 0.7400 time: 7.9935s\n",
            "Epoch: 0986 loss_train: 0.6644 acc_train: 0.8000 loss_val: 1.2463 acc_val: 0.7400 time: 8.0005s\n",
            "Epoch: 0987 loss_train: 0.5906 acc_train: 0.8400 loss_val: 1.2880 acc_val: 0.7400 time: 8.0080s\n",
            "Epoch: 0988 loss_train: 0.5948 acc_train: 0.8400 loss_val: 1.3296 acc_val: 0.7400 time: 8.0150s\n",
            "Epoch: 0989 loss_train: 0.6455 acc_train: 0.7800 loss_val: 1.3658 acc_val: 0.7200 time: 8.0217s\n",
            "Epoch: 0990 loss_train: 0.6126 acc_train: 0.8400 loss_val: 1.3840 acc_val: 0.7200 time: 8.0283s\n",
            "Epoch: 0991 loss_train: 0.5796 acc_train: 0.8800 loss_val: 1.3830 acc_val: 0.7200 time: 8.0349s\n",
            "Epoch: 0992 loss_train: 0.5818 acc_train: 0.8600 loss_val: 1.3417 acc_val: 0.7400 time: 8.0420s\n",
            "Epoch: 0993 loss_train: 0.6158 acc_train: 0.8400 loss_val: 1.2905 acc_val: 0.7400 time: 8.0500s\n",
            "Epoch: 0994 loss_train: 0.7169 acc_train: 0.8200 loss_val: 1.2367 acc_val: 0.7400 time: 8.0565s\n",
            "Epoch: 0995 loss_train: 0.6315 acc_train: 0.8200 loss_val: 1.1983 acc_val: 0.7600 time: 8.0629s\n",
            "Epoch: 0996 loss_train: 0.6290 acc_train: 0.8200 loss_val: 1.1810 acc_val: 0.7600 time: 8.0695s\n",
            "Epoch: 0997 loss_train: 0.6760 acc_train: 0.8000 loss_val: 1.1704 acc_val: 0.7600 time: 8.0762s\n",
            "Epoch: 0998 loss_train: 0.5584 acc_train: 0.8400 loss_val: 1.1663 acc_val: 0.7600 time: 8.0862s\n",
            "Epoch: 0999 loss_train: 0.6996 acc_train: 0.8000 loss_val: 1.1738 acc_val: 0.7600 time: 8.0969s\n",
            "Epoch: 1000 loss_train: 0.5850 acc_train: 0.8600 loss_val: 1.1928 acc_val: 0.7400 time: 8.1050s\n",
            "Epoch: 1001 loss_train: 0.5996 acc_train: 0.8600 loss_val: 1.2189 acc_val: 0.7400 time: 8.1138s\n",
            "Epoch: 1002 loss_train: 0.6856 acc_train: 0.8200 loss_val: 1.2443 acc_val: 0.7400 time: 8.1226s\n",
            "Epoch: 1003 loss_train: 0.6226 acc_train: 0.8600 loss_val: 1.2698 acc_val: 0.7400 time: 8.1306s\n",
            "Epoch: 1004 loss_train: 0.6585 acc_train: 0.8000 loss_val: 1.3000 acc_val: 0.7400 time: 8.1373s\n",
            "Epoch: 1005 loss_train: 0.5930 acc_train: 0.8600 loss_val: 1.3213 acc_val: 0.7400 time: 8.1440s\n",
            "Epoch: 1006 loss_train: 0.6236 acc_train: 0.8400 loss_val: 1.3335 acc_val: 0.7400 time: 8.1563s\n",
            "Epoch: 1007 loss_train: 0.6348 acc_train: 0.8200 loss_val: 1.3454 acc_val: 0.7400 time: 8.1647s\n",
            "Epoch: 1008 loss_train: 0.5859 acc_train: 0.8800 loss_val: 1.3384 acc_val: 0.7400 time: 8.1720s\n",
            "Epoch: 1009 loss_train: 0.5957 acc_train: 0.8400 loss_val: 1.3206 acc_val: 0.7400 time: 8.1805s\n",
            "Epoch: 1010 loss_train: 0.5940 acc_train: 0.8600 loss_val: 1.2855 acc_val: 0.7400 time: 8.1881s\n",
            "Epoch: 1011 loss_train: 0.6389 acc_train: 0.8400 loss_val: 1.2631 acc_val: 0.7400 time: 8.1957s\n",
            "Epoch: 1012 loss_train: 0.5925 acc_train: 0.8400 loss_val: 1.2374 acc_val: 0.7400 time: 8.2055s\n",
            "Epoch: 1013 loss_train: 0.6142 acc_train: 0.8400 loss_val: 1.1728 acc_val: 0.7600 time: 8.2148s\n",
            "Epoch: 1014 loss_train: 0.6199 acc_train: 0.8600 loss_val: 1.1404 acc_val: 0.7600 time: 8.2222s\n",
            "Epoch: 1015 loss_train: 0.6323 acc_train: 0.8200 loss_val: 1.1279 acc_val: 0.7800 time: 8.2292s\n",
            "Epoch: 1016 loss_train: 0.6528 acc_train: 0.8200 loss_val: 1.1253 acc_val: 0.7800 time: 8.2371s\n",
            "Epoch: 1017 loss_train: 0.6197 acc_train: 0.8200 loss_val: 1.1237 acc_val: 0.7800 time: 8.2453s\n",
            "Epoch: 1018 loss_train: 0.5888 acc_train: 0.8400 loss_val: 1.1230 acc_val: 0.7800 time: 8.2539s\n",
            "Epoch: 1019 loss_train: 0.5888 acc_train: 0.8600 loss_val: 1.1239 acc_val: 0.7800 time: 8.2590s\n",
            "Epoch: 1020 loss_train: 0.5473 acc_train: 0.8800 loss_val: 1.1265 acc_val: 0.7800 time: 8.2662s\n",
            "Epoch: 1021 loss_train: 0.6927 acc_train: 0.8000 loss_val: 1.1286 acc_val: 0.7800 time: 8.2734s\n",
            "Epoch: 1022 loss_train: 0.6777 acc_train: 0.8000 loss_val: 1.1318 acc_val: 0.7600 time: 8.2806s\n",
            "Epoch: 1023 loss_train: 0.5482 acc_train: 0.8800 loss_val: 1.1475 acc_val: 0.7600 time: 8.2896s\n",
            "Epoch: 1024 loss_train: 0.6290 acc_train: 0.8400 loss_val: 1.1672 acc_val: 0.7600 time: 8.3008s\n",
            "Epoch: 1025 loss_train: 0.5943 acc_train: 0.8400 loss_val: 1.1914 acc_val: 0.7600 time: 8.3111s\n",
            "Epoch: 1026 loss_train: 0.7720 acc_train: 0.8000 loss_val: 1.2223 acc_val: 0.7400 time: 8.3206s\n",
            "Epoch: 1027 loss_train: 0.5742 acc_train: 0.8800 loss_val: 1.2600 acc_val: 0.7400 time: 8.3274s\n",
            "Epoch: 1028 loss_train: 0.5724 acc_train: 0.8800 loss_val: 1.2785 acc_val: 0.7400 time: 8.3340s\n",
            "Epoch: 1029 loss_train: 0.6531 acc_train: 0.8400 loss_val: 1.2873 acc_val: 0.7400 time: 8.3416s\n",
            "Epoch: 1030 loss_train: 0.5145 acc_train: 0.8800 loss_val: 1.2928 acc_val: 0.7400 time: 8.3488s\n",
            "Epoch: 1031 loss_train: 0.5804 acc_train: 0.8600 loss_val: 1.2791 acc_val: 0.7400 time: 8.3568s\n",
            "Epoch: 1032 loss_train: 0.6086 acc_train: 0.8600 loss_val: 1.2568 acc_val: 0.7400 time: 8.3636s\n",
            "Epoch: 1033 loss_train: 0.6475 acc_train: 0.8200 loss_val: 1.2436 acc_val: 0.7400 time: 8.3707s\n",
            "Epoch: 1034 loss_train: 0.7703 acc_train: 0.7800 loss_val: 1.2460 acc_val: 0.7400 time: 8.3773s\n",
            "Epoch: 1035 loss_train: 0.5931 acc_train: 0.8600 loss_val: 1.2211 acc_val: 0.7400 time: 8.3854s\n",
            "Epoch: 1036 loss_train: 0.5960 acc_train: 0.8600 loss_val: 1.2007 acc_val: 0.7600 time: 8.3947s\n",
            "Epoch: 1037 loss_train: 0.5441 acc_train: 0.8400 loss_val: 1.1927 acc_val: 0.7600 time: 8.4030s\n",
            "Epoch: 1038 loss_train: 0.6864 acc_train: 0.8400 loss_val: 1.1674 acc_val: 0.7600 time: 8.4106s\n",
            "Epoch: 1039 loss_train: 0.6299 acc_train: 0.8400 loss_val: 1.1479 acc_val: 0.7600 time: 8.4176s\n",
            "Epoch: 1040 loss_train: 0.6200 acc_train: 0.8400 loss_val: 1.1400 acc_val: 0.7600 time: 8.4242s\n",
            "Epoch: 1041 loss_train: 0.5741 acc_train: 0.8600 loss_val: 1.1365 acc_val: 0.7600 time: 8.4310s\n",
            "Epoch: 1042 loss_train: 0.6372 acc_train: 0.8400 loss_val: 1.1321 acc_val: 0.7600 time: 8.4384s\n",
            "Epoch: 1043 loss_train: 0.6518 acc_train: 0.8400 loss_val: 1.1313 acc_val: 0.7600 time: 8.4461s\n",
            "Epoch: 1044 loss_train: 0.5931 acc_train: 0.8400 loss_val: 1.1357 acc_val: 0.7600 time: 8.4545s\n",
            "Epoch: 1045 loss_train: 0.5784 acc_train: 0.8600 loss_val: 1.1450 acc_val: 0.7600 time: 8.4621s\n",
            "Epoch: 1046 loss_train: 0.6120 acc_train: 0.8200 loss_val: 1.1570 acc_val: 0.7600 time: 8.4696s\n",
            "Epoch: 1047 loss_train: 0.6192 acc_train: 0.8400 loss_val: 1.1751 acc_val: 0.7600 time: 8.4773s\n",
            "Epoch: 1048 loss_train: 0.5943 acc_train: 0.8400 loss_val: 1.1966 acc_val: 0.7400 time: 8.4846s\n",
            "Epoch: 1049 loss_train: 0.6388 acc_train: 0.8400 loss_val: 1.2103 acc_val: 0.7400 time: 8.4928s\n",
            "Epoch: 1050 loss_train: 0.6128 acc_train: 0.8400 loss_val: 1.2167 acc_val: 0.7400 time: 8.4998s\n",
            "Epoch: 1051 loss_train: 0.6099 acc_train: 0.8400 loss_val: 1.2289 acc_val: 0.7400 time: 8.5092s\n",
            "Epoch: 1052 loss_train: 0.5289 acc_train: 0.8600 loss_val: 1.2459 acc_val: 0.7400 time: 8.5149s\n",
            "Epoch: 1053 loss_train: 0.6033 acc_train: 0.8400 loss_val: 1.2665 acc_val: 0.7400 time: 8.5274s\n",
            "Epoch: 1054 loss_train: 0.5481 acc_train: 0.8800 loss_val: 1.2625 acc_val: 0.7400 time: 8.5345s\n",
            "Epoch: 1055 loss_train: 0.5907 acc_train: 0.8600 loss_val: 1.2178 acc_val: 0.7400 time: 8.5417s\n",
            "Epoch: 1056 loss_train: 0.6024 acc_train: 0.8400 loss_val: 1.1804 acc_val: 0.7400 time: 8.5499s\n",
            "Epoch: 1057 loss_train: 0.6606 acc_train: 0.8400 loss_val: 1.1517 acc_val: 0.7600 time: 8.5570s\n",
            "Epoch: 1058 loss_train: 0.5841 acc_train: 0.8600 loss_val: 1.1387 acc_val: 0.7600 time: 8.5639s\n",
            "Epoch: 1059 loss_train: 0.6840 acc_train: 0.8000 loss_val: 1.1317 acc_val: 0.7600 time: 8.5718s\n",
            "Epoch: 1060 loss_train: 0.6286 acc_train: 0.8000 loss_val: 1.1301 acc_val: 0.7600 time: 8.5772s\n",
            "Epoch: 1061 loss_train: 0.5520 acc_train: 0.8600 loss_val: 1.1351 acc_val: 0.7600 time: 8.5841s\n",
            "Epoch: 1062 loss_train: 0.6342 acc_train: 0.8200 loss_val: 1.1511 acc_val: 0.7600 time: 8.5919s\n",
            "Epoch: 1063 loss_train: 0.6181 acc_train: 0.8200 loss_val: 1.1810 acc_val: 0.7400 time: 8.6001s\n",
            "Epoch: 1064 loss_train: 0.5461 acc_train: 0.8400 loss_val: 1.2246 acc_val: 0.7400 time: 8.6081s\n",
            "Epoch: 1065 loss_train: 0.5716 acc_train: 0.8200 loss_val: 1.2678 acc_val: 0.7400 time: 8.6167s\n",
            "Epoch: 1066 loss_train: 0.6450 acc_train: 0.8400 loss_val: 1.3047 acc_val: 0.7400 time: 8.6252s\n",
            "Epoch: 1067 loss_train: 0.6242 acc_train: 0.8600 loss_val: 1.3174 acc_val: 0.7400 time: 8.6337s\n",
            "Epoch: 1068 loss_train: 0.6910 acc_train: 0.8000 loss_val: 1.2622 acc_val: 0.7400 time: 8.6416s\n",
            "Epoch: 1069 loss_train: 0.6169 acc_train: 0.8400 loss_val: 1.2174 acc_val: 0.7400 time: 8.6493s\n",
            "Epoch: 1070 loss_train: 0.6424 acc_train: 0.8400 loss_val: 1.1866 acc_val: 0.7400 time: 8.6565s\n",
            "Epoch: 1071 loss_train: 0.5720 acc_train: 0.8800 loss_val: 1.2032 acc_val: 0.7400 time: 8.6642s\n",
            "Epoch: 1072 loss_train: 0.5870 acc_train: 0.8600 loss_val: 1.2227 acc_val: 0.7400 time: 8.6721s\n",
            "Epoch: 1073 loss_train: 0.6285 acc_train: 0.8200 loss_val: 1.2514 acc_val: 0.7400 time: 8.6809s\n",
            "Epoch: 1074 loss_train: 0.5540 acc_train: 0.8600 loss_val: 1.2791 acc_val: 0.7400 time: 8.6917s\n",
            "Epoch: 1075 loss_train: 0.6622 acc_train: 0.8200 loss_val: 1.2923 acc_val: 0.7400 time: 8.7009s\n",
            "Epoch: 1076 loss_train: 0.5212 acc_train: 0.8800 loss_val: 1.2941 acc_val: 0.7400 time: 8.7091s\n",
            "Epoch: 1077 loss_train: 0.5803 acc_train: 0.8200 loss_val: 1.2575 acc_val: 0.7400 time: 8.7177s\n",
            "Epoch: 1078 loss_train: 0.5729 acc_train: 0.8800 loss_val: 1.2115 acc_val: 0.7400 time: 8.7277s\n",
            "Epoch: 1079 loss_train: 0.6065 acc_train: 0.8400 loss_val: 1.1865 acc_val: 0.7400 time: 8.7333s\n",
            "Epoch: 1080 loss_train: 0.6573 acc_train: 0.8000 loss_val: 1.1752 acc_val: 0.7400 time: 8.7386s\n",
            "Epoch: 1081 loss_train: 0.5600 acc_train: 0.8600 loss_val: 1.1684 acc_val: 0.7600 time: 8.7594s\n",
            "Epoch: 1082 loss_train: 0.5283 acc_train: 0.8600 loss_val: 1.1649 acc_val: 0.7600 time: 8.7668s\n",
            "Epoch: 1083 loss_train: 0.6029 acc_train: 0.8600 loss_val: 1.1647 acc_val: 0.7400 time: 8.7734s\n",
            "Epoch: 1084 loss_train: 0.5435 acc_train: 0.8600 loss_val: 1.1605 acc_val: 0.7600 time: 8.7802s\n",
            "Epoch: 1085 loss_train: 0.5417 acc_train: 0.8600 loss_val: 1.1598 acc_val: 0.7600 time: 8.7876s\n",
            "Epoch: 1086 loss_train: 0.5547 acc_train: 0.8800 loss_val: 1.1738 acc_val: 0.7400 time: 8.7950s\n",
            "Epoch: 1087 loss_train: 0.5366 acc_train: 0.8600 loss_val: 1.2106 acc_val: 0.7400 time: 8.8026s\n",
            "Epoch: 1088 loss_train: 0.5846 acc_train: 0.8400 loss_val: 1.2963 acc_val: 0.7400 time: 8.8104s\n",
            "Epoch: 1089 loss_train: 0.6698 acc_train: 0.8400 loss_val: 1.3820 acc_val: 0.7400 time: 8.8179s\n",
            "Epoch: 1090 loss_train: 0.7041 acc_train: 0.8000 loss_val: 1.4429 acc_val: 0.7400 time: 8.8261s\n",
            "Epoch: 1091 loss_train: 0.6042 acc_train: 0.8200 loss_val: 1.4911 acc_val: 0.7200 time: 8.8347s\n",
            "Epoch: 1092 loss_train: 0.6962 acc_train: 0.8000 loss_val: 1.4607 acc_val: 0.7200 time: 8.8414s\n",
            "Epoch: 1093 loss_train: 0.5875 acc_train: 0.8400 loss_val: 1.4180 acc_val: 0.7400 time: 8.8479s\n",
            "Epoch: 1094 loss_train: 0.6481 acc_train: 0.8600 loss_val: 1.3778 acc_val: 0.7400 time: 8.8549s\n",
            "Epoch: 1095 loss_train: 0.5391 acc_train: 0.8600 loss_val: 1.3282 acc_val: 0.7400 time: 8.8663s\n",
            "Epoch: 1096 loss_train: 0.7178 acc_train: 0.8000 loss_val: 1.2900 acc_val: 0.7400 time: 8.8734s\n",
            "Epoch: 1097 loss_train: 0.5334 acc_train: 0.8600 loss_val: 1.2610 acc_val: 0.7400 time: 8.8799s\n",
            "Epoch: 1098 loss_train: 0.6959 acc_train: 0.8200 loss_val: 1.2383 acc_val: 0.7400 time: 8.8872s\n",
            "Epoch: 1099 loss_train: 0.6157 acc_train: 0.8200 loss_val: 1.2355 acc_val: 0.7400 time: 8.8937s\n",
            "Epoch: 1100 loss_train: 0.5967 acc_train: 0.8200 loss_val: 1.2319 acc_val: 0.7400 time: 8.9003s\n",
            "Epoch: 1101 loss_train: 0.6000 acc_train: 0.8600 loss_val: 1.2379 acc_val: 0.7400 time: 8.9084s\n",
            "Epoch: 1102 loss_train: 0.6833 acc_train: 0.8000 loss_val: 1.2298 acc_val: 0.7400 time: 8.9151s\n",
            "Epoch: 1103 loss_train: 0.5796 acc_train: 0.8600 loss_val: 1.2262 acc_val: 0.7400 time: 8.9225s\n",
            "Epoch: 1104 loss_train: 0.5368 acc_train: 0.8800 loss_val: 1.2387 acc_val: 0.7400 time: 8.9302s\n",
            "Epoch: 1105 loss_train: 0.5530 acc_train: 0.8600 loss_val: 1.2487 acc_val: 0.7400 time: 8.9368s\n",
            "Epoch: 1106 loss_train: 0.7306 acc_train: 0.8200 loss_val: 1.2652 acc_val: 0.7400 time: 8.9454s\n",
            "Epoch: 1107 loss_train: 0.6038 acc_train: 0.8600 loss_val: 1.2889 acc_val: 0.7400 time: 8.9507s\n",
            "Epoch: 1108 loss_train: 0.5381 acc_train: 0.8600 loss_val: 1.3124 acc_val: 0.7400 time: 8.9616s\n",
            "Epoch: 1109 loss_train: 0.5667 acc_train: 0.8800 loss_val: 1.3352 acc_val: 0.7400 time: 8.9691s\n",
            "Epoch: 1110 loss_train: 0.5439 acc_train: 0.8800 loss_val: 1.3291 acc_val: 0.7400 time: 8.9774s\n",
            "Epoch: 1111 loss_train: 0.5933 acc_train: 0.8600 loss_val: 1.2439 acc_val: 0.7400 time: 8.9856s\n",
            "Epoch: 1112 loss_train: 0.5925 acc_train: 0.8600 loss_val: 1.1841 acc_val: 0.7600 time: 8.9937s\n",
            "Epoch: 1113 loss_train: 0.6137 acc_train: 0.8600 loss_val: 1.1612 acc_val: 0.7600 time: 9.0009s\n",
            "Epoch: 1114 loss_train: 0.6018 acc_train: 0.8400 loss_val: 1.1536 acc_val: 0.7800 time: 9.0075s\n",
            "Epoch: 1115 loss_train: 0.6136 acc_train: 0.8200 loss_val: 1.1507 acc_val: 0.7800 time: 9.0141s\n",
            "Epoch: 1116 loss_train: 0.6297 acc_train: 0.8400 loss_val: 1.1486 acc_val: 0.7800 time: 9.0207s\n",
            "Epoch: 1117 loss_train: 0.6186 acc_train: 0.8400 loss_val: 1.1487 acc_val: 0.7800 time: 9.0280s\n",
            "Epoch: 1118 loss_train: 0.6774 acc_train: 0.8200 loss_val: 1.1499 acc_val: 0.7600 time: 9.0358s\n",
            "Epoch: 1119 loss_train: 0.6570 acc_train: 0.8200 loss_val: 1.1513 acc_val: 0.7600 time: 9.0473s\n",
            "Epoch: 1120 loss_train: 0.6340 acc_train: 0.8200 loss_val: 1.1577 acc_val: 0.7600 time: 9.0547s\n",
            "Epoch: 1121 loss_train: 0.5669 acc_train: 0.8400 loss_val: 1.1559 acc_val: 0.7600 time: 9.0616s\n",
            "Epoch: 1122 loss_train: 0.6086 acc_train: 0.8200 loss_val: 1.1605 acc_val: 0.7600 time: 9.0679s\n",
            "Epoch: 1123 loss_train: 0.5366 acc_train: 0.8600 loss_val: 1.1672 acc_val: 0.7600 time: 9.0743s\n",
            "Epoch: 1124 loss_train: 0.6516 acc_train: 0.8400 loss_val: 1.1758 acc_val: 0.7600 time: 9.0809s\n",
            "Epoch: 1125 loss_train: 0.6510 acc_train: 0.8200 loss_val: 1.1911 acc_val: 0.7600 time: 9.0877s\n",
            "Epoch: 1126 loss_train: 0.6895 acc_train: 0.8200 loss_val: 1.1995 acc_val: 0.7600 time: 9.0945s\n",
            "Epoch: 1127 loss_train: 0.5865 acc_train: 0.8800 loss_val: 1.2022 acc_val: 0.7600 time: 9.1012s\n",
            "Epoch: 1128 loss_train: 0.6408 acc_train: 0.8200 loss_val: 1.1861 acc_val: 0.7600 time: 9.1078s\n",
            "Epoch: 1129 loss_train: 0.6761 acc_train: 0.8000 loss_val: 1.1727 acc_val: 0.7600 time: 9.1144s\n",
            "Epoch: 1130 loss_train: 0.6429 acc_train: 0.8200 loss_val: 1.1671 acc_val: 0.7600 time: 9.1215s\n",
            "Epoch: 1131 loss_train: 0.6052 acc_train: 0.8600 loss_val: 1.1652 acc_val: 0.7600 time: 9.1282s\n",
            "Epoch: 1132 loss_train: 0.6427 acc_train: 0.8200 loss_val: 1.1624 acc_val: 0.7600 time: 9.1347s\n",
            "Epoch: 1133 loss_train: 0.5642 acc_train: 0.8600 loss_val: 1.1597 acc_val: 0.7600 time: 9.1412s\n",
            "Epoch: 1134 loss_train: 0.5711 acc_train: 0.8400 loss_val: 1.1574 acc_val: 0.7600 time: 9.1477s\n",
            "Epoch: 1135 loss_train: 0.6266 acc_train: 0.8000 loss_val: 1.1584 acc_val: 0.7600 time: 9.1540s\n",
            "Epoch: 1136 loss_train: 0.6112 acc_train: 0.8400 loss_val: 1.1602 acc_val: 0.7600 time: 9.1621s\n",
            "Epoch: 1137 loss_train: 0.6759 acc_train: 0.8200 loss_val: 1.1641 acc_val: 0.7600 time: 9.1724s\n",
            "Epoch: 1138 loss_train: 0.6469 acc_train: 0.8200 loss_val: 1.1720 acc_val: 0.7600 time: 9.1789s\n",
            "Epoch: 1139 loss_train: 0.6395 acc_train: 0.8000 loss_val: 1.1793 acc_val: 0.7600 time: 9.1853s\n",
            "Epoch: 1140 loss_train: 0.5905 acc_train: 0.8600 loss_val: 1.1850 acc_val: 0.7600 time: 9.1920s\n",
            "Epoch: 1141 loss_train: 0.6411 acc_train: 0.8400 loss_val: 1.1925 acc_val: 0.7600 time: 9.1992s\n",
            "Epoch: 1142 loss_train: 0.6632 acc_train: 0.8200 loss_val: 1.2009 acc_val: 0.7600 time: 9.2073s\n",
            "Epoch: 1143 loss_train: 0.6103 acc_train: 0.8200 loss_val: 1.1955 acc_val: 0.7600 time: 9.2138s\n",
            "Epoch: 1144 loss_train: 0.6358 acc_train: 0.8000 loss_val: 1.1969 acc_val: 0.7600 time: 9.2251s\n",
            "Epoch: 1145 loss_train: 0.6331 acc_train: 0.8400 loss_val: 1.2041 acc_val: 0.7600 time: 9.2348s\n",
            "Epoch: 1146 loss_train: 0.6620 acc_train: 0.8400 loss_val: 1.2023 acc_val: 0.7600 time: 9.2401s\n",
            "Epoch: 1147 loss_train: 0.6173 acc_train: 0.8200 loss_val: 1.2059 acc_val: 0.7600 time: 9.2474s\n",
            "Epoch: 1148 loss_train: 0.7086 acc_train: 0.8000 loss_val: 1.2216 acc_val: 0.7400 time: 9.2568s\n",
            "Epoch: 1149 loss_train: 0.5589 acc_train: 0.8600 loss_val: 1.2230 acc_val: 0.7400 time: 9.2637s\n",
            "Epoch: 1150 loss_train: 0.5641 acc_train: 0.8600 loss_val: 1.2270 acc_val: 0.7400 time: 9.2705s\n",
            "Epoch: 1151 loss_train: 0.5484 acc_train: 0.8600 loss_val: 1.2112 acc_val: 0.7400 time: 9.2770s\n",
            "Epoch: 1152 loss_train: 0.6768 acc_train: 0.8400 loss_val: 1.1867 acc_val: 0.7600 time: 9.2849s\n",
            "Epoch: 1153 loss_train: 0.5626 acc_train: 0.8400 loss_val: 1.1697 acc_val: 0.7600 time: 9.2920s\n",
            "Epoch: 1154 loss_train: 0.5679 acc_train: 0.8400 loss_val: 1.1589 acc_val: 0.7600 time: 9.3004s\n",
            "Epoch: 1155 loss_train: 0.6515 acc_train: 0.8400 loss_val: 1.1506 acc_val: 0.7600 time: 9.3074s\n",
            "Epoch: 1156 loss_train: 0.6105 acc_train: 0.8400 loss_val: 1.1442 acc_val: 0.7600 time: 9.3141s\n",
            "Epoch: 1157 loss_train: 0.5508 acc_train: 0.8600 loss_val: 1.1478 acc_val: 0.7600 time: 9.3208s\n",
            "Epoch: 1158 loss_train: 0.6169 acc_train: 0.8200 loss_val: 1.1523 acc_val: 0.7600 time: 9.3285s\n",
            "Epoch: 1159 loss_train: 0.5922 acc_train: 0.8000 loss_val: 1.1618 acc_val: 0.7600 time: 9.3363s\n",
            "Epoch: 1160 loss_train: 0.6321 acc_train: 0.8200 loss_val: 1.1717 acc_val: 0.7400 time: 9.3428s\n",
            "Epoch: 1161 loss_train: 0.5577 acc_train: 0.8400 loss_val: 1.1702 acc_val: 0.7600 time: 9.3493s\n",
            "Epoch: 1162 loss_train: 0.6637 acc_train: 0.8400 loss_val: 1.1612 acc_val: 0.7600 time: 9.3577s\n",
            "Epoch: 1163 loss_train: 0.6087 acc_train: 0.8400 loss_val: 1.1571 acc_val: 0.7600 time: 9.3643s\n",
            "Epoch: 1164 loss_train: 0.4989 acc_train: 0.8800 loss_val: 1.1542 acc_val: 0.7600 time: 9.3739s\n",
            "Epoch: 1165 loss_train: 0.5764 acc_train: 0.8200 loss_val: 1.1587 acc_val: 0.7600 time: 9.3803s\n",
            "Epoch: 1166 loss_train: 0.6179 acc_train: 0.8400 loss_val: 1.1613 acc_val: 0.7600 time: 9.3935s\n",
            "Epoch: 1167 loss_train: 0.6505 acc_train: 0.8400 loss_val: 1.1599 acc_val: 0.7600 time: 9.4045s\n",
            "Epoch: 1168 loss_train: 0.6026 acc_train: 0.8400 loss_val: 1.1651 acc_val: 0.7600 time: 9.4124s\n",
            "Epoch: 1169 loss_train: 0.5458 acc_train: 0.8800 loss_val: 1.1711 acc_val: 0.7600 time: 9.4205s\n",
            "Epoch: 1170 loss_train: 0.6586 acc_train: 0.8000 loss_val: 1.1958 acc_val: 0.7400 time: 9.4283s\n",
            "Epoch: 1171 loss_train: 0.6513 acc_train: 0.8200 loss_val: 1.2443 acc_val: 0.7400 time: 9.4368s\n",
            "Epoch: 1172 loss_train: 0.6167 acc_train: 0.8800 loss_val: 1.2842 acc_val: 0.7400 time: 9.4446s\n",
            "Epoch: 1173 loss_train: 0.5533 acc_train: 0.8600 loss_val: 1.3217 acc_val: 0.7400 time: 9.4528s\n",
            "Epoch: 1174 loss_train: 0.5350 acc_train: 0.8800 loss_val: 1.3396 acc_val: 0.7400 time: 9.4604s\n",
            "Epoch: 1175 loss_train: 0.5697 acc_train: 0.8600 loss_val: 1.3449 acc_val: 0.7400 time: 9.4705s\n",
            "Epoch: 1176 loss_train: 0.6973 acc_train: 0.8200 loss_val: 1.3512 acc_val: 0.7400 time: 9.4786s\n",
            "Epoch: 1177 loss_train: 0.5851 acc_train: 0.8800 loss_val: 1.3675 acc_val: 0.7400 time: 9.4880s\n",
            "Epoch: 1178 loss_train: 0.5877 acc_train: 0.8200 loss_val: 1.3750 acc_val: 0.7400 time: 9.4978s\n",
            "Epoch: 1179 loss_train: 0.5728 acc_train: 0.8600 loss_val: 1.3623 acc_val: 0.7400 time: 9.5099s\n",
            "Epoch: 1180 loss_train: 0.6182 acc_train: 0.8200 loss_val: 1.3109 acc_val: 0.7400 time: 9.5210s\n",
            "Epoch: 1181 loss_train: 0.5761 acc_train: 0.8400 loss_val: 1.2795 acc_val: 0.7400 time: 9.5303s\n",
            "Epoch: 1182 loss_train: 0.6295 acc_train: 0.8000 loss_val: 1.2556 acc_val: 0.7400 time: 9.5408s\n",
            "Epoch: 1183 loss_train: 0.6109 acc_train: 0.8600 loss_val: 1.2469 acc_val: 0.7400 time: 9.5500s\n",
            "Epoch: 1184 loss_train: 0.6266 acc_train: 0.8400 loss_val: 1.2382 acc_val: 0.7400 time: 9.5595s\n",
            "Epoch: 1185 loss_train: 0.6379 acc_train: 0.8000 loss_val: 1.2437 acc_val: 0.7400 time: 9.5689s\n",
            "Epoch: 1186 loss_train: 0.5986 acc_train: 0.8200 loss_val: 1.2567 acc_val: 0.7400 time: 9.5776s\n",
            "Epoch: 1187 loss_train: 0.5758 acc_train: 0.8600 loss_val: 1.2659 acc_val: 0.7400 time: 9.5952s\n",
            "Epoch: 1188 loss_train: 0.6258 acc_train: 0.8400 loss_val: 1.2812 acc_val: 0.7400 time: 9.6056s\n",
            "Epoch: 1189 loss_train: 0.6043 acc_train: 0.8400 loss_val: 1.3081 acc_val: 0.7400 time: 9.6155s\n",
            "Epoch: 1190 loss_train: 0.6056 acc_train: 0.8400 loss_val: 1.3449 acc_val: 0.7400 time: 9.6242s\n",
            "Epoch: 1191 loss_train: 0.6106 acc_train: 0.8600 loss_val: 1.3871 acc_val: 0.7400 time: 9.6322s\n",
            "Epoch: 1192 loss_train: 0.5748 acc_train: 0.8600 loss_val: 1.4360 acc_val: 0.7400 time: 9.6402s\n",
            "Epoch: 1193 loss_train: 0.5510 acc_train: 0.8600 loss_val: 1.4644 acc_val: 0.7200 time: 9.6476s\n",
            "Epoch: 1194 loss_train: 0.5059 acc_train: 0.8800 loss_val: 1.4174 acc_val: 0.7400 time: 9.6561s\n",
            "Epoch: 1195 loss_train: 0.5353 acc_train: 0.8600 loss_val: 1.3611 acc_val: 0.7400 time: 9.6642s\n",
            "Epoch: 1196 loss_train: 0.6660 acc_train: 0.8200 loss_val: 1.3040 acc_val: 0.7400 time: 9.6723s\n",
            "Epoch: 1197 loss_train: 0.5461 acc_train: 0.8600 loss_val: 1.2565 acc_val: 0.7400 time: 9.6795s\n",
            "Epoch: 1198 loss_train: 0.5677 acc_train: 0.8400 loss_val: 1.2404 acc_val: 0.7400 time: 9.6889s\n",
            "Epoch: 1199 loss_train: 0.6295 acc_train: 0.8400 loss_val: 1.2368 acc_val: 0.7400 time: 9.6960s\n",
            "Epoch: 1200 loss_train: 0.5603 acc_train: 0.8400 loss_val: 1.2609 acc_val: 0.7400 time: 9.7031s\n",
            "Epoch: 1201 loss_train: 0.6017 acc_train: 0.8400 loss_val: 1.3010 acc_val: 0.7400 time: 9.7109s\n",
            "Epoch: 1202 loss_train: 0.5781 acc_train: 0.8400 loss_val: 1.3253 acc_val: 0.7400 time: 9.7181s\n",
            "Epoch: 1203 loss_train: 0.6894 acc_train: 0.8200 loss_val: 1.3665 acc_val: 0.7400 time: 9.7257s\n",
            "Epoch: 1204 loss_train: 0.5277 acc_train: 0.8600 loss_val: 1.4108 acc_val: 0.7400 time: 9.7334s\n",
            "Epoch: 1205 loss_train: 0.5885 acc_train: 0.8400 loss_val: 1.4411 acc_val: 0.7400 time: 9.7401s\n",
            "Epoch: 1206 loss_train: 0.6041 acc_train: 0.8400 loss_val: 1.4580 acc_val: 0.7400 time: 9.7466s\n",
            "Epoch: 1207 loss_train: 0.5758 acc_train: 0.8400 loss_val: 1.4576 acc_val: 0.7400 time: 9.7529s\n",
            "Epoch: 1208 loss_train: 0.5151 acc_train: 0.9000 loss_val: 1.4109 acc_val: 0.7400 time: 9.7618s\n",
            "Epoch: 1209 loss_train: 0.6165 acc_train: 0.8400 loss_val: 1.3666 acc_val: 0.7400 time: 9.7698s\n",
            "Epoch: 1210 loss_train: 0.5790 acc_train: 0.8600 loss_val: 1.3315 acc_val: 0.7400 time: 9.7768s\n",
            "Epoch: 1211 loss_train: 0.5576 acc_train: 0.8600 loss_val: 1.2919 acc_val: 0.7400 time: 9.7840s\n",
            "Epoch: 1212 loss_train: 0.5754 acc_train: 0.8400 loss_val: 1.2505 acc_val: 0.7400 time: 9.7920s\n",
            "Epoch: 1213 loss_train: 0.6503 acc_train: 0.8400 loss_val: 1.2158 acc_val: 0.7400 time: 9.8005s\n",
            "Epoch: 1214 loss_train: 0.6266 acc_train: 0.8400 loss_val: 1.1673 acc_val: 0.7600 time: 9.8059s\n",
            "Epoch: 1215 loss_train: 0.5582 acc_train: 0.8600 loss_val: 1.1506 acc_val: 0.7800 time: 9.8181s\n",
            "Epoch: 1216 loss_train: 0.5752 acc_train: 0.8400 loss_val: 1.1445 acc_val: 0.7800 time: 9.8268s\n",
            "Epoch: 1217 loss_train: 0.5676 acc_train: 0.8600 loss_val: 1.1420 acc_val: 0.7800 time: 9.8335s\n",
            "Epoch: 1218 loss_train: 0.6384 acc_train: 0.8200 loss_val: 1.1393 acc_val: 0.7800 time: 9.8412s\n",
            "Epoch: 1219 loss_train: 0.5725 acc_train: 0.8400 loss_val: 1.1418 acc_val: 0.7600 time: 9.8491s\n",
            "Epoch: 1220 loss_train: 0.6429 acc_train: 0.8200 loss_val: 1.1543 acc_val: 0.7600 time: 9.8561s\n",
            "Epoch: 1221 loss_train: 0.6705 acc_train: 0.8000 loss_val: 1.1793 acc_val: 0.7400 time: 9.8639s\n",
            "Epoch: 1222 loss_train: 0.5508 acc_train: 0.8400 loss_val: 1.2335 acc_val: 0.7400 time: 9.8704s\n",
            "Epoch: 1223 loss_train: 0.6519 acc_train: 0.8200 loss_val: 1.2997 acc_val: 0.7400 time: 9.8769s\n",
            "Epoch: 1224 loss_train: 0.6520 acc_train: 0.8400 loss_val: 1.3549 acc_val: 0.7400 time: 9.8834s\n",
            "Epoch: 1225 loss_train: 0.6691 acc_train: 0.8000 loss_val: 1.4092 acc_val: 0.7400 time: 9.8900s\n",
            "Epoch: 1226 loss_train: 0.6493 acc_train: 0.8200 loss_val: 1.4452 acc_val: 0.7200 time: 9.8978s\n",
            "Epoch: 1227 loss_train: 0.5378 acc_train: 0.8600 loss_val: 1.4681 acc_val: 0.7200 time: 9.9070s\n",
            "Epoch: 1228 loss_train: 0.5298 acc_train: 0.8400 loss_val: 1.4700 acc_val: 0.7200 time: 9.9139s\n",
            "Epoch: 1229 loss_train: 0.6940 acc_train: 0.8200 loss_val: 1.4730 acc_val: 0.7200 time: 9.9206s\n",
            "Epoch: 1230 loss_train: 0.6032 acc_train: 0.8400 loss_val: 1.4064 acc_val: 0.7400 time: 9.9276s\n",
            "Epoch: 1231 loss_train: 0.6066 acc_train: 0.8400 loss_val: 1.3598 acc_val: 0.7400 time: 9.9340s\n",
            "Epoch: 1232 loss_train: 0.6620 acc_train: 0.8000 loss_val: 1.3261 acc_val: 0.7400 time: 9.9404s\n",
            "Epoch: 1233 loss_train: 0.6467 acc_train: 0.8200 loss_val: 1.2880 acc_val: 0.7400 time: 9.9468s\n",
            "Epoch: 1234 loss_train: 0.6851 acc_train: 0.8200 loss_val: 1.2530 acc_val: 0.7400 time: 9.9532s\n",
            "Epoch: 1235 loss_train: 0.6034 acc_train: 0.8400 loss_val: 1.2458 acc_val: 0.7400 time: 9.9647s\n",
            "Epoch: 1236 loss_train: 0.5372 acc_train: 0.8600 loss_val: 1.2602 acc_val: 0.7400 time: 9.9715s\n",
            "Epoch: 1237 loss_train: 0.6491 acc_train: 0.8400 loss_val: 1.2909 acc_val: 0.7400 time: 9.9780s\n",
            "Epoch: 1238 loss_train: 0.6055 acc_train: 0.8400 loss_val: 1.3189 acc_val: 0.7400 time: 9.9852s\n",
            "Epoch: 1239 loss_train: 0.6195 acc_train: 0.8200 loss_val: 1.3387 acc_val: 0.7400 time: 9.9920s\n",
            "Epoch: 1240 loss_train: 0.6637 acc_train: 0.8200 loss_val: 1.3595 acc_val: 0.7400 time: 9.9990s\n",
            "Epoch: 1241 loss_train: 0.5979 acc_train: 0.8200 loss_val: 1.3858 acc_val: 0.7400 time: 10.0059s\n",
            "Epoch: 1242 loss_train: 0.6024 acc_train: 0.8400 loss_val: 1.3954 acc_val: 0.7400 time: 10.0150s\n",
            "Epoch: 1243 loss_train: 0.6531 acc_train: 0.8000 loss_val: 1.4099 acc_val: 0.7400 time: 10.0200s\n",
            "Epoch: 1244 loss_train: 0.6017 acc_train: 0.8400 loss_val: 1.4358 acc_val: 0.7400 time: 10.0253s\n",
            "Epoch: 1245 loss_train: 0.6260 acc_train: 0.8200 loss_val: 1.4528 acc_val: 0.7200 time: 10.0396s\n",
            "Epoch: 1246 loss_train: 0.5352 acc_train: 0.8600 loss_val: 1.4581 acc_val: 0.7200 time: 10.0464s\n",
            "Epoch: 1247 loss_train: 0.5710 acc_train: 0.8600 loss_val: 1.4568 acc_val: 0.7200 time: 10.0534s\n",
            "Epoch: 1248 loss_train: 0.6027 acc_train: 0.8400 loss_val: 1.3899 acc_val: 0.7400 time: 10.0627s\n",
            "Epoch: 1249 loss_train: 0.5837 acc_train: 0.8400 loss_val: 1.3258 acc_val: 0.7400 time: 10.0732s\n",
            "Epoch: 1250 loss_train: 0.5634 acc_train: 0.8400 loss_val: 1.2833 acc_val: 0.7400 time: 10.0802s\n",
            "Epoch: 1251 loss_train: 0.5402 acc_train: 0.8800 loss_val: 1.2543 acc_val: 0.7400 time: 10.0889s\n",
            "Epoch: 1252 loss_train: 0.5621 acc_train: 0.8400 loss_val: 1.2342 acc_val: 0.7400 time: 10.0963s\n",
            "Epoch: 1253 loss_train: 0.5415 acc_train: 0.8600 loss_val: 1.2227 acc_val: 0.7400 time: 10.1030s\n",
            "Epoch: 1254 loss_train: 0.5626 acc_train: 0.8800 loss_val: 1.2235 acc_val: 0.7400 time: 10.1098s\n",
            "Epoch: 1255 loss_train: 0.5778 acc_train: 0.8600 loss_val: 1.2366 acc_val: 0.7400 time: 10.1168s\n",
            "Epoch: 1256 loss_train: 0.5991 acc_train: 0.8400 loss_val: 1.2406 acc_val: 0.7400 time: 10.1250s\n",
            "Epoch: 1257 loss_train: 0.6198 acc_train: 0.8200 loss_val: 1.2590 acc_val: 0.7400 time: 10.1338s\n",
            "Epoch: 1258 loss_train: 0.5819 acc_train: 0.8200 loss_val: 1.2920 acc_val: 0.7400 time: 10.1428s\n",
            "Epoch: 1259 loss_train: 0.5827 acc_train: 0.8400 loss_val: 1.3370 acc_val: 0.7400 time: 10.1497s\n",
            "Epoch: 1260 loss_train: 0.6126 acc_train: 0.8600 loss_val: 1.3741 acc_val: 0.7400 time: 10.1562s\n",
            "Epoch: 1261 loss_train: 0.5287 acc_train: 0.8600 loss_val: 1.4059 acc_val: 0.7400 time: 10.1641s\n",
            "Epoch: 1262 loss_train: 0.5733 acc_train: 0.8600 loss_val: 1.4320 acc_val: 0.7400 time: 10.1712s\n",
            "Epoch: 1263 loss_train: 0.5647 acc_train: 0.8600 loss_val: 1.4532 acc_val: 0.7400 time: 10.1782s\n",
            "Epoch: 1264 loss_train: 0.6040 acc_train: 0.8000 loss_val: 1.4760 acc_val: 0.7400 time: 10.1852s\n",
            "Epoch: 1265 loss_train: 0.6212 acc_train: 0.8400 loss_val: 1.4585 acc_val: 0.7400 time: 10.1919s\n",
            "Epoch: 1266 loss_train: 0.5248 acc_train: 0.8800 loss_val: 1.4382 acc_val: 0.7400 time: 10.1989s\n",
            "Epoch: 1267 loss_train: 0.5203 acc_train: 0.8800 loss_val: 1.4133 acc_val: 0.7400 time: 10.2064s\n",
            "Epoch: 1268 loss_train: 0.5540 acc_train: 0.8600 loss_val: 1.3788 acc_val: 0.7400 time: 10.2146s\n",
            "Epoch: 1269 loss_train: 0.6648 acc_train: 0.8400 loss_val: 1.3572 acc_val: 0.7400 time: 10.2225s\n",
            "Epoch: 1270 loss_train: 0.6742 acc_train: 0.8000 loss_val: 1.3462 acc_val: 0.7400 time: 10.2310s\n",
            "Epoch: 1271 loss_train: 0.6408 acc_train: 0.8400 loss_val: 1.3412 acc_val: 0.7400 time: 10.2496s\n",
            "Epoch: 1272 loss_train: 0.7170 acc_train: 0.8000 loss_val: 1.3409 acc_val: 0.7400 time: 10.2584s\n",
            "Epoch: 1273 loss_train: 0.5366 acc_train: 0.8800 loss_val: 1.3589 acc_val: 0.7400 time: 10.2665s\n",
            "Epoch: 1274 loss_train: 0.5900 acc_train: 0.8200 loss_val: 1.3856 acc_val: 0.7400 time: 10.2732s\n",
            "Epoch: 1275 loss_train: 0.6183 acc_train: 0.8400 loss_val: 1.4180 acc_val: 0.7400 time: 10.2804s\n",
            "Epoch: 1276 loss_train: 0.6162 acc_train: 0.8000 loss_val: 1.4504 acc_val: 0.7400 time: 10.2887s\n",
            "Epoch: 1277 loss_train: 0.5642 acc_train: 0.8400 loss_val: 1.4675 acc_val: 0.7400 time: 10.2955s\n",
            "Epoch: 1278 loss_train: 0.5727 acc_train: 0.8400 loss_val: 1.4811 acc_val: 0.7200 time: 10.3034s\n",
            "Epoch: 1279 loss_train: 0.5394 acc_train: 0.8600 loss_val: 1.4993 acc_val: 0.7200 time: 10.3111s\n",
            "Epoch: 1280 loss_train: 0.6112 acc_train: 0.8200 loss_val: 1.4778 acc_val: 0.7400 time: 10.3182s\n",
            "Epoch: 1281 loss_train: 0.5875 acc_train: 0.8400 loss_val: 1.4634 acc_val: 0.7400 time: 10.3252s\n",
            "Epoch: 1282 loss_train: 0.6242 acc_train: 0.8400 loss_val: 1.4430 acc_val: 0.7400 time: 10.3341s\n",
            "Epoch: 1283 loss_train: 0.5886 acc_train: 0.8200 loss_val: 1.4300 acc_val: 0.7400 time: 10.3431s\n",
            "Epoch: 1284 loss_train: 0.6535 acc_train: 0.8200 loss_val: 1.3815 acc_val: 0.7400 time: 10.3496s\n",
            "Epoch: 1285 loss_train: 0.6703 acc_train: 0.8200 loss_val: 1.3667 acc_val: 0.7400 time: 10.3562s\n",
            "Epoch: 1286 loss_train: 0.5837 acc_train: 0.8800 loss_val: 1.3646 acc_val: 0.7400 time: 10.3625s\n",
            "Epoch: 1287 loss_train: 0.5868 acc_train: 0.8000 loss_val: 1.3797 acc_val: 0.7400 time: 10.3690s\n",
            "Epoch: 1288 loss_train: 0.6884 acc_train: 0.8000 loss_val: 1.3972 acc_val: 0.7400 time: 10.3758s\n",
            "Epoch: 1289 loss_train: 0.5792 acc_train: 0.8400 loss_val: 1.4030 acc_val: 0.7400 time: 10.3822s\n",
            "Epoch: 1290 loss_train: 0.6289 acc_train: 0.8400 loss_val: 1.4198 acc_val: 0.7400 time: 10.3887s\n",
            "Epoch: 1291 loss_train: 0.6489 acc_train: 0.8400 loss_val: 1.4202 acc_val: 0.7400 time: 10.3950s\n",
            "Epoch: 1292 loss_train: 0.6794 acc_train: 0.8200 loss_val: 1.4208 acc_val: 0.7400 time: 10.4012s\n",
            "Epoch: 1293 loss_train: 0.5730 acc_train: 0.8600 loss_val: 1.4399 acc_val: 0.7400 time: 10.4077s\n",
            "Epoch: 1294 loss_train: 0.5819 acc_train: 0.8200 loss_val: 1.4667 acc_val: 0.7400 time: 10.4142s\n",
            "Epoch: 1295 loss_train: 0.5768 acc_train: 0.8600 loss_val: 1.4802 acc_val: 0.7400 time: 10.4215s\n",
            "Epoch: 1296 loss_train: 0.5660 acc_train: 0.8600 loss_val: 1.4768 acc_val: 0.7400 time: 10.4286s\n",
            "Epoch: 1297 loss_train: 0.6166 acc_train: 0.8000 loss_val: 1.4866 acc_val: 0.7400 time: 10.4352s\n",
            "Epoch: 1298 loss_train: 0.6344 acc_train: 0.8400 loss_val: 1.4976 acc_val: 0.7200 time: 10.4417s\n",
            "Epoch: 1299 loss_train: 0.6126 acc_train: 0.8200 loss_val: 1.5181 acc_val: 0.7200 time: 10.4539s\n",
            "Epoch: 1300 loss_train: 0.6045 acc_train: 0.8400 loss_val: 1.5568 acc_val: 0.7000 time: 10.4607s\n",
            "Epoch: 1301 loss_train: 0.6502 acc_train: 0.8400 loss_val: 1.5999 acc_val: 0.7000 time: 10.4675s\n",
            "Epoch: 1302 loss_train: 0.5825 acc_train: 0.8600 loss_val: 1.5541 acc_val: 0.7200 time: 10.4754s\n",
            "Epoch: 1303 loss_train: 0.5685 acc_train: 0.8200 loss_val: 1.5062 acc_val: 0.7200 time: 10.4821s\n",
            "Epoch: 1304 loss_train: 0.5803 acc_train: 0.8400 loss_val: 1.4588 acc_val: 0.7400 time: 10.4889s\n",
            "Epoch: 1305 loss_train: 0.5617 acc_train: 0.8400 loss_val: 1.4114 acc_val: 0.7400 time: 10.4957s\n",
            "Epoch: 1306 loss_train: 0.5791 acc_train: 0.8400 loss_val: 1.3819 acc_val: 0.7400 time: 10.5025s\n",
            "Epoch: 1307 loss_train: 0.6393 acc_train: 0.8400 loss_val: 1.3565 acc_val: 0.7400 time: 10.5092s\n",
            "Epoch: 1308 loss_train: 0.6136 acc_train: 0.8200 loss_val: 1.3433 acc_val: 0.7400 time: 10.5159s\n",
            "Epoch: 1309 loss_train: 0.5932 acc_train: 0.8400 loss_val: 1.3316 acc_val: 0.7400 time: 10.5225s\n",
            "Epoch: 1310 loss_train: 0.5651 acc_train: 0.8600 loss_val: 1.3404 acc_val: 0.7400 time: 10.5291s\n",
            "Epoch: 1311 loss_train: 0.5705 acc_train: 0.8600 loss_val: 1.3568 acc_val: 0.7400 time: 10.5361s\n",
            "Epoch: 1312 loss_train: 0.5435 acc_train: 0.8600 loss_val: 1.3832 acc_val: 0.7400 time: 10.5427s\n",
            "Epoch: 1313 loss_train: 0.5592 acc_train: 0.8800 loss_val: 1.4031 acc_val: 0.7400 time: 10.5505s\n",
            "Epoch: 1314 loss_train: 0.5262 acc_train: 0.9000 loss_val: 1.3464 acc_val: 0.7400 time: 10.5577s\n",
            "Epoch: 1315 loss_train: 0.6689 acc_train: 0.8200 loss_val: 1.2933 acc_val: 0.7400 time: 10.5678s\n",
            "Epoch: 1316 loss_train: 0.6412 acc_train: 0.8000 loss_val: 1.2776 acc_val: 0.7400 time: 10.5740s\n",
            "Epoch: 1317 loss_train: 0.5466 acc_train: 0.8600 loss_val: 1.2746 acc_val: 0.7400 time: 10.5803s\n",
            "Epoch: 1318 loss_train: 0.6245 acc_train: 0.8400 loss_val: 1.2923 acc_val: 0.7400 time: 10.5875s\n",
            "Epoch: 1319 loss_train: 0.5917 acc_train: 0.8400 loss_val: 1.3343 acc_val: 0.7400 time: 10.5954s\n",
            "Epoch: 1320 loss_train: 0.6399 acc_train: 0.8000 loss_val: 1.3849 acc_val: 0.7400 time: 10.6020s\n",
            "Epoch: 1321 loss_train: 0.7041 acc_train: 0.8000 loss_val: 1.4501 acc_val: 0.7400 time: 10.6090s\n",
            "Epoch: 1322 loss_train: 0.6232 acc_train: 0.8400 loss_val: 1.4596 acc_val: 0.7400 time: 10.6158s\n",
            "Epoch: 1323 loss_train: 0.6683 acc_train: 0.8200 loss_val: 1.4772 acc_val: 0.7200 time: 10.6224s\n",
            "Epoch: 1324 loss_train: 0.5850 acc_train: 0.8600 loss_val: 1.4907 acc_val: 0.7200 time: 10.6313s\n",
            "Epoch: 1325 loss_train: 0.5923 acc_train: 0.8400 loss_val: 1.4842 acc_val: 0.7200 time: 10.6389s\n",
            "Epoch: 1326 loss_train: 0.5097 acc_train: 0.9000 loss_val: 1.3992 acc_val: 0.7400 time: 10.6474s\n",
            "Epoch: 1327 loss_train: 0.5633 acc_train: 0.8600 loss_val: 1.3367 acc_val: 0.7400 time: 10.6547s\n",
            "Epoch: 1328 loss_train: 0.6128 acc_train: 0.8400 loss_val: 1.2824 acc_val: 0.7400 time: 10.6676s\n",
            "Epoch: 1329 loss_train: 0.5848 acc_train: 0.8400 loss_val: 1.2415 acc_val: 0.7400 time: 10.6750s\n",
            "Epoch: 1330 loss_train: 0.6273 acc_train: 0.8400 loss_val: 1.2173 acc_val: 0.7400 time: 10.6819s\n",
            "Epoch: 1331 loss_train: 0.6467 acc_train: 0.8200 loss_val: 1.2135 acc_val: 0.7400 time: 10.6937s\n",
            "Epoch: 1332 loss_train: 0.5076 acc_train: 0.8800 loss_val: 1.2195 acc_val: 0.7400 time: 10.7020s\n",
            "Epoch: 1333 loss_train: 0.6330 acc_train: 0.8400 loss_val: 1.2343 acc_val: 0.7400 time: 10.7101s\n",
            "Epoch: 1334 loss_train: 0.6221 acc_train: 0.8400 loss_val: 1.2606 acc_val: 0.7400 time: 10.7178s\n",
            "Epoch: 1335 loss_train: 0.5756 acc_train: 0.8400 loss_val: 1.2871 acc_val: 0.7400 time: 10.7248s\n",
            "Epoch: 1336 loss_train: 0.5827 acc_train: 0.8400 loss_val: 1.3200 acc_val: 0.7400 time: 10.7317s\n",
            "Epoch: 1337 loss_train: 0.7645 acc_train: 0.7800 loss_val: 1.3522 acc_val: 0.7400 time: 10.7416s\n",
            "Epoch: 1338 loss_train: 0.5682 acc_train: 0.8200 loss_val: 1.3879 acc_val: 0.7400 time: 10.7482s\n",
            "Epoch: 1339 loss_train: 0.6160 acc_train: 0.8600 loss_val: 1.3991 acc_val: 0.7400 time: 10.7586s\n",
            "Epoch: 1340 loss_train: 0.5831 acc_train: 0.8400 loss_val: 1.4236 acc_val: 0.7400 time: 10.7658s\n",
            "Epoch: 1341 loss_train: 0.6792 acc_train: 0.8200 loss_val: 1.4200 acc_val: 0.7400 time: 10.7733s\n",
            "Epoch: 1342 loss_train: 0.5048 acc_train: 0.8600 loss_val: 1.4049 acc_val: 0.7400 time: 10.7809s\n",
            "Epoch: 1343 loss_train: 0.6042 acc_train: 0.8000 loss_val: 1.4005 acc_val: 0.7400 time: 10.7882s\n",
            "Epoch: 1344 loss_train: 0.6332 acc_train: 0.8200 loss_val: 1.3871 acc_val: 0.7400 time: 10.7997s\n",
            "Epoch: 1345 loss_train: 0.5573 acc_train: 0.8400 loss_val: 1.3792 acc_val: 0.7400 time: 10.8077s\n",
            "Epoch: 1346 loss_train: 0.5952 acc_train: 0.8400 loss_val: 1.3917 acc_val: 0.7400 time: 10.8159s\n",
            "Epoch: 1347 loss_train: 0.6405 acc_train: 0.8200 loss_val: 1.3968 acc_val: 0.7400 time: 10.8239s\n",
            "Epoch: 1348 loss_train: 0.5189 acc_train: 0.8800 loss_val: 1.3842 acc_val: 0.7400 time: 10.8309s\n",
            "Epoch: 1349 loss_train: 0.6437 acc_train: 0.8200 loss_val: 1.3798 acc_val: 0.7400 time: 10.8384s\n",
            "Epoch: 1350 loss_train: 0.5799 acc_train: 0.8400 loss_val: 1.3731 acc_val: 0.7400 time: 10.8460s\n",
            "Epoch: 1351 loss_train: 0.5446 acc_train: 0.8400 loss_val: 1.3769 acc_val: 0.7400 time: 10.8542s\n",
            "Epoch: 1352 loss_train: 0.5742 acc_train: 0.8400 loss_val: 1.3697 acc_val: 0.7400 time: 10.8634s\n",
            "Epoch: 1353 loss_train: 0.5891 acc_train: 0.8400 loss_val: 1.3542 acc_val: 0.7400 time: 10.8733s\n",
            "Epoch: 1354 loss_train: 0.5568 acc_train: 0.8800 loss_val: 1.3567 acc_val: 0.7400 time: 10.8843s\n",
            "Epoch: 1355 loss_train: 0.6655 acc_train: 0.8200 loss_val: 1.3715 acc_val: 0.7400 time: 10.8915s\n",
            "Epoch: 1356 loss_train: 0.6204 acc_train: 0.8400 loss_val: 1.3808 acc_val: 0.7400 time: 10.9029s\n",
            "Epoch: 1357 loss_train: 0.5979 acc_train: 0.8400 loss_val: 1.4047 acc_val: 0.7400 time: 10.9107s\n",
            "Epoch: 1358 loss_train: 0.6615 acc_train: 0.8000 loss_val: 1.3982 acc_val: 0.7400 time: 10.9187s\n",
            "Epoch: 1359 loss_train: 0.6400 acc_train: 0.8400 loss_val: 1.3749 acc_val: 0.7400 time: 10.9272s\n",
            "Epoch: 1360 loss_train: 0.5932 acc_train: 0.8400 loss_val: 1.3529 acc_val: 0.7400 time: 10.9344s\n",
            "Epoch: 1361 loss_train: 0.5291 acc_train: 0.8400 loss_val: 1.3158 acc_val: 0.7400 time: 10.9422s\n",
            "Epoch: 1362 loss_train: 0.5904 acc_train: 0.8200 loss_val: 1.3016 acc_val: 0.7400 time: 10.9496s\n",
            "Epoch: 1363 loss_train: 0.5423 acc_train: 0.9000 loss_val: 1.2849 acc_val: 0.7400 time: 10.9568s\n",
            "Epoch: 1364 loss_train: 0.5482 acc_train: 0.8600 loss_val: 1.2723 acc_val: 0.7400 time: 10.9640s\n",
            "Epoch: 1365 loss_train: 0.5935 acc_train: 0.8400 loss_val: 1.2733 acc_val: 0.7400 time: 10.9720s\n",
            "Epoch: 1366 loss_train: 0.5308 acc_train: 0.8800 loss_val: 1.2766 acc_val: 0.7400 time: 10.9800s\n",
            "Epoch: 1367 loss_train: 0.5980 acc_train: 0.8400 loss_val: 1.3096 acc_val: 0.7400 time: 10.9873s\n",
            "Epoch: 1368 loss_train: 0.6157 acc_train: 0.8000 loss_val: 1.3550 acc_val: 0.7400 time: 10.9956s\n",
            "Epoch: 1369 loss_train: 0.5746 acc_train: 0.8200 loss_val: 1.3622 acc_val: 0.7400 time: 11.0029s\n",
            "Epoch: 1370 loss_train: 0.6065 acc_train: 0.8200 loss_val: 1.3239 acc_val: 0.7400 time: 11.0106s\n",
            "Epoch: 1371 loss_train: 0.6048 acc_train: 0.8400 loss_val: 1.3230 acc_val: 0.7400 time: 11.0175s\n",
            "Epoch: 1372 loss_train: 0.6253 acc_train: 0.8400 loss_val: 1.3234 acc_val: 0.7400 time: 11.0259s\n",
            "Epoch: 1373 loss_train: 0.5631 acc_train: 0.8400 loss_val: 1.3412 acc_val: 0.7400 time: 11.0345s\n",
            "Epoch: 1374 loss_train: 0.5398 acc_train: 0.8800 loss_val: 1.3625 acc_val: 0.7400 time: 11.0450s\n",
            "Epoch: 1375 loss_train: 0.5606 acc_train: 0.8400 loss_val: 1.4020 acc_val: 0.7400 time: 11.0530s\n",
            "Epoch: 1376 loss_train: 0.6173 acc_train: 0.8200 loss_val: 1.4424 acc_val: 0.7400 time: 11.0598s\n",
            "Epoch: 1377 loss_train: 0.6243 acc_train: 0.8600 loss_val: 1.4769 acc_val: 0.7200 time: 11.0674s\n",
            "Epoch: 1378 loss_train: 0.5810 acc_train: 0.8600 loss_val: 1.4975 acc_val: 0.7200 time: 11.0750s\n",
            "Epoch: 1379 loss_train: 0.7135 acc_train: 0.8000 loss_val: 1.5007 acc_val: 0.7200 time: 11.0823s\n",
            "Epoch: 1380 loss_train: 0.5913 acc_train: 0.8600 loss_val: 1.4586 acc_val: 0.7400 time: 11.0946s\n",
            "Epoch: 1381 loss_train: 0.6126 acc_train: 0.8400 loss_val: 1.4190 acc_val: 0.7400 time: 11.1011s\n",
            "Epoch: 1382 loss_train: 0.5688 acc_train: 0.8600 loss_val: 1.3528 acc_val: 0.7400 time: 11.1076s\n",
            "Epoch: 1383 loss_train: 0.5592 acc_train: 0.8400 loss_val: 1.3032 acc_val: 0.7400 time: 11.1144s\n",
            "Epoch: 1384 loss_train: 0.6103 acc_train: 0.8400 loss_val: 1.2779 acc_val: 0.7400 time: 11.1214s\n",
            "Epoch: 1385 loss_train: 0.5592 acc_train: 0.8400 loss_val: 1.2524 acc_val: 0.7400 time: 11.1315s\n",
            "Epoch: 1386 loss_train: 0.6015 acc_train: 0.8200 loss_val: 1.2343 acc_val: 0.7400 time: 11.1390s\n",
            "Epoch: 1387 loss_train: 0.5525 acc_train: 0.8600 loss_val: 1.2192 acc_val: 0.7400 time: 11.1457s\n",
            "Epoch: 1388 loss_train: 0.6408 acc_train: 0.8200 loss_val: 1.2302 acc_val: 0.7400 time: 11.1531s\n",
            "Epoch: 1389 loss_train: 0.5818 acc_train: 0.8600 loss_val: 1.2511 acc_val: 0.7400 time: 11.1596s\n",
            "Epoch: 1390 loss_train: 0.6751 acc_train: 0.8200 loss_val: 1.2783 acc_val: 0.7400 time: 11.1706s\n",
            "Epoch: 1391 loss_train: 0.6278 acc_train: 0.8400 loss_val: 1.3179 acc_val: 0.7400 time: 11.1789s\n",
            "Epoch: 1392 loss_train: 0.6683 acc_train: 0.8200 loss_val: 1.3605 acc_val: 0.7400 time: 11.1867s\n",
            "Epoch: 1393 loss_train: 0.5728 acc_train: 0.8200 loss_val: 1.4098 acc_val: 0.7400 time: 11.1943s\n",
            "Epoch: 1394 loss_train: 0.5281 acc_train: 0.8600 loss_val: 1.4408 acc_val: 0.7400 time: 11.2040s\n",
            "Epoch: 1395 loss_train: 0.6277 acc_train: 0.8200 loss_val: 1.3183 acc_val: 0.7400 time: 11.2131s\n",
            "Epoch: 1396 loss_train: 0.5796 acc_train: 0.8600 loss_val: 1.2076 acc_val: 0.7400 time: 11.2200s\n",
            "Epoch: 1397 loss_train: 0.6100 acc_train: 0.8000 loss_val: 1.1341 acc_val: 0.7600 time: 11.2275s\n",
            "Epoch: 1398 loss_train: 0.5958 acc_train: 0.8200 loss_val: 1.1194 acc_val: 0.7800 time: 11.2349s\n",
            "Epoch: 1399 loss_train: 0.5368 acc_train: 0.8600 loss_val: 1.1212 acc_val: 0.7800 time: 11.2421s\n",
            "Epoch: 1400 loss_train: 0.6284 acc_train: 0.8200 loss_val: 1.1216 acc_val: 0.7800 time: 11.2497s\n",
            "Epoch: 1401 loss_train: 0.6356 acc_train: 0.8200 loss_val: 1.1227 acc_val: 0.7800 time: 11.2602s\n",
            "Epoch: 1402 loss_train: 0.5917 acc_train: 0.8200 loss_val: 1.1210 acc_val: 0.7800 time: 11.2658s\n",
            "Epoch: 1403 loss_train: 0.7148 acc_train: 0.8200 loss_val: 1.1188 acc_val: 0.7800 time: 11.2751s\n",
            "Epoch: 1404 loss_train: 0.6673 acc_train: 0.8400 loss_val: 1.1162 acc_val: 0.7800 time: 11.2819s\n",
            "Epoch: 1405 loss_train: 0.6536 acc_train: 0.8200 loss_val: 1.1150 acc_val: 0.7800 time: 11.2885s\n",
            "Epoch: 1406 loss_train: 0.5794 acc_train: 0.8600 loss_val: 1.1141 acc_val: 0.7800 time: 11.2960s\n",
            "Epoch: 1407 loss_train: 0.5956 acc_train: 0.8400 loss_val: 1.1172 acc_val: 0.7800 time: 11.3067s\n",
            "Epoch: 1408 loss_train: 0.5700 acc_train: 0.8600 loss_val: 1.1286 acc_val: 0.7600 time: 11.3141s\n",
            "Epoch: 1409 loss_train: 0.6389 acc_train: 0.8200 loss_val: 1.1534 acc_val: 0.7600 time: 11.3223s\n",
            "Epoch: 1410 loss_train: 0.5450 acc_train: 0.8600 loss_val: 1.1924 acc_val: 0.7400 time: 11.3292s\n",
            "Epoch: 1411 loss_train: 0.6212 acc_train: 0.8400 loss_val: 1.2481 acc_val: 0.7400 time: 11.3378s\n",
            "Epoch: 1412 loss_train: 0.6009 acc_train: 0.8200 loss_val: 1.3189 acc_val: 0.7400 time: 11.3453s\n",
            "Epoch: 1413 loss_train: 0.6238 acc_train: 0.8600 loss_val: 1.3905 acc_val: 0.7400 time: 11.3522s\n",
            "Epoch: 1414 loss_train: 0.6732 acc_train: 0.8000 loss_val: 1.4927 acc_val: 0.7200 time: 11.3589s\n",
            "Epoch: 1415 loss_train: 0.5922 acc_train: 0.8400 loss_val: 1.5711 acc_val: 0.7000 time: 11.3656s\n",
            "Epoch: 1416 loss_train: 0.5378 acc_train: 0.8600 loss_val: 1.6145 acc_val: 0.7000 time: 11.3726s\n",
            "Epoch: 1417 loss_train: 0.6719 acc_train: 0.8200 loss_val: 1.3792 acc_val: 0.7400 time: 11.3800s\n",
            "Epoch: 1418 loss_train: 0.5989 acc_train: 0.8600 loss_val: 1.1682 acc_val: 0.7600 time: 11.3872s\n",
            "Epoch: 1419 loss_train: 0.6486 acc_train: 0.8400 loss_val: 1.1368 acc_val: 0.7800 time: 11.3938s\n",
            "Epoch: 1420 loss_train: 0.5798 acc_train: 0.8600 loss_val: 1.1447 acc_val: 0.7800 time: 11.4003s\n",
            "Epoch: 1421 loss_train: 0.6606 acc_train: 0.8000 loss_val: 1.1477 acc_val: 0.7800 time: 11.4067s\n",
            "Epoch: 1422 loss_train: 0.6686 acc_train: 0.8200 loss_val: 1.1462 acc_val: 0.7800 time: 11.4149s\n",
            "Epoch: 1423 loss_train: 0.6339 acc_train: 0.8400 loss_val: 1.1424 acc_val: 0.7800 time: 11.4280s\n",
            "Epoch: 1424 loss_train: 0.6483 acc_train: 0.8000 loss_val: 1.1390 acc_val: 0.7800 time: 11.4357s\n",
            "Epoch: 1425 loss_train: 0.5668 acc_train: 0.8400 loss_val: 1.1338 acc_val: 0.7800 time: 11.4435s\n",
            "Epoch: 1426 loss_train: 0.6005 acc_train: 0.8600 loss_val: 1.1287 acc_val: 0.7800 time: 11.4505s\n",
            "Epoch: 1427 loss_train: 0.6793 acc_train: 0.8200 loss_val: 1.1250 acc_val: 0.7800 time: 11.4575s\n",
            "Epoch: 1428 loss_train: 0.5575 acc_train: 0.8600 loss_val: 1.1246 acc_val: 0.7800 time: 11.4646s\n",
            "Epoch: 1429 loss_train: 0.6553 acc_train: 0.8000 loss_val: 1.1335 acc_val: 0.7800 time: 11.4740s\n",
            "Epoch: 1430 loss_train: 0.5906 acc_train: 0.8400 loss_val: 1.1603 acc_val: 0.7400 time: 11.4821s\n",
            "Epoch: 1431 loss_train: 0.5711 acc_train: 0.8600 loss_val: 1.2006 acc_val: 0.7400 time: 11.4899s\n",
            "Epoch: 1432 loss_train: 0.5847 acc_train: 0.8400 loss_val: 1.2421 acc_val: 0.7400 time: 11.4976s\n",
            "Epoch: 1433 loss_train: 0.5626 acc_train: 0.8600 loss_val: 1.1928 acc_val: 0.7400 time: 11.5100s\n",
            "Epoch: 1434 loss_train: 0.6052 acc_train: 0.8400 loss_val: 1.1578 acc_val: 0.7400 time: 11.5208s\n",
            "Epoch: 1435 loss_train: 0.5886 acc_train: 0.8400 loss_val: 1.1417 acc_val: 0.7800 time: 11.5286s\n",
            "Epoch: 1436 loss_train: 0.6991 acc_train: 0.8000 loss_val: 1.1355 acc_val: 0.7800 time: 11.5366s\n",
            "Epoch: 1437 loss_train: 0.6404 acc_train: 0.8200 loss_val: 1.1349 acc_val: 0.7800 time: 11.5445s\n",
            "Epoch: 1438 loss_train: 0.5562 acc_train: 0.8600 loss_val: 1.1397 acc_val: 0.7800 time: 11.5530s\n",
            "Epoch: 1439 loss_train: 0.5110 acc_train: 0.8800 loss_val: 1.1501 acc_val: 0.7600 time: 11.5607s\n",
            "Epoch: 1440 loss_train: 0.6399 acc_train: 0.8000 loss_val: 1.1729 acc_val: 0.7600 time: 11.5686s\n",
            "Epoch: 1441 loss_train: 0.7205 acc_train: 0.8200 loss_val: 1.1987 acc_val: 0.7400 time: 11.5766s\n",
            "Epoch: 1442 loss_train: 0.6293 acc_train: 0.8600 loss_val: 1.2358 acc_val: 0.7400 time: 11.5839s\n",
            "Epoch: 1443 loss_train: 0.5836 acc_train: 0.8400 loss_val: 1.2738 acc_val: 0.7400 time: 11.5928s\n",
            "Epoch: 1444 loss_train: 0.5327 acc_train: 0.8800 loss_val: 1.3154 acc_val: 0.7400 time: 11.6009s\n",
            "Epoch: 1445 loss_train: 0.6186 acc_train: 0.8200 loss_val: 1.3652 acc_val: 0.7200 time: 11.6111s\n",
            "Epoch: 1446 loss_train: 0.5796 acc_train: 0.8600 loss_val: 1.4118 acc_val: 0.7200 time: 11.6193s\n",
            "Epoch: 1447 loss_train: 0.6726 acc_train: 0.8200 loss_val: 1.4685 acc_val: 0.7200 time: 11.6266s\n",
            "Epoch: 1448 loss_train: 0.5513 acc_train: 0.8400 loss_val: 1.5356 acc_val: 0.7200 time: 11.6340s\n",
            "Epoch: 1449 loss_train: 0.5734 acc_train: 0.8400 loss_val: 1.5905 acc_val: 0.7000 time: 11.6466s\n",
            "Epoch: 1450 loss_train: 0.5967 acc_train: 0.8200 loss_val: 1.6450 acc_val: 0.7000 time: 11.6548s\n",
            "Epoch: 1451 loss_train: 0.5606 acc_train: 0.8600 loss_val: 1.6533 acc_val: 0.7000 time: 11.6617s\n",
            "Epoch: 1452 loss_train: 0.6336 acc_train: 0.8600 loss_val: 1.5162 acc_val: 0.7200 time: 11.6692s\n",
            "Epoch: 1453 loss_train: 0.6418 acc_train: 0.8400 loss_val: 1.4106 acc_val: 0.7200 time: 11.6773s\n",
            "Epoch: 1454 loss_train: 0.6290 acc_train: 0.8200 loss_val: 1.3149 acc_val: 0.7400 time: 11.6841s\n",
            "Epoch: 1455 loss_train: 0.6381 acc_train: 0.8200 loss_val: 1.2471 acc_val: 0.7400 time: 11.6908s\n",
            "Epoch: 1456 loss_train: 0.6839 acc_train: 0.8200 loss_val: 1.2098 acc_val: 0.7600 time: 11.6972s\n",
            "Epoch: 1457 loss_train: 0.5787 acc_train: 0.8200 loss_val: 1.1950 acc_val: 0.7600 time: 11.7040s\n",
            "Epoch: 1458 loss_train: 0.6722 acc_train: 0.8400 loss_val: 1.1870 acc_val: 0.7600 time: 11.7106s\n",
            "Epoch: 1459 loss_train: 0.6400 acc_train: 0.8200 loss_val: 1.1876 acc_val: 0.7600 time: 11.7288s\n",
            "Epoch: 1460 loss_train: 0.5437 acc_train: 0.8600 loss_val: 1.1958 acc_val: 0.7600 time: 11.7405s\n",
            "Epoch: 1461 loss_train: 0.6576 acc_train: 0.8200 loss_val: 1.2052 acc_val: 0.7600 time: 11.7493s\n",
            "Epoch: 1462 loss_train: 0.5161 acc_train: 0.9000 loss_val: 1.2220 acc_val: 0.7400 time: 11.7587s\n",
            "Epoch: 1463 loss_train: 0.5708 acc_train: 0.8600 loss_val: 1.2502 acc_val: 0.7400 time: 11.7713s\n",
            "Epoch: 1464 loss_train: 0.5947 acc_train: 0.8400 loss_val: 1.2592 acc_val: 0.7400 time: 11.7816s\n",
            "Epoch: 1465 loss_train: 0.5775 acc_train: 0.8200 loss_val: 1.2725 acc_val: 0.7400 time: 11.7891s\n",
            "Epoch: 1466 loss_train: 0.5745 acc_train: 0.8400 loss_val: 1.2451 acc_val: 0.7400 time: 11.7960s\n",
            "Epoch: 1467 loss_train: 0.5623 acc_train: 0.8400 loss_val: 1.2304 acc_val: 0.7400 time: 11.8027s\n",
            "Epoch: 1468 loss_train: 0.5852 acc_train: 0.8600 loss_val: 1.2269 acc_val: 0.7600 time: 11.8093s\n",
            "Epoch: 1469 loss_train: 0.5815 acc_train: 0.8400 loss_val: 1.2252 acc_val: 0.7600 time: 11.8203s\n",
            "Epoch: 1470 loss_train: 0.5701 acc_train: 0.8400 loss_val: 1.2304 acc_val: 0.7600 time: 11.8285s\n",
            "Epoch: 1471 loss_train: 0.6315 acc_train: 0.8400 loss_val: 1.2436 acc_val: 0.7400 time: 11.8361s\n",
            "Epoch: 1472 loss_train: 0.7088 acc_train: 0.8000 loss_val: 1.2750 acc_val: 0.7400 time: 11.8433s\n",
            "Epoch: 1473 loss_train: 0.6430 acc_train: 0.8400 loss_val: 1.2694 acc_val: 0.7400 time: 11.8531s\n",
            "Epoch: 1474 loss_train: 0.5341 acc_train: 0.8600 loss_val: 1.2608 acc_val: 0.7400 time: 11.8599s\n",
            "Epoch: 1475 loss_train: 0.5641 acc_train: 0.8400 loss_val: 1.2513 acc_val: 0.7400 time: 11.8666s\n",
            "Epoch: 1476 loss_train: 0.5074 acc_train: 0.9000 loss_val: 1.2393 acc_val: 0.7400 time: 11.8745s\n",
            "Epoch: 1477 loss_train: 0.5668 acc_train: 0.8200 loss_val: 1.2441 acc_val: 0.7400 time: 11.8811s\n",
            "Epoch: 1478 loss_train: 0.5109 acc_train: 0.9000 loss_val: 1.2476 acc_val: 0.7400 time: 11.8876s\n",
            "Epoch: 1479 loss_train: 0.5375 acc_train: 0.8600 loss_val: 1.2529 acc_val: 0.7400 time: 11.8939s\n",
            "Epoch: 1480 loss_train: 0.6002 acc_train: 0.8200 loss_val: 1.2681 acc_val: 0.7400 time: 11.9003s\n",
            "Epoch: 1481 loss_train: 0.5466 acc_train: 0.8600 loss_val: 1.2722 acc_val: 0.7400 time: 11.9076s\n",
            "Epoch: 1482 loss_train: 0.6742 acc_train: 0.8200 loss_val: 1.2733 acc_val: 0.7400 time: 11.9151s\n",
            "Epoch: 1483 loss_train: 0.6091 acc_train: 0.8400 loss_val: 1.2866 acc_val: 0.7400 time: 11.9226s\n",
            "Epoch: 1484 loss_train: 0.5801 acc_train: 0.8400 loss_val: 1.2987 acc_val: 0.7400 time: 11.9300s\n",
            "Epoch: 1485 loss_train: 0.5636 acc_train: 0.8400 loss_val: 1.3103 acc_val: 0.7400 time: 11.9368s\n",
            "Epoch: 1486 loss_train: 0.5542 acc_train: 0.8600 loss_val: 1.3218 acc_val: 0.7400 time: 11.9427s\n",
            "Epoch: 1487 loss_train: 0.5517 acc_train: 0.8400 loss_val: 1.3275 acc_val: 0.7400 time: 11.9549s\n",
            "Epoch: 1488 loss_train: 0.7011 acc_train: 0.8200 loss_val: 1.3245 acc_val: 0.7400 time: 11.9616s\n",
            "Epoch: 1489 loss_train: 0.6140 acc_train: 0.8400 loss_val: 1.3300 acc_val: 0.7400 time: 11.9725s\n",
            "Epoch: 1490 loss_train: 0.6355 acc_train: 0.8200 loss_val: 1.3370 acc_val: 0.7400 time: 11.9802s\n",
            "Epoch: 1491 loss_train: 0.6025 acc_train: 0.8200 loss_val: 1.3373 acc_val: 0.7400 time: 11.9879s\n",
            "Epoch: 1492 loss_train: 0.6001 acc_train: 0.8400 loss_val: 1.3401 acc_val: 0.7400 time: 11.9950s\n",
            "Epoch: 1493 loss_train: 0.6425 acc_train: 0.8200 loss_val: 1.3376 acc_val: 0.7400 time: 12.0015s\n",
            "Epoch: 1494 loss_train: 0.5782 acc_train: 0.8400 loss_val: 1.3460 acc_val: 0.7400 time: 12.0084s\n",
            "Epoch: 1495 loss_train: 0.5960 acc_train: 0.8400 loss_val: 1.3556 acc_val: 0.7400 time: 12.0158s\n",
            "Epoch: 1496 loss_train: 0.6016 acc_train: 0.8600 loss_val: 1.3555 acc_val: 0.7400 time: 12.0229s\n",
            "Epoch: 1497 loss_train: 0.5962 acc_train: 0.8400 loss_val: 1.3588 acc_val: 0.7400 time: 12.0306s\n",
            "Epoch: 1498 loss_train: 0.5388 acc_train: 0.8400 loss_val: 1.3670 acc_val: 0.7400 time: 12.0381s\n",
            "Epoch: 1499 loss_train: 0.6083 acc_train: 0.8200 loss_val: 1.3654 acc_val: 0.7400 time: 12.0472s\n",
            "Epoch: 1500 loss_train: 0.5760 acc_train: 0.8600 loss_val: 1.3711 acc_val: 0.7400 time: 12.0552s\n",
            "Test set results: loss= 1.3035 accuracy= 0.7600\n",
            "GCN(\n",
            "  (gc1): GraphConvolution()\n",
            "  (gc2): GraphConvolution()\n",
            ")\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Optimization Finished!\n",
            "Total time elapsed: 12.5070s\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}