{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GCN_secondStep.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "https://github.com/vahidseydi/CGN/blob/main/GCN.ipynb",
      "authorship_tag": "ABX9TyMswXbXk6rcE8/LmD93uaja",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vahidseydi/CGN/blob/main/GCN_secondStep.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQttr1AB71xz"
      },
      "source": [
        "Downloading dataset file from Github "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Oop7M3Nyj_0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b06de685-5e6f-4d11-b13d-d98b2400c67d"
      },
      "source": [
        "! wget 'https://github.com/vahidseydi/CGN/blob/main/Data/amazon_electronics_computers%20(1).npz?raw=true'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-27 09:05:05--  https://github.com/vahidseydi/CGN/blob/main/Data/amazon_electronics_computers%20(1).npz?raw=true\n",
            "Resolving github.com (github.com)... 140.82.112.3\n",
            "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/vahidseydi/CGN/raw/main/Data/amazon_electronics_computers%20(1).npz [following]\n",
            "--2021-04-27 09:05:06--  https://github.com/vahidseydi/CGN/raw/main/Data/amazon_electronics_computers%20(1).npz\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/vahidseydi/CGN/main/Data/amazon_electronics_computers%20(1).npz [following]\n",
            "--2021-04-27 09:05:06--  https://raw.githubusercontent.com/vahidseydi/CGN/main/Data/amazon_electronics_computers%20(1).npz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 31921488 (30M) [application/octet-stream]\n",
            "Saving to: ‘amazon_electronics_computers (1).npz?raw=true.5’\n",
            "\n",
            "amazon_electronics_ 100%[===================>]  30.44M  55.7MB/s    in 0.5s    \n",
            "\n",
            "2021-04-27 09:05:06 (55.7 MB/s) - ‘amazon_electronics_computers (1).npz?raw=true.5’ saved [31921488/31921488]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MYbUcRVdtDe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "275e6fb3-e1da-4019-862e-1698cbb61cb2"
      },
      "source": [
        "import numpy as np\n",
        "npz_data=np.load('/content/amazon_electronics_computers (1).npz?raw=true')\n",
        "npz_data.files"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['adj_data',\n",
              " 'adj_indices',\n",
              " 'adj_indptr',\n",
              " 'adj_shape',\n",
              " 'attr_data',\n",
              " 'attr_indices',\n",
              " 'attr_indptr',\n",
              " 'attr_shape',\n",
              " 'labels',\n",
              " 'class_names']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmNiWW8bd-Xx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28a9a6ba-6024-4b6d-8d25-c5318fdaf8ce"
      },
      "source": [
        "class_names=npz_data['class_names']\n",
        "class_names"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Desktops', 'Data Storage', 'Laptops', 'Monitors',\n",
              "       'Computer Components', 'Video Projectors', 'Routers', 'Tablets',\n",
              "       'Networking Products', 'Webcams'], dtype='<U19')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tvHqESuyw4L"
      },
      "source": [
        "labels =npz_data['labels']\n",
        "y=labels.size\n",
        "#p for calculating percentage for idx_train,val,test\n",
        "p = lambda x: x*y/100\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9NewBDvWAdE"
      },
      "source": [
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import torch\n",
        "from scipy.sparse import  csr_matrix\n",
        "\n",
        "\n",
        "def load_data():\n",
        "\n",
        " features = sp.csr_matrix((npz_data['attr_data'], npz_data['attr_indices'], npz_data['attr_indptr']),\n",
        "                                        shape=npz_data['attr_shape'])\n",
        "\n",
        " # build graph\n",
        " \n",
        " adj= sp.csr_matrix(sp.csr_matrix((npz_data['adj_data'], npz_data['adj_indices'], npz_data['adj_indptr']),\n",
        "                                   shape=npz_data['adj_shape']))\n",
        " \n",
        " # build symmetric adjacency matrix\n",
        "\n",
        " adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
        " adj = normalize(adj + sp.eye(adj.shape[0]))\n",
        " #اول اعداد 0 و 1 هست بعد از نرمالایز تغییر میکند \n",
        "\n",
        " labels=npz_data['labels']\n",
        "\n",
        " idx_train =range(round(p(70)))\n",
        " idx_val = range(idx_train[-1],idx_train[-1]+round(p(20)))\n",
        " idx_test =range(idx_val[-1],idx_val[-1]+round(p(10)))\n",
        "\n",
        " idx_train = torch.LongTensor(idx_train)\n",
        " idx_val = torch.LongTensor(idx_val)\n",
        " idx_test = torch.LongTensor(idx_test)\n",
        "\n",
        " features = torch.FloatTensor(np.array(features.todense()))\n",
        " adj = torch.FloatTensor(np.array(adj.todense()))\n",
        " labels = torch.LongTensor(labels)\n",
        "\n",
        "\n",
        " return adj, features, labels, idx_train, idx_val, idx_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9YheZ8yNTSL"
      },
      "source": [
        "def normalize(mx):\n",
        "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
        "    rowsum = np.array(mx.sum(1))\n",
        "    #sum in every row \n",
        "    r_inv = np.power(rowsum, -1).flatten()\n",
        "    # every sum to the power of -1 \n",
        "    r_inv[np.isinf(r_inv)] = 0.\n",
        "    r_mat_inv = sp.diags(r_inv)\n",
        "    #diagonal matrice \n",
        "    mx = r_mat_inv.dot(mx)\n",
        "    return mx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iha5MQaYDJpG"
      },
      "source": [
        "def accuracy(output, labels):\n",
        "    preds = output.max(1)[1].type_as(labels)\n",
        "    correct = preds.eq(labels).double()\n",
        "    correct = correct.sum()\n",
        "    return correct / len(labels)    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5XDbpcGLyib"
      },
      "source": [
        "adj, features, labels, idx_train, idx_val, idx_test = load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXRAbQWUm7OU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e6eeaad-5ee3-4227-ebd1-d4d4fe728fda"
      },
      "source": [
        " labels[idx_test].size()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1375])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVTk2QDAndzK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5bd8dc1-2953-4087-bf74-ef7614b7a886"
      },
      "source": [
        "labels[idx_val].size()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2750])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XqEpZ6zbaYR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b2030b4-beb7-484b-9f23-b6392f03d056"
      },
      "source": [
        "int(idx_train[-1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9625"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWoZ0dZ2hML-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f87a95fc-c940-4ba1-f961-f35f685ca6f6"
      },
      "source": [
        "idx_test.size()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1375])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYWoRQYqhbY7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f40e3045-53ee-42d0-e654-4b47f0d56f53"
      },
      "source": [
        "adj[idx_test[0]:int(idx_test[-1])+1,idx_test[0]:int(idx_test[-1])+1].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1375, 1375])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uhtd1F5L-Ci",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ed8de56-d05b-406c-cf64-c63bc90bd73f"
      },
      "source": [
        "\n",
        "adj[:int(idx_train[-1]+1),:int(idx_train[-1]+1)].shape\n",
        "    \n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([9626, 9626])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qeOnC-myL0Tg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "563184fa-d7f2-4723-a943-9eac29a141f7"
      },
      "source": [
        "features[idx_train].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([9626, 767])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Wntrj3ukIjd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7910a52a-b0d3-4aa6-95cf-0ccc7549fafa"
      },
      "source": [
        "adj[idx_val[0]:int(idx_val[-1])+1,idx_val[0]:int(idx_val[-1])+1].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2750, 2750])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uuJ_NuRkOJj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3547f455-ade0-4e2e-f76f-0ec038f7a4fb"
      },
      "source": [
        "features[idx_val[0]:int(idx_val[-1])+1,idx_val[0]:int(idx_val[-1])+1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([], size=(2750, 0))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "us3XSHLHDWoP"
      },
      "source": [
        "#layers\n",
        "\n",
        "import math\n",
        "\n",
        "import torch\n",
        "\n",
        "from torch.nn.parameter import Parameter\n",
        "\n",
        "#class parameter ,param haro cache mikone\n",
        "\n",
        "from torch.nn.modules.module import Module\n",
        "\n",
        "\n",
        "class GraphConvolution(Module):\n",
        "    \"\"\"\n",
        "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features ,out_features, bias=True):\n",
        "        super(GraphConvolution, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        \n",
        "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
        "        if bias:\n",
        "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
        "        self.weight.data.uniform_(-stdv, stdv)\n",
        "        if self.bias is not None:\n",
        "            self.bias.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self, input, input_adj):\n",
        "        support = torch.mm(input, self.weight)\n",
        "        \n",
        "        #Sparse matrix multiplication=spmm\n",
        "        output = torch.spmm(input_adj, support)\n",
        "        if self.bias is not None:\n",
        "            return output + self.bias\n",
        "        else:\n",
        "            return output\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXDaJHg7DeHl"
      },
      "source": [
        "#models\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "#from layers import GraphConvolution\n",
        "\n",
        "\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self, nfeat, nhid,nclass, dropout):\n",
        "        super(GCN, self).__init__()\n",
        "\n",
        "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
        "        self.gc2 = GraphConvolution(nhid, nclass)\n",
        "   \n",
        "        self.dropout = dropout\n",
        "        \n",
        "    def forward(self, x, input_adj):\n",
        "        x = F.relu(self.gc1(x, input_adj))\n",
        "        x = F.dropout(x, self.dropout, training=self.training)\n",
        "        x = self.gc2(x, input_adj)\n",
        "        return F.log_softmax(x, dim=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kAvN1C2QDq89",
        "outputId": "1a0a19f5-57c8-4d67-9740-325ad870cada"
      },
      "source": [
        "#train\n",
        "\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from torchsummary import summary\n",
        "\n",
        "import time\n",
        "import argparse\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.pyplot as plt2\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "#from utils import load_data, accuracy\n",
        "#from models import GCN\n",
        "\n",
        "# Training settings\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
        "                    help='Disables CUDA training.')\n",
        "parser.add_argument('--fastmode', action='store_true', default=False,\n",
        "                    help='Validate during training pass.')\n",
        "parser.add_argument('--seed', type=int, default=42, help='Random seed.')\n",
        "parser.add_argument('--epochs', type=int, default=1500,\n",
        "                    help='Number of epochs to train.')\n",
        "parser.add_argument('--lr', type=float, default=0.01,\n",
        "                    help='Initial learning rate.')\n",
        "parser.add_argument('--weight_decay', type=float, default=5e-4,\n",
        "                    help='Weight decay (L2 loss on parameters).')\n",
        "parser.add_argument('--hidden', type=int, default=16,\n",
        "                    help='Number of hidden units.')\n",
        "\n",
        "parser.add_argument('--dropout', type=float, default=0.5,\n",
        "                    help='Dropout rate (1 - keep probability).')\n",
        "\n",
        "#args = parser.parse_args()\n",
        "#error midad khate bala,khate paein jaigozin shod\n",
        "\n",
        "args = parser.parse_known_args()[0]\n",
        "\n",
        "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
        "\n",
        "np.random.seed(args.seed)\n",
        "torch.manual_seed(args.seed)\n",
        "if args.cuda:\n",
        "    torch.cuda.manual_seed(args.seed)\n",
        "\n",
        "# Load data\n",
        "\n",
        "adj, features, labels, idx_train, idx_val, idx_test = load_data()\n",
        "\n",
        "# Model and optimizer\n",
        "model = GCN(nfeat=features.shape[1],\n",
        "            nhid=args.hidden,\n",
        "            nclass=class_names.size,\n",
        "            dropout=args.dropout)\n",
        "optimizer = optim.Adam(model.parameters(),\n",
        "                       lr=args.lr, weight_decay=args.weight_decay)\n",
        "\n",
        "\n",
        "## to set cuda as your device if possible\n",
        "##training on  GPU\n",
        "\n",
        "if args.cuda:\n",
        "    model.cuda()\n",
        "    features = features.cuda()\n",
        "    adj = adj.cuda()\n",
        "    labels = labels.cuda()\n",
        "    idx_train = idx_train.cuda()\n",
        "    idx_val = idx_val.cuda()\n",
        "    idx_test = idx_test.cuda()\n",
        "    \n",
        "    ## train:adjust the weights on the neural network\n",
        "    ## validation:used to minimize overfitting\n",
        " \n",
        "def test():\n",
        "    ## Turn on evaluation mode which disables dropout.\n",
        "    model.eval()\n",
        "    output = model(features[idx_test[0]:int(idx_test[-1])+1,:]\n",
        "                   ,adj[idx_test[0]:int(idx_test[-1])+1,idx_test[0]:int(idx_test[-1])+1])\n",
        "    loss_test = F.nll_loss(output, labels[idx_test])\n",
        "    acc_test = accuracy(output, labels[idx_test])\n",
        "    print(\"Test set results:\",\n",
        "          \"loss= {:.4f}\".format(loss_test.item()),\n",
        "          \"accuracy= {:.4f}\".format(acc_test.item()))\n",
        "\n",
        "def train(epoch):\n",
        "  model.train()\n",
        "  optimizer.zero_grad()\n",
        "  output = model(features[idx_train],adj[:int(idx_train[-1]+1),:int(idx_train[-1])+1])\n",
        "  loss_train = F.nll_loss(output, labels[idx_train])\n",
        "  acc_train = accuracy(output, labels[idx_train])\n",
        "  # Computing the gradients necessary to adjust the weights\n",
        "  loss_train.backward()\n",
        "  # Updating the weights of the neural network\n",
        "  optimizer.step()\n",
        "  losses.append(loss_train.item())\n",
        "  acc.append(acc_train.item())\n",
        "\n",
        "  if not args.fastmode:\n",
        "    # Evaluate validation set performance separately,\n",
        "    # deactivates dropout during validation run.\n",
        "    model.eval()\n",
        "    output = model(features[idx_val[0]:int(idx_val[-1])+1,:]\n",
        "                   , adj[idx_val[0]:int(idx_val[-1])+1,idx_val[0]:int(idx_val[-1])+1])\n",
        "  loss_val = F.nll_loss(output, labels[idx_val])\n",
        "  acc_val = accuracy(output, labels[idx_val])\n",
        "  losses_val.append(loss_val.item())\n",
        "  acc_valid.append(acc_val.item())\n",
        "\n",
        "  print('Epoch: {:04d}'.format(epoch+1),\n",
        "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
        "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
        "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
        "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
        "          'time: {:.4f}s'.format(time.time() - t))\n",
        "\n",
        "# Train model\n",
        "t_total = time.time()\n",
        "losses = []\n",
        "acc=[]\n",
        "losses_val = []\n",
        "acc_valid=[]\n",
        "t = time.time()\n",
        "for epoch in range(args.epochs):\n",
        "    train(epoch)\n",
        "\n",
        "#plotting loss_train_val:\n",
        "\n",
        "plt.plot(np.array(losses),label ='loss_train Plot')\n",
        "plt.plot(np.array(losses_val),label ='loss_val Plot')\n",
        "plt.title('loss_train_validation Plot')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('value')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "#plotting acc_train_val:\n",
        "\n",
        "plt2.plot(np.array(acc),label ='Accuracy_train Plot')\n",
        "plt2.plot(np.array(acc_valid),label ='Accuracy_val Plot')\n",
        "plt2.title('acc_train_validation Plot')\n",
        "plt2.xlabel('epoch')\n",
        "plt2.ylabel('value')\n",
        "plt2.legend()\n",
        "plt2.show()    \n",
        "\n",
        "print(\"Optimization Finished!\")\n",
        "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
        "\n",
        "print(model)\n",
        "\n",
        "# Testing\n",
        "test()\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0001 loss_train: 2.6459 acc_train: 0.0691 loss_val: 2.3678 acc_val: 0.0309 time: 0.5852s\n",
            "Epoch: 0002 loss_train: 2.3421 acc_train: 0.0259 loss_val: 2.3315 acc_val: 0.1469 time: 1.1982s\n",
            "Epoch: 0003 loss_train: 2.2393 acc_train: 0.0534 loss_val: 2.3105 acc_val: 0.1531 time: 1.8019s\n",
            "Epoch: 0004 loss_train: 2.2468 acc_train: 0.1525 loss_val: 2.2938 acc_val: 0.1549 time: 2.4407s\n",
            "Epoch: 0005 loss_train: 2.2125 acc_train: 0.1536 loss_val: 2.2827 acc_val: 0.1556 time: 3.0433s\n",
            "Epoch: 0006 loss_train: 2.1358 acc_train: 0.1547 loss_val: 2.2759 acc_val: 0.1571 time: 3.6155s\n",
            "Epoch: 0007 loss_train: 2.0875 acc_train: 0.1748 loss_val: 2.2727 acc_val: 0.1585 time: 4.2069s\n",
            "Epoch: 0008 loss_train: 2.0108 acc_train: 0.3250 loss_val: 2.2707 acc_val: 0.1582 time: 4.7937s\n",
            "Epoch: 0009 loss_train: 1.9838 acc_train: 0.3562 loss_val: 2.2664 acc_val: 0.1567 time: 5.3750s\n",
            "Epoch: 0010 loss_train: 1.9569 acc_train: 0.3626 loss_val: 2.2551 acc_val: 0.1578 time: 5.9639s\n",
            "Epoch: 0011 loss_train: 1.9260 acc_train: 0.3628 loss_val: 2.2477 acc_val: 0.1593 time: 6.5380s\n",
            "Epoch: 0012 loss_train: 1.9019 acc_train: 0.3663 loss_val: 2.2452 acc_val: 0.1625 time: 7.1062s\n",
            "Epoch: 0013 loss_train: 1.8742 acc_train: 0.3618 loss_val: 2.2410 acc_val: 0.1931 time: 7.6920s\n",
            "Epoch: 0014 loss_train: 1.8402 acc_train: 0.3656 loss_val: 2.2309 acc_val: 0.2207 time: 8.2816s\n",
            "Epoch: 0015 loss_train: 1.7979 acc_train: 0.3698 loss_val: 2.2175 acc_val: 0.2175 time: 8.8481s\n",
            "Epoch: 0016 loss_train: 1.7798 acc_train: 0.3680 loss_val: 2.2080 acc_val: 0.2218 time: 9.4270s\n",
            "Epoch: 0017 loss_train: 1.7541 acc_train: 0.3684 loss_val: 2.2034 acc_val: 0.2229 time: 10.0155s\n",
            "Epoch: 0018 loss_train: 1.7137 acc_train: 0.3717 loss_val: 2.2007 acc_val: 0.2200 time: 10.5851s\n",
            "Epoch: 0019 loss_train: 1.6934 acc_train: 0.3722 loss_val: 2.1927 acc_val: 0.2171 time: 11.1550s\n",
            "Epoch: 0020 loss_train: 1.6497 acc_train: 0.3760 loss_val: 2.1809 acc_val: 0.2200 time: 11.7131s\n",
            "Epoch: 0021 loss_train: 1.6425 acc_train: 0.3800 loss_val: 2.1745 acc_val: 0.2189 time: 12.2848s\n",
            "Epoch: 0022 loss_train: 1.6077 acc_train: 0.3883 loss_val: 2.1721 acc_val: 0.2196 time: 12.8477s\n",
            "Epoch: 0023 loss_train: 1.5807 acc_train: 0.4023 loss_val: 2.1686 acc_val: 0.2236 time: 13.4186s\n",
            "Epoch: 0024 loss_train: 1.5558 acc_train: 0.4111 loss_val: 2.1597 acc_val: 0.2320 time: 13.9938s\n",
            "Epoch: 0025 loss_train: 1.5292 acc_train: 0.4275 loss_val: 2.1486 acc_val: 0.2385 time: 14.5659s\n",
            "Epoch: 0026 loss_train: 1.5018 acc_train: 0.4368 loss_val: 2.1406 acc_val: 0.2415 time: 15.1387s\n",
            "Epoch: 0027 loss_train: 1.4876 acc_train: 0.4438 loss_val: 2.1335 acc_val: 0.2425 time: 15.7043s\n",
            "Epoch: 0028 loss_train: 1.4622 acc_train: 0.4530 loss_val: 2.1250 acc_val: 0.2495 time: 16.2674s\n",
            "Epoch: 0029 loss_train: 1.4324 acc_train: 0.4644 loss_val: 2.1146 acc_val: 0.2640 time: 16.8393s\n",
            "Epoch: 0030 loss_train: 1.4263 acc_train: 0.4664 loss_val: 2.1051 acc_val: 0.2756 time: 17.4250s\n",
            "Epoch: 0031 loss_train: 1.3987 acc_train: 0.4862 loss_val: 2.1003 acc_val: 0.2800 time: 17.9974s\n",
            "Epoch: 0032 loss_train: 1.3801 acc_train: 0.5572 loss_val: 2.0910 acc_val: 0.2858 time: 18.5588s\n",
            "Epoch: 0033 loss_train: 1.3416 acc_train: 0.5950 loss_val: 2.0771 acc_val: 0.3029 time: 19.1245s\n",
            "Epoch: 0034 loss_train: 1.3259 acc_train: 0.5936 loss_val: 2.0655 acc_val: 0.3153 time: 19.6957s\n",
            "Epoch: 0035 loss_train: 1.3020 acc_train: 0.5941 loss_val: 2.0645 acc_val: 0.2855 time: 20.2810s\n",
            "Epoch: 0036 loss_train: 1.2671 acc_train: 0.6403 loss_val: 2.0559 acc_val: 0.2800 time: 20.8642s\n",
            "Epoch: 0037 loss_train: 1.2459 acc_train: 0.6455 loss_val: 2.0417 acc_val: 0.3084 time: 21.4410s\n",
            "Epoch: 0038 loss_train: 1.2112 acc_train: 0.6614 loss_val: 2.0315 acc_val: 0.3229 time: 22.0287s\n",
            "Epoch: 0039 loss_train: 1.1912 acc_train: 0.6557 loss_val: 2.0255 acc_val: 0.3247 time: 22.6248s\n",
            "Epoch: 0040 loss_train: 1.1621 acc_train: 0.6635 loss_val: 2.0270 acc_val: 0.3076 time: 23.2162s\n",
            "Epoch: 0041 loss_train: 1.1422 acc_train: 0.6487 loss_val: 2.0137 acc_val: 0.3287 time: 23.8001s\n",
            "Epoch: 0042 loss_train: 1.1447 acc_train: 0.6452 loss_val: 2.0119 acc_val: 0.3422 time: 24.3937s\n",
            "Epoch: 0043 loss_train: 1.1135 acc_train: 0.6687 loss_val: 2.0114 acc_val: 0.3473 time: 24.9774s\n",
            "Epoch: 0044 loss_train: 1.0903 acc_train: 0.6795 loss_val: 1.9985 acc_val: 0.3702 time: 25.5627s\n",
            "Epoch: 0045 loss_train: 1.0814 acc_train: 0.6774 loss_val: 1.9936 acc_val: 0.3811 time: 26.1316s\n",
            "Epoch: 0046 loss_train: 1.0823 acc_train: 0.6811 loss_val: 1.9866 acc_val: 0.3822 time: 26.7006s\n",
            "Epoch: 0047 loss_train: 1.0428 acc_train: 0.6864 loss_val: 1.9811 acc_val: 0.3825 time: 27.2682s\n",
            "Epoch: 0048 loss_train: 1.0369 acc_train: 0.6926 loss_val: 1.9808 acc_val: 0.3727 time: 27.8567s\n",
            "Epoch: 0049 loss_train: 1.0152 acc_train: 0.7010 loss_val: 1.9718 acc_val: 0.3833 time: 28.4246s\n",
            "Epoch: 0050 loss_train: 0.9771 acc_train: 0.7048 loss_val: 1.9579 acc_val: 0.4033 time: 28.9884s\n",
            "Epoch: 0051 loss_train: 0.9681 acc_train: 0.6957 loss_val: 1.9477 acc_val: 0.4138 time: 29.5707s\n",
            "Epoch: 0052 loss_train: 0.9576 acc_train: 0.7029 loss_val: 1.9418 acc_val: 0.4135 time: 30.1418s\n",
            "Epoch: 0053 loss_train: 0.9425 acc_train: 0.7103 loss_val: 1.9531 acc_val: 0.3891 time: 30.7157s\n",
            "Epoch: 0054 loss_train: 0.9341 acc_train: 0.7142 loss_val: 1.9520 acc_val: 0.3895 time: 31.2904s\n",
            "Epoch: 0055 loss_train: 0.9194 acc_train: 0.7198 loss_val: 1.9346 acc_val: 0.4309 time: 31.8852s\n",
            "Epoch: 0056 loss_train: 0.9027 acc_train: 0.7145 loss_val: 1.9245 acc_val: 0.4527 time: 32.4681s\n",
            "Epoch: 0057 loss_train: 0.8928 acc_train: 0.7184 loss_val: 1.9242 acc_val: 0.4425 time: 33.0382s\n",
            "Epoch: 0058 loss_train: 0.8772 acc_train: 0.7252 loss_val: 1.9272 acc_val: 0.4204 time: 33.6040s\n",
            "Epoch: 0059 loss_train: 0.8589 acc_train: 0.7304 loss_val: 1.9227 acc_val: 0.4138 time: 34.1851s\n",
            "Epoch: 0060 loss_train: 0.8486 acc_train: 0.7230 loss_val: 1.9123 acc_val: 0.4309 time: 34.7676s\n",
            "Epoch: 0061 loss_train: 0.8488 acc_train: 0.7207 loss_val: 1.9050 acc_val: 0.4531 time: 35.3326s\n",
            "Epoch: 0062 loss_train: 0.8311 acc_train: 0.7296 loss_val: 1.9049 acc_val: 0.4556 time: 35.9154s\n",
            "Epoch: 0063 loss_train: 0.8311 acc_train: 0.7380 loss_val: 1.8945 acc_val: 0.4622 time: 36.4797s\n",
            "Epoch: 0064 loss_train: 0.8099 acc_train: 0.7395 loss_val: 1.8935 acc_val: 0.4553 time: 37.0422s\n",
            "Epoch: 0065 loss_train: 0.8006 acc_train: 0.7374 loss_val: 1.8975 acc_val: 0.4367 time: 37.6324s\n",
            "Epoch: 0066 loss_train: 0.7923 acc_train: 0.7414 loss_val: 1.8834 acc_val: 0.4611 time: 38.1962s\n",
            "Epoch: 0067 loss_train: 0.7813 acc_train: 0.7431 loss_val: 1.8776 acc_val: 0.4756 time: 38.7562s\n",
            "Epoch: 0068 loss_train: 0.7783 acc_train: 0.7475 loss_val: 1.8768 acc_val: 0.4785 time: 39.3200s\n",
            "Epoch: 0069 loss_train: 0.7927 acc_train: 0.7421 loss_val: 1.8748 acc_val: 0.4775 time: 39.8879s\n",
            "Epoch: 0070 loss_train: 0.7698 acc_train: 0.7518 loss_val: 1.8698 acc_val: 0.4771 time: 40.4616s\n",
            "Epoch: 0071 loss_train: 0.7583 acc_train: 0.7448 loss_val: 1.8675 acc_val: 0.4782 time: 41.0255s\n",
            "Epoch: 0072 loss_train: 0.7437 acc_train: 0.7513 loss_val: 1.8655 acc_val: 0.4829 time: 41.5765s\n",
            "Epoch: 0073 loss_train: 0.7610 acc_train: 0.7524 loss_val: 1.8565 acc_val: 0.5022 time: 42.1549s\n",
            "Epoch: 0074 loss_train: 0.7359 acc_train: 0.7596 loss_val: 1.8483 acc_val: 0.5135 time: 42.7232s\n",
            "Epoch: 0075 loss_train: 0.7490 acc_train: 0.7575 loss_val: 1.8471 acc_val: 0.5127 time: 43.2761s\n",
            "Epoch: 0076 loss_train: 0.7241 acc_train: 0.7587 loss_val: 1.8515 acc_val: 0.5062 time: 43.8466s\n",
            "Epoch: 0077 loss_train: 0.7216 acc_train: 0.7612 loss_val: 1.8505 acc_val: 0.5065 time: 44.4028s\n",
            "Epoch: 0078 loss_train: 0.7274 acc_train: 0.7595 loss_val: 1.8419 acc_val: 0.5156 time: 44.9739s\n",
            "Epoch: 0079 loss_train: 0.7186 acc_train: 0.7561 loss_val: 1.8285 acc_val: 0.5371 time: 45.5220s\n",
            "Epoch: 0080 loss_train: 0.7107 acc_train: 0.7651 loss_val: 1.8269 acc_val: 0.5407 time: 46.0942s\n",
            "Epoch: 0081 loss_train: 0.7113 acc_train: 0.7646 loss_val: 1.8331 acc_val: 0.5331 time: 46.6578s\n",
            "Epoch: 0082 loss_train: 0.6908 acc_train: 0.7765 loss_val: 1.8366 acc_val: 0.5324 time: 47.2338s\n",
            "Epoch: 0083 loss_train: 0.7035 acc_train: 0.7707 loss_val: 1.8293 acc_val: 0.5440 time: 47.7970s\n",
            "Epoch: 0084 loss_train: 0.6874 acc_train: 0.7759 loss_val: 1.8234 acc_val: 0.5440 time: 48.3578s\n",
            "Epoch: 0085 loss_train: 0.6891 acc_train: 0.7764 loss_val: 1.8182 acc_val: 0.5524 time: 48.9125s\n",
            "Epoch: 0086 loss_train: 0.6976 acc_train: 0.7709 loss_val: 1.8136 acc_val: 0.5644 time: 49.4569s\n",
            "Epoch: 0087 loss_train: 0.6852 acc_train: 0.7780 loss_val: 1.8183 acc_val: 0.5640 time: 50.0128s\n",
            "Epoch: 0088 loss_train: 0.6784 acc_train: 0.7817 loss_val: 1.8241 acc_val: 0.5465 time: 50.5767s\n",
            "Epoch: 0089 loss_train: 0.6718 acc_train: 0.7953 loss_val: 1.8146 acc_val: 0.5629 time: 51.1396s\n",
            "Epoch: 0090 loss_train: 0.6720 acc_train: 0.7871 loss_val: 1.8064 acc_val: 0.5702 time: 51.6887s\n",
            "Epoch: 0091 loss_train: 0.6782 acc_train: 0.7758 loss_val: 1.8043 acc_val: 0.5749 time: 52.2474s\n",
            "Epoch: 0092 loss_train: 0.6522 acc_train: 0.7936 loss_val: 1.8070 acc_val: 0.5713 time: 52.8102s\n",
            "Epoch: 0093 loss_train: 0.6596 acc_train: 0.7941 loss_val: 1.8078 acc_val: 0.5742 time: 53.3759s\n",
            "Epoch: 0094 loss_train: 0.6669 acc_train: 0.7945 loss_val: 1.8030 acc_val: 0.5789 time: 53.9365s\n",
            "Epoch: 0095 loss_train: 0.6376 acc_train: 0.8045 loss_val: 1.7894 acc_val: 0.5927 time: 54.4856s\n",
            "Epoch: 0096 loss_train: 0.6412 acc_train: 0.8000 loss_val: 1.7772 acc_val: 0.6007 time: 55.0440s\n",
            "Epoch: 0097 loss_train: 0.6529 acc_train: 0.7950 loss_val: 1.7823 acc_val: 0.5982 time: 55.5852s\n",
            "Epoch: 0098 loss_train: 0.6453 acc_train: 0.7963 loss_val: 1.7937 acc_val: 0.5862 time: 56.1434s\n",
            "Epoch: 0099 loss_train: 0.6288 acc_train: 0.8093 loss_val: 1.7951 acc_val: 0.5869 time: 56.6970s\n",
            "Epoch: 0100 loss_train: 0.6256 acc_train: 0.8089 loss_val: 1.7870 acc_val: 0.5913 time: 57.2534s\n",
            "Epoch: 0101 loss_train: 0.6260 acc_train: 0.8113 loss_val: 1.7756 acc_val: 0.6087 time: 57.8012s\n",
            "Epoch: 0102 loss_train: 0.6222 acc_train: 0.8091 loss_val: 1.7705 acc_val: 0.6102 time: 58.3804s\n",
            "Epoch: 0103 loss_train: 0.6202 acc_train: 0.8149 loss_val: 1.7726 acc_val: 0.6109 time: 58.9301s\n",
            "Epoch: 0104 loss_train: 0.6126 acc_train: 0.8241 loss_val: 1.7712 acc_val: 0.6124 time: 59.4867s\n",
            "Epoch: 0105 loss_train: 0.6026 acc_train: 0.8179 loss_val: 1.7643 acc_val: 0.6211 time: 60.0476s\n",
            "Epoch: 0106 loss_train: 0.6049 acc_train: 0.8139 loss_val: 1.7580 acc_val: 0.6225 time: 60.6104s\n",
            "Epoch: 0107 loss_train: 0.6117 acc_train: 0.8079 loss_val: 1.7621 acc_val: 0.6178 time: 61.1775s\n",
            "Epoch: 0108 loss_train: 0.6050 acc_train: 0.8164 loss_val: 1.7711 acc_val: 0.6109 time: 61.7567s\n",
            "Epoch: 0109 loss_train: 0.6008 acc_train: 0.8212 loss_val: 1.7701 acc_val: 0.6098 time: 62.3395s\n",
            "Epoch: 0110 loss_train: 0.5978 acc_train: 0.8209 loss_val: 1.7647 acc_val: 0.6138 time: 62.9079s\n",
            "Epoch: 0111 loss_train: 0.6073 acc_train: 0.8206 loss_val: 1.7564 acc_val: 0.6225 time: 63.4687s\n",
            "Epoch: 0112 loss_train: 0.5878 acc_train: 0.8291 loss_val: 1.7522 acc_val: 0.6298 time: 64.0314s\n",
            "Epoch: 0113 loss_train: 0.5948 acc_train: 0.8204 loss_val: 1.7520 acc_val: 0.6335 time: 64.5844s\n",
            "Epoch: 0114 loss_train: 0.5781 acc_train: 0.8260 loss_val: 1.7548 acc_val: 0.6302 time: 65.1579s\n",
            "Epoch: 0115 loss_train: 0.5869 acc_train: 0.8276 loss_val: 1.7517 acc_val: 0.6342 time: 65.7173s\n",
            "Epoch: 0116 loss_train: 0.5746 acc_train: 0.8322 loss_val: 1.7463 acc_val: 0.6400 time: 66.2834s\n",
            "Epoch: 0117 loss_train: 0.5793 acc_train: 0.8267 loss_val: 1.7423 acc_val: 0.6447 time: 66.8483s\n",
            "Epoch: 0118 loss_train: 0.5867 acc_train: 0.8258 loss_val: 1.7469 acc_val: 0.6411 time: 67.4158s\n",
            "Epoch: 0119 loss_train: 0.5784 acc_train: 0.8330 loss_val: 1.7479 acc_val: 0.6422 time: 67.9757s\n",
            "Epoch: 0120 loss_train: 0.5832 acc_train: 0.8251 loss_val: 1.7393 acc_val: 0.6458 time: 68.5371s\n",
            "Epoch: 0121 loss_train: 0.5733 acc_train: 0.8296 loss_val: 1.7344 acc_val: 0.6480 time: 69.1005s\n",
            "Epoch: 0122 loss_train: 0.5757 acc_train: 0.8339 loss_val: 1.7370 acc_val: 0.6465 time: 69.6552s\n",
            "Epoch: 0123 loss_train: 0.5803 acc_train: 0.8215 loss_val: 1.7416 acc_val: 0.6444 time: 70.1971s\n",
            "Epoch: 0124 loss_train: 0.5868 acc_train: 0.8229 loss_val: 1.7400 acc_val: 0.6505 time: 70.7384s\n",
            "Epoch: 0125 loss_train: 0.5771 acc_train: 0.8291 loss_val: 1.7316 acc_val: 0.6531 time: 71.2941s\n",
            "Epoch: 0126 loss_train: 0.5663 acc_train: 0.8357 loss_val: 1.7250 acc_val: 0.6571 time: 71.8267s\n",
            "Epoch: 0127 loss_train: 0.5624 acc_train: 0.8278 loss_val: 1.7254 acc_val: 0.6538 time: 72.3778s\n",
            "Epoch: 0128 loss_train: 0.5705 acc_train: 0.8268 loss_val: 1.7383 acc_val: 0.6480 time: 72.9216s\n",
            "Epoch: 0129 loss_train: 0.5656 acc_train: 0.8377 loss_val: 1.7392 acc_val: 0.6476 time: 73.4615s\n",
            "Epoch: 0130 loss_train: 0.5498 acc_train: 0.8406 loss_val: 1.7274 acc_val: 0.6538 time: 74.0162s\n",
            "Epoch: 0131 loss_train: 0.5773 acc_train: 0.8208 loss_val: 1.7208 acc_val: 0.6585 time: 74.5509s\n",
            "Epoch: 0132 loss_train: 0.5528 acc_train: 0.8335 loss_val: 1.7230 acc_val: 0.6549 time: 75.0985s\n",
            "Epoch: 0133 loss_train: 0.5677 acc_train: 0.8245 loss_val: 1.7300 acc_val: 0.6520 time: 75.6456s\n",
            "Epoch: 0134 loss_train: 0.5511 acc_train: 0.8355 loss_val: 1.7300 acc_val: 0.6535 time: 76.1908s\n",
            "Epoch: 0135 loss_train: 0.5525 acc_train: 0.8362 loss_val: 1.7117 acc_val: 0.6655 time: 76.7289s\n",
            "Epoch: 0136 loss_train: 0.5417 acc_train: 0.8374 loss_val: 1.7029 acc_val: 0.6731 time: 77.2780s\n",
            "Epoch: 0137 loss_train: 0.5514 acc_train: 0.8357 loss_val: 1.7078 acc_val: 0.6716 time: 77.8195s\n",
            "Epoch: 0138 loss_train: 0.5344 acc_train: 0.8370 loss_val: 1.7205 acc_val: 0.6673 time: 78.3687s\n",
            "Epoch: 0139 loss_train: 0.5490 acc_train: 0.8316 loss_val: 1.7263 acc_val: 0.6560 time: 78.9280s\n",
            "Epoch: 0140 loss_train: 0.5560 acc_train: 0.8383 loss_val: 1.7155 acc_val: 0.6633 time: 79.4654s\n",
            "Epoch: 0141 loss_train: 0.5382 acc_train: 0.8403 loss_val: 1.7060 acc_val: 0.6680 time: 80.0109s\n",
            "Epoch: 0142 loss_train: 0.5376 acc_train: 0.8358 loss_val: 1.7046 acc_val: 0.6731 time: 80.5548s\n",
            "Epoch: 0143 loss_train: 0.5441 acc_train: 0.8331 loss_val: 1.7103 acc_val: 0.6720 time: 81.1220s\n",
            "Epoch: 0144 loss_train: 0.5413 acc_train: 0.8408 loss_val: 1.7162 acc_val: 0.6720 time: 81.6669s\n",
            "Epoch: 0145 loss_train: 0.5448 acc_train: 0.8423 loss_val: 1.7044 acc_val: 0.6742 time: 82.2167s\n",
            "Epoch: 0146 loss_train: 0.5318 acc_train: 0.8380 loss_val: 1.6992 acc_val: 0.6745 time: 82.7583s\n",
            "Epoch: 0147 loss_train: 0.5330 acc_train: 0.8423 loss_val: 1.6987 acc_val: 0.6753 time: 83.2970s\n",
            "Epoch: 0148 loss_train: 0.5278 acc_train: 0.8366 loss_val: 1.6996 acc_val: 0.6873 time: 83.8491s\n",
            "Epoch: 0149 loss_train: 0.5399 acc_train: 0.8392 loss_val: 1.7073 acc_val: 0.6825 time: 84.3919s\n",
            "Epoch: 0150 loss_train: 0.5100 acc_train: 0.8514 loss_val: 1.7064 acc_val: 0.6800 time: 84.9399s\n",
            "Epoch: 0151 loss_train: 0.5331 acc_train: 0.8376 loss_val: 1.6955 acc_val: 0.6825 time: 85.4890s\n",
            "Epoch: 0152 loss_train: 0.5298 acc_train: 0.8476 loss_val: 1.6839 acc_val: 0.6778 time: 86.0330s\n",
            "Epoch: 0153 loss_train: 0.5348 acc_train: 0.8391 loss_val: 1.6832 acc_val: 0.6862 time: 86.5786s\n",
            "Epoch: 0154 loss_train: 0.5235 acc_train: 0.8381 loss_val: 1.6998 acc_val: 0.6909 time: 87.1202s\n",
            "Epoch: 0155 loss_train: 0.5170 acc_train: 0.8527 loss_val: 1.7107 acc_val: 0.6847 time: 87.6688s\n",
            "Epoch: 0156 loss_train: 0.5323 acc_train: 0.8510 loss_val: 1.6929 acc_val: 0.6829 time: 88.2109s\n",
            "Epoch: 0157 loss_train: 0.5092 acc_train: 0.8481 loss_val: 1.6802 acc_val: 0.6836 time: 88.7628s\n",
            "Epoch: 0158 loss_train: 0.5219 acc_train: 0.8437 loss_val: 1.6805 acc_val: 0.6905 time: 89.3202s\n",
            "Epoch: 0159 loss_train: 0.5260 acc_train: 0.8445 loss_val: 1.6811 acc_val: 0.6956 time: 89.8657s\n",
            "Epoch: 0160 loss_train: 0.5202 acc_train: 0.8427 loss_val: 1.6912 acc_val: 0.6960 time: 90.3998s\n",
            "Epoch: 0161 loss_train: 0.5177 acc_train: 0.8471 loss_val: 1.6898 acc_val: 0.6938 time: 90.9378s\n",
            "Epoch: 0162 loss_train: 0.5101 acc_train: 0.8478 loss_val: 1.6847 acc_val: 0.6789 time: 91.4864s\n",
            "Epoch: 0163 loss_train: 0.5165 acc_train: 0.8484 loss_val: 1.6761 acc_val: 0.6815 time: 92.0369s\n",
            "Epoch: 0164 loss_train: 0.5092 acc_train: 0.8499 loss_val: 1.6735 acc_val: 0.6909 time: 92.5721s\n",
            "Epoch: 0165 loss_train: 0.5041 acc_train: 0.8504 loss_val: 1.6833 acc_val: 0.6942 time: 93.1079s\n",
            "Epoch: 0166 loss_train: 0.4986 acc_train: 0.8542 loss_val: 1.6815 acc_val: 0.6942 time: 93.6496s\n",
            "Epoch: 0167 loss_train: 0.5048 acc_train: 0.8507 loss_val: 1.6683 acc_val: 0.6982 time: 94.1839s\n",
            "Epoch: 0168 loss_train: 0.5012 acc_train: 0.8462 loss_val: 1.6617 acc_val: 0.7000 time: 94.7226s\n",
            "Epoch: 0169 loss_train: 0.4956 acc_train: 0.8482 loss_val: 1.6676 acc_val: 0.6985 time: 95.2639s\n",
            "Epoch: 0170 loss_train: 0.5004 acc_train: 0.8475 loss_val: 1.6792 acc_val: 0.6978 time: 95.8115s\n",
            "Epoch: 0171 loss_train: 0.4923 acc_train: 0.8579 loss_val: 1.6732 acc_val: 0.6989 time: 96.3562s\n",
            "Epoch: 0172 loss_train: 0.4918 acc_train: 0.8513 loss_val: 1.6655 acc_val: 0.7015 time: 96.9055s\n",
            "Epoch: 0173 loss_train: 0.4891 acc_train: 0.8534 loss_val: 1.6668 acc_val: 0.6967 time: 97.4326s\n",
            "Epoch: 0174 loss_train: 0.4898 acc_train: 0.8532 loss_val: 1.6757 acc_val: 0.6851 time: 97.9877s\n",
            "Epoch: 0175 loss_train: 0.4906 acc_train: 0.8538 loss_val: 1.6761 acc_val: 0.6891 time: 98.5401s\n",
            "Epoch: 0176 loss_train: 0.4844 acc_train: 0.8506 loss_val: 1.6686 acc_val: 0.6967 time: 99.1058s\n",
            "Epoch: 0177 loss_train: 0.4835 acc_train: 0.8527 loss_val: 1.6602 acc_val: 0.6993 time: 99.6381s\n",
            "Epoch: 0178 loss_train: 0.4887 acc_train: 0.8519 loss_val: 1.6593 acc_val: 0.6931 time: 100.1736s\n",
            "Epoch: 0179 loss_train: 0.4866 acc_train: 0.8557 loss_val: 1.6546 acc_val: 0.6993 time: 100.7043s\n",
            "Epoch: 0180 loss_train: 0.4878 acc_train: 0.8524 loss_val: 1.6499 acc_val: 0.7098 time: 101.2366s\n",
            "Epoch: 0181 loss_train: 0.4780 acc_train: 0.8535 loss_val: 1.6538 acc_val: 0.7087 time: 101.7902s\n",
            "Epoch: 0182 loss_train: 0.4758 acc_train: 0.8581 loss_val: 1.6635 acc_val: 0.7062 time: 102.3228s\n",
            "Epoch: 0183 loss_train: 0.4830 acc_train: 0.8571 loss_val: 1.6612 acc_val: 0.7007 time: 102.8693s\n",
            "Epoch: 0184 loss_train: 0.4862 acc_train: 0.8500 loss_val: 1.6549 acc_val: 0.7058 time: 103.4074s\n",
            "Epoch: 0185 loss_train: 0.4769 acc_train: 0.8581 loss_val: 1.6484 acc_val: 0.7095 time: 103.9615s\n",
            "Epoch: 0186 loss_train: 0.4779 acc_train: 0.8547 loss_val: 1.6545 acc_val: 0.7135 time: 104.4973s\n",
            "Epoch: 0187 loss_train: 0.4750 acc_train: 0.8525 loss_val: 1.6602 acc_val: 0.7189 time: 105.0526s\n",
            "Epoch: 0188 loss_train: 0.4760 acc_train: 0.8613 loss_val: 1.6517 acc_val: 0.7124 time: 105.5908s\n",
            "Epoch: 0189 loss_train: 0.4759 acc_train: 0.8541 loss_val: 1.6466 acc_val: 0.7142 time: 106.1453s\n",
            "Epoch: 0190 loss_train: 0.4536 acc_train: 0.8627 loss_val: 1.6451 acc_val: 0.7196 time: 106.6832s\n",
            "Epoch: 0191 loss_train: 0.4653 acc_train: 0.8632 loss_val: 1.6456 acc_val: 0.7127 time: 107.2229s\n",
            "Epoch: 0192 loss_train: 0.4716 acc_train: 0.8498 loss_val: 1.6431 acc_val: 0.7138 time: 107.7535s\n",
            "Epoch: 0193 loss_train: 0.4672 acc_train: 0.8557 loss_val: 1.6442 acc_val: 0.7185 time: 108.2910s\n",
            "Epoch: 0194 loss_train: 0.4712 acc_train: 0.8574 loss_val: 1.6477 acc_val: 0.7102 time: 108.8286s\n",
            "Epoch: 0195 loss_train: 0.4645 acc_train: 0.8603 loss_val: 1.6420 acc_val: 0.7131 time: 109.3687s\n",
            "Epoch: 0196 loss_train: 0.4728 acc_train: 0.8542 loss_val: 1.6435 acc_val: 0.7189 time: 109.9069s\n",
            "Epoch: 0197 loss_train: 0.4667 acc_train: 0.8629 loss_val: 1.6387 acc_val: 0.7124 time: 110.4470s\n",
            "Epoch: 0198 loss_train: 0.4661 acc_train: 0.8520 loss_val: 1.6379 acc_val: 0.7153 time: 110.9924s\n",
            "Epoch: 0199 loss_train: 0.4517 acc_train: 0.8626 loss_val: 1.6381 acc_val: 0.7236 time: 111.5357s\n",
            "Epoch: 0200 loss_train: 0.4752 acc_train: 0.8561 loss_val: 1.6361 acc_val: 0.7196 time: 112.0729s\n",
            "Epoch: 0201 loss_train: 0.4452 acc_train: 0.8660 loss_val: 1.6378 acc_val: 0.7171 time: 112.6150s\n",
            "Epoch: 0202 loss_train: 0.4597 acc_train: 0.8565 loss_val: 1.6428 acc_val: 0.7200 time: 113.1612s\n",
            "Epoch: 0203 loss_train: 0.4597 acc_train: 0.8614 loss_val: 1.6402 acc_val: 0.7222 time: 113.6950s\n",
            "Epoch: 0204 loss_train: 0.4486 acc_train: 0.8661 loss_val: 1.6348 acc_val: 0.7167 time: 114.2398s\n",
            "Epoch: 0205 loss_train: 0.4592 acc_train: 0.8562 loss_val: 1.6349 acc_val: 0.7215 time: 114.7833s\n",
            "Epoch: 0206 loss_train: 0.4512 acc_train: 0.8656 loss_val: 1.6354 acc_val: 0.7233 time: 115.3257s\n",
            "Epoch: 0207 loss_train: 0.4532 acc_train: 0.8657 loss_val: 1.6224 acc_val: 0.7207 time: 115.8635s\n",
            "Epoch: 0208 loss_train: 0.4604 acc_train: 0.8576 loss_val: 1.6225 acc_val: 0.7215 time: 116.4035s\n",
            "Epoch: 0209 loss_train: 0.4492 acc_train: 0.8618 loss_val: 1.6290 acc_val: 0.7273 time: 116.9412s\n",
            "Epoch: 0210 loss_train: 0.4445 acc_train: 0.8712 loss_val: 1.6214 acc_val: 0.7262 time: 117.4748s\n",
            "Epoch: 0211 loss_train: 0.4444 acc_train: 0.8644 loss_val: 1.6192 acc_val: 0.7233 time: 118.0100s\n",
            "Epoch: 0212 loss_train: 0.4581 acc_train: 0.8550 loss_val: 1.6338 acc_val: 0.7247 time: 118.5478s\n",
            "Epoch: 0213 loss_train: 0.4588 acc_train: 0.8671 loss_val: 1.6387 acc_val: 0.7127 time: 119.1101s\n",
            "Epoch: 0214 loss_train: 0.4543 acc_train: 0.8613 loss_val: 1.6269 acc_val: 0.7240 time: 119.6596s\n",
            "Epoch: 0215 loss_train: 0.4509 acc_train: 0.8632 loss_val: 1.6253 acc_val: 0.7236 time: 120.1949s\n",
            "Epoch: 0216 loss_train: 0.4565 acc_train: 0.8607 loss_val: 1.6265 acc_val: 0.7222 time: 120.7284s\n",
            "Epoch: 0217 loss_train: 0.4439 acc_train: 0.8656 loss_val: 1.6148 acc_val: 0.7287 time: 121.2735s\n",
            "Epoch: 0218 loss_train: 0.4315 acc_train: 0.8665 loss_val: 1.6071 acc_val: 0.7349 time: 121.8271s\n",
            "Epoch: 0219 loss_train: 0.4505 acc_train: 0.8616 loss_val: 1.6141 acc_val: 0.7320 time: 122.3669s\n",
            "Epoch: 0220 loss_train: 0.4425 acc_train: 0.8666 loss_val: 1.6164 acc_val: 0.7178 time: 122.9096s\n",
            "Epoch: 0221 loss_train: 0.4461 acc_train: 0.8562 loss_val: 1.6133 acc_val: 0.7287 time: 123.4423s\n",
            "Epoch: 0222 loss_train: 0.4224 acc_train: 0.8720 loss_val: 1.6186 acc_val: 0.7313 time: 123.9884s\n",
            "Epoch: 0223 loss_train: 0.4400 acc_train: 0.8652 loss_val: 1.6261 acc_val: 0.7258 time: 124.5392s\n",
            "Epoch: 0224 loss_train: 0.4360 acc_train: 0.8719 loss_val: 1.6113 acc_val: 0.7236 time: 125.0814s\n",
            "Epoch: 0225 loss_train: 0.4391 acc_train: 0.8632 loss_val: 1.6051 acc_val: 0.7331 time: 125.6279s\n",
            "Epoch: 0226 loss_train: 0.4419 acc_train: 0.8667 loss_val: 1.6121 acc_val: 0.7364 time: 126.1733s\n",
            "Epoch: 0227 loss_train: 0.4362 acc_train: 0.8663 loss_val: 1.6196 acc_val: 0.7302 time: 126.6996s\n",
            "Epoch: 0228 loss_train: 0.4355 acc_train: 0.8632 loss_val: 1.6205 acc_val: 0.7309 time: 127.2278s\n",
            "Epoch: 0229 loss_train: 0.4409 acc_train: 0.8661 loss_val: 1.6087 acc_val: 0.7305 time: 127.7580s\n",
            "Epoch: 0230 loss_train: 0.4365 acc_train: 0.8678 loss_val: 1.6115 acc_val: 0.7269 time: 128.3035s\n",
            "Epoch: 0231 loss_train: 0.4378 acc_train: 0.8641 loss_val: 1.6105 acc_val: 0.7265 time: 128.8430s\n",
            "Epoch: 0232 loss_train: 0.4362 acc_train: 0.8668 loss_val: 1.6106 acc_val: 0.7429 time: 129.3940s\n",
            "Epoch: 0233 loss_train: 0.4368 acc_train: 0.8695 loss_val: 1.6052 acc_val: 0.7378 time: 129.9599s\n",
            "Epoch: 0234 loss_train: 0.4391 acc_train: 0.8589 loss_val: 1.6098 acc_val: 0.7269 time: 130.5085s\n",
            "Epoch: 0235 loss_train: 0.4334 acc_train: 0.8681 loss_val: 1.6045 acc_val: 0.7316 time: 131.0527s\n",
            "Epoch: 0236 loss_train: 0.4294 acc_train: 0.8674 loss_val: 1.5982 acc_val: 0.7331 time: 131.5791s\n",
            "Epoch: 0237 loss_train: 0.4295 acc_train: 0.8644 loss_val: 1.6143 acc_val: 0.7400 time: 132.1249s\n",
            "Epoch: 0238 loss_train: 0.4390 acc_train: 0.8699 loss_val: 1.6157 acc_val: 0.7295 time: 132.6673s\n",
            "Epoch: 0239 loss_train: 0.4289 acc_train: 0.8692 loss_val: 1.6030 acc_val: 0.7331 time: 133.2053s\n",
            "Epoch: 0240 loss_train: 0.4214 acc_train: 0.8715 loss_val: 1.6000 acc_val: 0.7335 time: 133.7408s\n",
            "Epoch: 0241 loss_train: 0.4264 acc_train: 0.8691 loss_val: 1.6016 acc_val: 0.7335 time: 134.2649s\n",
            "Epoch: 0242 loss_train: 0.4215 acc_train: 0.8699 loss_val: 1.6076 acc_val: 0.7400 time: 134.7917s\n",
            "Epoch: 0243 loss_train: 0.4264 acc_train: 0.8668 loss_val: 1.6007 acc_val: 0.7393 time: 135.3261s\n",
            "Epoch: 0244 loss_train: 0.4229 acc_train: 0.8735 loss_val: 1.5943 acc_val: 0.7353 time: 135.8552s\n",
            "Epoch: 0245 loss_train: 0.4362 acc_train: 0.8621 loss_val: 1.5990 acc_val: 0.7298 time: 136.4003s\n",
            "Epoch: 0246 loss_train: 0.4159 acc_train: 0.8742 loss_val: 1.6016 acc_val: 0.7316 time: 136.9324s\n",
            "Epoch: 0247 loss_train: 0.4225 acc_train: 0.8738 loss_val: 1.6038 acc_val: 0.7335 time: 137.4587s\n",
            "Epoch: 0248 loss_train: 0.4199 acc_train: 0.8721 loss_val: 1.6045 acc_val: 0.7389 time: 137.9878s\n",
            "Epoch: 0249 loss_train: 0.4306 acc_train: 0.8736 loss_val: 1.5909 acc_val: 0.7393 time: 138.5096s\n",
            "Epoch: 0250 loss_train: 0.4178 acc_train: 0.8674 loss_val: 1.5840 acc_val: 0.7371 time: 139.0408s\n",
            "Epoch: 0251 loss_train: 0.4227 acc_train: 0.8712 loss_val: 1.5944 acc_val: 0.7389 time: 139.5728s\n",
            "Epoch: 0252 loss_train: 0.4172 acc_train: 0.8752 loss_val: 1.6049 acc_val: 0.7324 time: 140.1237s\n",
            "Epoch: 0253 loss_train: 0.4278 acc_train: 0.8762 loss_val: 1.5991 acc_val: 0.7375 time: 140.6618s\n",
            "Epoch: 0254 loss_train: 0.4286 acc_train: 0.8661 loss_val: 1.6024 acc_val: 0.7382 time: 141.1927s\n",
            "Epoch: 0255 loss_train: 0.4302 acc_train: 0.8716 loss_val: 1.5932 acc_val: 0.7382 time: 141.9982s\n",
            "Epoch: 0256 loss_train: 0.4305 acc_train: 0.8651 loss_val: 1.5855 acc_val: 0.7415 time: 142.5287s\n",
            "Epoch: 0257 loss_train: 0.4275 acc_train: 0.8671 loss_val: 1.5864 acc_val: 0.7455 time: 143.0650s\n",
            "Epoch: 0258 loss_train: 0.4162 acc_train: 0.8727 loss_val: 1.5912 acc_val: 0.7455 time: 143.5985s\n",
            "Epoch: 0259 loss_train: 0.4145 acc_train: 0.8734 loss_val: 1.5967 acc_val: 0.7429 time: 144.1367s\n",
            "Epoch: 0260 loss_train: 0.4192 acc_train: 0.8759 loss_val: 1.5926 acc_val: 0.7382 time: 144.6679s\n",
            "Epoch: 0261 loss_train: 0.4234 acc_train: 0.8707 loss_val: 1.5932 acc_val: 0.7407 time: 145.2055s\n",
            "Epoch: 0262 loss_train: 0.4193 acc_train: 0.8756 loss_val: 1.5934 acc_val: 0.7440 time: 145.7474s\n",
            "Epoch: 0263 loss_train: 0.4172 acc_train: 0.8758 loss_val: 1.5858 acc_val: 0.7400 time: 146.2950s\n",
            "Epoch: 0264 loss_train: 0.4101 acc_train: 0.8722 loss_val: 1.5854 acc_val: 0.7404 time: 146.8315s\n",
            "Epoch: 0265 loss_train: 0.4126 acc_train: 0.8741 loss_val: 1.5875 acc_val: 0.7425 time: 147.3665s\n",
            "Epoch: 0266 loss_train: 0.4167 acc_train: 0.8744 loss_val: 1.5861 acc_val: 0.7407 time: 147.9110s\n",
            "Epoch: 0267 loss_train: 0.4135 acc_train: 0.8731 loss_val: 1.5818 acc_val: 0.7327 time: 148.4439s\n",
            "Epoch: 0268 loss_train: 0.4094 acc_train: 0.8749 loss_val: 1.5824 acc_val: 0.7407 time: 148.9864s\n",
            "Epoch: 0269 loss_train: 0.4080 acc_train: 0.8735 loss_val: 1.5866 acc_val: 0.7324 time: 149.5262s\n",
            "Epoch: 0270 loss_train: 0.4080 acc_train: 0.8751 loss_val: 1.5857 acc_val: 0.7349 time: 150.0702s\n",
            "Epoch: 0271 loss_train: 0.4067 acc_train: 0.8734 loss_val: 1.5850 acc_val: 0.7436 time: 150.6201s\n",
            "Epoch: 0272 loss_train: 0.4242 acc_train: 0.8721 loss_val: 1.5851 acc_val: 0.7364 time: 151.1495s\n",
            "Epoch: 0273 loss_train: 0.4087 acc_train: 0.8709 loss_val: 1.5917 acc_val: 0.7291 time: 151.6859s\n",
            "Epoch: 0274 loss_train: 0.4158 acc_train: 0.8723 loss_val: 1.5879 acc_val: 0.7378 time: 152.2419s\n",
            "Epoch: 0275 loss_train: 0.4156 acc_train: 0.8724 loss_val: 1.5809 acc_val: 0.7331 time: 152.7831s\n",
            "Epoch: 0276 loss_train: 0.4115 acc_train: 0.8729 loss_val: 1.5851 acc_val: 0.7455 time: 153.3122s\n",
            "Epoch: 0277 loss_train: 0.4070 acc_train: 0.8778 loss_val: 1.5801 acc_val: 0.7418 time: 153.8546s\n",
            "Epoch: 0278 loss_train: 0.4167 acc_train: 0.8765 loss_val: 1.5691 acc_val: 0.7385 time: 154.3831s\n",
            "Epoch: 0279 loss_train: 0.3995 acc_train: 0.8709 loss_val: 1.5762 acc_val: 0.7429 time: 154.9289s\n",
            "Epoch: 0280 loss_train: 0.4027 acc_train: 0.8721 loss_val: 1.5869 acc_val: 0.7473 time: 155.4584s\n",
            "Epoch: 0281 loss_train: 0.4136 acc_train: 0.8782 loss_val: 1.5831 acc_val: 0.7404 time: 156.0119s\n",
            "Epoch: 0282 loss_train: 0.4164 acc_train: 0.8673 loss_val: 1.5842 acc_val: 0.7371 time: 156.5565s\n",
            "Epoch: 0283 loss_train: 0.4036 acc_train: 0.8742 loss_val: 1.5786 acc_val: 0.7418 time: 157.0920s\n",
            "Epoch: 0284 loss_train: 0.4085 acc_train: 0.8780 loss_val: 1.5700 acc_val: 0.7295 time: 157.6250s\n",
            "Epoch: 0285 loss_train: 0.4197 acc_train: 0.8654 loss_val: 1.5899 acc_val: 0.7487 time: 158.1607s\n",
            "Epoch: 0286 loss_train: 0.4238 acc_train: 0.8735 loss_val: 1.5842 acc_val: 0.7436 time: 158.7034s\n",
            "Epoch: 0287 loss_train: 0.4244 acc_train: 0.8746 loss_val: 1.5667 acc_val: 0.7233 time: 159.2515s\n",
            "Epoch: 0288 loss_train: 0.4314 acc_train: 0.8580 loss_val: 1.5787 acc_val: 0.7407 time: 159.7845s\n",
            "Epoch: 0289 loss_train: 0.4079 acc_train: 0.8746 loss_val: 1.5978 acc_val: 0.7411 time: 160.3243s\n",
            "Epoch: 0290 loss_train: 0.4252 acc_train: 0.8751 loss_val: 1.5898 acc_val: 0.7331 time: 160.8680s\n",
            "Epoch: 0291 loss_train: 0.4200 acc_train: 0.8675 loss_val: 1.5777 acc_val: 0.7400 time: 161.3967s\n",
            "Epoch: 0292 loss_train: 0.4116 acc_train: 0.8711 loss_val: 1.5757 acc_val: 0.7455 time: 161.9337s\n",
            "Epoch: 0293 loss_train: 0.4070 acc_train: 0.8784 loss_val: 1.5645 acc_val: 0.7258 time: 162.4761s\n",
            "Epoch: 0294 loss_train: 0.4152 acc_train: 0.8680 loss_val: 1.5597 acc_val: 0.7385 time: 163.0293s\n",
            "Epoch: 0295 loss_train: 0.4010 acc_train: 0.8773 loss_val: 1.5720 acc_val: 0.7440 time: 163.5611s\n",
            "Epoch: 0296 loss_train: 0.4113 acc_train: 0.8737 loss_val: 1.5829 acc_val: 0.7444 time: 164.1014s\n",
            "Epoch: 0297 loss_train: 0.3993 acc_train: 0.8815 loss_val: 1.5815 acc_val: 0.7298 time: 164.6297s\n",
            "Epoch: 0298 loss_train: 0.4030 acc_train: 0.8749 loss_val: 1.5667 acc_val: 0.7342 time: 165.1712s\n",
            "Epoch: 0299 loss_train: 0.4057 acc_train: 0.8744 loss_val: 1.5594 acc_val: 0.7364 time: 165.6988s\n",
            "Epoch: 0300 loss_train: 0.4103 acc_train: 0.8695 loss_val: 1.5759 acc_val: 0.7524 time: 166.2335s\n",
            "Epoch: 0301 loss_train: 0.4117 acc_train: 0.8774 loss_val: 1.5731 acc_val: 0.7451 time: 166.7680s\n",
            "Epoch: 0302 loss_train: 0.3959 acc_train: 0.8779 loss_val: 1.5672 acc_val: 0.7436 time: 167.3080s\n",
            "Epoch: 0303 loss_train: 0.4009 acc_train: 0.8719 loss_val: 1.5700 acc_val: 0.7447 time: 167.8303s\n",
            "Epoch: 0304 loss_train: 0.4008 acc_train: 0.8786 loss_val: 1.5697 acc_val: 0.7360 time: 168.3643s\n",
            "Epoch: 0305 loss_train: 0.3964 acc_train: 0.8779 loss_val: 1.5608 acc_val: 0.7433 time: 168.8947s\n",
            "Epoch: 0306 loss_train: 0.4077 acc_train: 0.8732 loss_val: 1.5586 acc_val: 0.7516 time: 169.4270s\n",
            "Epoch: 0307 loss_train: 0.4126 acc_train: 0.8734 loss_val: 1.5577 acc_val: 0.7407 time: 169.9573s\n",
            "Epoch: 0308 loss_train: 0.4063 acc_train: 0.8661 loss_val: 1.5690 acc_val: 0.7462 time: 170.4831s\n",
            "Epoch: 0309 loss_train: 0.3962 acc_train: 0.8796 loss_val: 1.5721 acc_val: 0.7505 time: 171.0302s\n",
            "Epoch: 0310 loss_train: 0.4022 acc_train: 0.8802 loss_val: 1.5674 acc_val: 0.7400 time: 171.5683s\n",
            "Epoch: 0311 loss_train: 0.4089 acc_train: 0.8770 loss_val: 1.5746 acc_val: 0.7356 time: 172.1032s\n",
            "Epoch: 0312 loss_train: 0.4023 acc_train: 0.8737 loss_val: 1.5646 acc_val: 0.7484 time: 172.6406s\n",
            "Epoch: 0313 loss_train: 0.4078 acc_train: 0.8805 loss_val: 1.5512 acc_val: 0.7360 time: 173.1893s\n",
            "Epoch: 0314 loss_train: 0.4074 acc_train: 0.8702 loss_val: 1.5615 acc_val: 0.7458 time: 173.7266s\n",
            "Epoch: 0315 loss_train: 0.4112 acc_train: 0.8725 loss_val: 1.5689 acc_val: 0.7567 time: 174.2668s\n",
            "Epoch: 0316 loss_train: 0.4022 acc_train: 0.8804 loss_val: 1.5644 acc_val: 0.7429 time: 174.7990s\n",
            "Epoch: 0317 loss_train: 0.4102 acc_train: 0.8717 loss_val: 1.5653 acc_val: 0.7389 time: 175.3289s\n",
            "Epoch: 0318 loss_train: 0.3976 acc_train: 0.8749 loss_val: 1.5679 acc_val: 0.7375 time: 175.8555s\n",
            "Epoch: 0319 loss_train: 0.4102 acc_train: 0.8764 loss_val: 1.5558 acc_val: 0.7429 time: 176.3858s\n",
            "Epoch: 0320 loss_train: 0.4007 acc_train: 0.8751 loss_val: 1.5579 acc_val: 0.7447 time: 176.9326s\n",
            "Epoch: 0321 loss_train: 0.4024 acc_train: 0.8743 loss_val: 1.5715 acc_val: 0.7542 time: 177.4605s\n",
            "Epoch: 0322 loss_train: 0.3952 acc_train: 0.8841 loss_val: 1.5657 acc_val: 0.7444 time: 177.9964s\n",
            "Epoch: 0323 loss_train: 0.3901 acc_train: 0.8792 loss_val: 1.5549 acc_val: 0.7385 time: 178.5467s\n",
            "Epoch: 0324 loss_train: 0.3941 acc_train: 0.8764 loss_val: 1.5570 acc_val: 0.7433 time: 179.0883s\n",
            "Epoch: 0325 loss_train: 0.3862 acc_train: 0.8804 loss_val: 1.5587 acc_val: 0.7444 time: 179.6183s\n",
            "Epoch: 0326 loss_train: 0.3854 acc_train: 0.8829 loss_val: 1.5516 acc_val: 0.7422 time: 180.1671s\n",
            "Epoch: 0327 loss_train: 0.3926 acc_train: 0.8736 loss_val: 1.5532 acc_val: 0.7505 time: 180.7091s\n",
            "Epoch: 0328 loss_train: 0.3939 acc_train: 0.8808 loss_val: 1.5496 acc_val: 0.7385 time: 181.2541s\n",
            "Epoch: 0329 loss_train: 0.3889 acc_train: 0.8804 loss_val: 1.5548 acc_val: 0.7338 time: 181.7872s\n",
            "Epoch: 0330 loss_train: 0.3947 acc_train: 0.8736 loss_val: 1.5647 acc_val: 0.7440 time: 182.3407s\n",
            "Epoch: 0331 loss_train: 0.3876 acc_train: 0.8830 loss_val: 1.5645 acc_val: 0.7451 time: 182.8858s\n",
            "Epoch: 0332 loss_train: 0.3901 acc_train: 0.8806 loss_val: 1.5605 acc_val: 0.7436 time: 183.4180s\n",
            "Epoch: 0333 loss_train: 0.3730 acc_train: 0.8865 loss_val: 1.5518 acc_val: 0.7400 time: 183.9521s\n",
            "Epoch: 0334 loss_train: 0.3935 acc_train: 0.8761 loss_val: 1.5515 acc_val: 0.7473 time: 184.4855s\n",
            "Epoch: 0335 loss_train: 0.3889 acc_train: 0.8751 loss_val: 1.5499 acc_val: 0.7495 time: 185.0367s\n",
            "Epoch: 0336 loss_train: 0.3832 acc_train: 0.8845 loss_val: 1.5508 acc_val: 0.7444 time: 185.5710s\n",
            "Epoch: 0337 loss_train: 0.3897 acc_train: 0.8791 loss_val: 1.5510 acc_val: 0.7425 time: 186.1193s\n",
            "Epoch: 0338 loss_train: 0.3881 acc_train: 0.8805 loss_val: 1.5515 acc_val: 0.7404 time: 186.6635s\n",
            "Epoch: 0339 loss_train: 0.3898 acc_train: 0.8793 loss_val: 1.5504 acc_val: 0.7338 time: 187.1951s\n",
            "Epoch: 0340 loss_train: 0.3911 acc_train: 0.8769 loss_val: 1.5573 acc_val: 0.7524 time: 187.7297s\n",
            "Epoch: 0341 loss_train: 0.3932 acc_train: 0.8823 loss_val: 1.5581 acc_val: 0.7585 time: 188.2618s\n",
            "Epoch: 0342 loss_train: 0.3834 acc_train: 0.8823 loss_val: 1.5474 acc_val: 0.7447 time: 188.8127s\n",
            "Epoch: 0343 loss_train: 0.3874 acc_train: 0.8804 loss_val: 1.5454 acc_val: 0.7425 time: 189.3641s\n",
            "Epoch: 0344 loss_train: 0.3857 acc_train: 0.8803 loss_val: 1.5556 acc_val: 0.7505 time: 189.9086s\n",
            "Epoch: 0345 loss_train: 0.3988 acc_train: 0.8802 loss_val: 1.5578 acc_val: 0.7465 time: 190.4360s\n",
            "Epoch: 0346 loss_train: 0.3942 acc_train: 0.8776 loss_val: 1.5534 acc_val: 0.7498 time: 190.9863s\n",
            "Epoch: 0347 loss_train: 0.3853 acc_train: 0.8872 loss_val: 1.5433 acc_val: 0.7451 time: 191.5274s\n",
            "Epoch: 0348 loss_train: 0.3823 acc_train: 0.8854 loss_val: 1.5373 acc_val: 0.7338 time: 192.0684s\n",
            "Epoch: 0349 loss_train: 0.3884 acc_train: 0.8727 loss_val: 1.5517 acc_val: 0.7502 time: 192.6215s\n",
            "Epoch: 0350 loss_train: 0.3930 acc_train: 0.8815 loss_val: 1.5492 acc_val: 0.7495 time: 193.1668s\n",
            "Epoch: 0351 loss_train: 0.3857 acc_train: 0.8831 loss_val: 1.5429 acc_val: 0.7320 time: 193.6958s\n",
            "Epoch: 0352 loss_train: 0.3782 acc_train: 0.8807 loss_val: 1.5518 acc_val: 0.7389 time: 194.2208s\n",
            "Epoch: 0353 loss_train: 0.3980 acc_train: 0.8763 loss_val: 1.5536 acc_val: 0.7520 time: 194.7575s\n",
            "Epoch: 0354 loss_train: 0.3814 acc_train: 0.8863 loss_val: 1.5466 acc_val: 0.7491 time: 195.2826s\n",
            "Epoch: 0355 loss_train: 0.3821 acc_train: 0.8824 loss_val: 1.5391 acc_val: 0.7396 time: 195.8121s\n",
            "Epoch: 0356 loss_train: 0.3838 acc_train: 0.8776 loss_val: 1.5443 acc_val: 0.7473 time: 196.3358s\n",
            "Epoch: 0357 loss_train: 0.3935 acc_train: 0.8822 loss_val: 1.5382 acc_val: 0.7440 time: 196.8726s\n",
            "Epoch: 0358 loss_train: 0.3885 acc_train: 0.8790 loss_val: 1.5447 acc_val: 0.7425 time: 197.4131s\n",
            "Epoch: 0359 loss_train: 0.3855 acc_train: 0.8812 loss_val: 1.5521 acc_val: 0.7382 time: 197.9540s\n",
            "Epoch: 0360 loss_train: 0.3833 acc_train: 0.8833 loss_val: 1.5447 acc_val: 0.7425 time: 198.4810s\n",
            "Epoch: 0361 loss_train: 0.3766 acc_train: 0.8829 loss_val: 1.5379 acc_val: 0.7436 time: 199.0101s\n",
            "Epoch: 0362 loss_train: 0.3802 acc_train: 0.8811 loss_val: 1.5423 acc_val: 0.7498 time: 199.5484s\n",
            "Epoch: 0363 loss_train: 0.3721 acc_train: 0.8856 loss_val: 1.5429 acc_val: 0.7495 time: 200.0816s\n",
            "Epoch: 0364 loss_train: 0.3759 acc_train: 0.8826 loss_val: 1.5338 acc_val: 0.7495 time: 200.6190s\n",
            "Epoch: 0365 loss_train: 0.3807 acc_train: 0.8830 loss_val: 1.5298 acc_val: 0.7335 time: 201.1557s\n",
            "Epoch: 0366 loss_train: 0.3860 acc_train: 0.8758 loss_val: 1.5470 acc_val: 0.7473 time: 201.6991s\n",
            "Epoch: 0367 loss_train: 0.3782 acc_train: 0.8848 loss_val: 1.5523 acc_val: 0.7513 time: 202.2233s\n",
            "Epoch: 0368 loss_train: 0.3801 acc_train: 0.8844 loss_val: 1.5431 acc_val: 0.7382 time: 202.7654s\n",
            "Epoch: 0369 loss_train: 0.3783 acc_train: 0.8818 loss_val: 1.5374 acc_val: 0.7385 time: 203.3159s\n",
            "Epoch: 0370 loss_train: 0.3701 acc_train: 0.8858 loss_val: 1.5356 acc_val: 0.7407 time: 203.8627s\n",
            "Epoch: 0371 loss_train: 0.3812 acc_train: 0.8832 loss_val: 1.5308 acc_val: 0.7429 time: 204.4012s\n",
            "Epoch: 0372 loss_train: 0.3789 acc_train: 0.8817 loss_val: 1.5384 acc_val: 0.7545 time: 204.9391s\n",
            "Epoch: 0373 loss_train: 0.3767 acc_train: 0.8841 loss_val: 1.5400 acc_val: 0.7458 time: 205.4702s\n",
            "Epoch: 0374 loss_train: 0.3759 acc_train: 0.8838 loss_val: 1.5411 acc_val: 0.7418 time: 206.0131s\n",
            "Epoch: 0375 loss_train: 0.3790 acc_train: 0.8812 loss_val: 1.5400 acc_val: 0.7433 time: 206.5520s\n",
            "Epoch: 0376 loss_train: 0.3774 acc_train: 0.8832 loss_val: 1.5401 acc_val: 0.7444 time: 207.0868s\n",
            "Epoch: 0377 loss_train: 0.3776 acc_train: 0.8823 loss_val: 1.5443 acc_val: 0.7440 time: 207.6144s\n",
            "Epoch: 0378 loss_train: 0.3743 acc_train: 0.8826 loss_val: 1.5377 acc_val: 0.7495 time: 208.1516s\n",
            "Epoch: 0379 loss_train: 0.3855 acc_train: 0.8818 loss_val: 1.5316 acc_val: 0.7498 time: 208.6885s\n",
            "Epoch: 0380 loss_train: 0.3791 acc_train: 0.8865 loss_val: 1.5263 acc_val: 0.7360 time: 209.2202s\n",
            "Epoch: 0381 loss_train: 0.3785 acc_train: 0.8759 loss_val: 1.5389 acc_val: 0.7411 time: 209.7566s\n",
            "Epoch: 0382 loss_train: 0.3878 acc_train: 0.8811 loss_val: 1.5407 acc_val: 0.7513 time: 210.2949s\n",
            "Epoch: 0383 loss_train: 0.3747 acc_train: 0.8869 loss_val: 1.5374 acc_val: 0.7287 time: 210.8239s\n",
            "Epoch: 0384 loss_train: 0.4017 acc_train: 0.8669 loss_val: 1.5500 acc_val: 0.7422 time: 211.3499s\n",
            "Epoch: 0385 loss_train: 0.3827 acc_train: 0.8898 loss_val: 1.5415 acc_val: 0.7407 time: 211.8906s\n",
            "Epoch: 0386 loss_train: 0.3773 acc_train: 0.8874 loss_val: 1.5266 acc_val: 0.7280 time: 212.4307s\n",
            "Epoch: 0387 loss_train: 0.3954 acc_train: 0.8731 loss_val: 1.5339 acc_val: 0.7527 time: 212.9647s\n",
            "Epoch: 0388 loss_train: 0.3782 acc_train: 0.8855 loss_val: 1.5447 acc_val: 0.7455 time: 213.5027s\n",
            "Epoch: 0389 loss_train: 0.3833 acc_train: 0.8828 loss_val: 1.5272 acc_val: 0.7415 time: 214.0371s\n",
            "Epoch: 0390 loss_train: 0.3781 acc_train: 0.8807 loss_val: 1.5311 acc_val: 0.7444 time: 214.5564s\n",
            "Epoch: 0391 loss_train: 0.3759 acc_train: 0.8850 loss_val: 1.5343 acc_val: 0.7407 time: 215.0943s\n",
            "Epoch: 0392 loss_train: 0.3751 acc_train: 0.8846 loss_val: 1.5354 acc_val: 0.7375 time: 215.6272s\n",
            "Epoch: 0393 loss_train: 0.3787 acc_train: 0.8836 loss_val: 1.5262 acc_val: 0.7487 time: 216.1579s\n",
            "Epoch: 0394 loss_train: 0.3633 acc_train: 0.8860 loss_val: 1.5201 acc_val: 0.7436 time: 216.6820s\n",
            "Epoch: 0395 loss_train: 0.3861 acc_train: 0.8817 loss_val: 1.5291 acc_val: 0.7465 time: 217.2105s\n",
            "Epoch: 0396 loss_train: 0.3768 acc_train: 0.8805 loss_val: 1.5373 acc_val: 0.7447 time: 217.7365s\n",
            "Epoch: 0397 loss_train: 0.3723 acc_train: 0.8884 loss_val: 1.5300 acc_val: 0.7465 time: 218.2646s\n",
            "Epoch: 0398 loss_train: 0.3738 acc_train: 0.8844 loss_val: 1.5252 acc_val: 0.7429 time: 218.7902s\n",
            "Epoch: 0399 loss_train: 0.3729 acc_train: 0.8838 loss_val: 1.5261 acc_val: 0.7433 time: 219.3191s\n",
            "Epoch: 0400 loss_train: 0.3669 acc_train: 0.8842 loss_val: 1.5297 acc_val: 0.7502 time: 219.8540s\n",
            "Epoch: 0401 loss_train: 0.3721 acc_train: 0.8841 loss_val: 1.5289 acc_val: 0.7531 time: 220.3876s\n",
            "Epoch: 0402 loss_train: 0.3779 acc_train: 0.8833 loss_val: 1.5238 acc_val: 0.7444 time: 220.9140s\n",
            "Epoch: 0403 loss_train: 0.3696 acc_train: 0.8832 loss_val: 1.5253 acc_val: 0.7338 time: 221.4387s\n",
            "Epoch: 0404 loss_train: 0.3695 acc_train: 0.8793 loss_val: 1.5290 acc_val: 0.7465 time: 221.9907s\n",
            "Epoch: 0405 loss_train: 0.3622 acc_train: 0.8854 loss_val: 1.5376 acc_val: 0.7578 time: 222.5187s\n",
            "Epoch: 0406 loss_train: 0.3829 acc_train: 0.8854 loss_val: 1.5239 acc_val: 0.7436 time: 223.0576s\n",
            "Epoch: 0407 loss_train: 0.3804 acc_train: 0.8845 loss_val: 1.5200 acc_val: 0.7422 time: 223.5972s\n",
            "Epoch: 0408 loss_train: 0.3845 acc_train: 0.8795 loss_val: 1.5256 acc_val: 0.7509 time: 224.1378s\n",
            "Epoch: 0409 loss_train: 0.3687 acc_train: 0.8859 loss_val: 1.5339 acc_val: 0.7513 time: 224.6619s\n",
            "Epoch: 0410 loss_train: 0.3739 acc_train: 0.8844 loss_val: 1.5336 acc_val: 0.7480 time: 225.1998s\n",
            "Epoch: 0411 loss_train: 0.3808 acc_train: 0.8824 loss_val: 1.5335 acc_val: 0.7487 time: 225.7300s\n",
            "Epoch: 0412 loss_train: 0.3863 acc_train: 0.8816 loss_val: 1.5213 acc_val: 0.7305 time: 226.3029s\n",
            "Epoch: 0413 loss_train: 0.3904 acc_train: 0.8723 loss_val: 1.5283 acc_val: 0.7556 time: 226.8420s\n",
            "Epoch: 0414 loss_train: 0.3817 acc_train: 0.8858 loss_val: 1.5323 acc_val: 0.7589 time: 227.3737s\n",
            "Epoch: 0415 loss_train: 0.3835 acc_train: 0.8830 loss_val: 1.5180 acc_val: 0.7255 time: 227.9033s\n",
            "Epoch: 0416 loss_train: 0.3897 acc_train: 0.8731 loss_val: 1.5258 acc_val: 0.7465 time: 228.4428s\n",
            "Epoch: 0417 loss_train: 0.3747 acc_train: 0.8886 loss_val: 1.5311 acc_val: 0.7473 time: 228.9839s\n",
            "Epoch: 0418 loss_train: 0.3655 acc_train: 0.8848 loss_val: 1.5321 acc_val: 0.7476 time: 229.5104s\n",
            "Epoch: 0419 loss_train: 0.3801 acc_train: 0.8843 loss_val: 1.5233 acc_val: 0.7516 time: 230.0558s\n",
            "Epoch: 0420 loss_train: 0.3618 acc_train: 0.8888 loss_val: 1.5176 acc_val: 0.7480 time: 230.5958s\n",
            "Epoch: 0421 loss_train: 0.3649 acc_train: 0.8875 loss_val: 1.5140 acc_val: 0.7396 time: 231.1427s\n",
            "Epoch: 0422 loss_train: 0.3727 acc_train: 0.8834 loss_val: 1.5169 acc_val: 0.7473 time: 231.6854s\n",
            "Epoch: 0423 loss_train: 0.3643 acc_train: 0.8914 loss_val: 1.5242 acc_val: 0.7502 time: 232.2280s\n",
            "Epoch: 0424 loss_train: 0.3680 acc_train: 0.8861 loss_val: 1.5216 acc_val: 0.7462 time: 232.7661s\n",
            "Epoch: 0425 loss_train: 0.3808 acc_train: 0.8816 loss_val: 1.5190 acc_val: 0.7393 time: 233.2955s\n",
            "Epoch: 0426 loss_train: 0.3682 acc_train: 0.8841 loss_val: 1.5197 acc_val: 0.7495 time: 233.8378s\n",
            "Epoch: 0427 loss_train: 0.3761 acc_train: 0.8875 loss_val: 1.5183 acc_val: 0.7455 time: 234.3747s\n",
            "Epoch: 0428 loss_train: 0.3672 acc_train: 0.8844 loss_val: 1.5207 acc_val: 0.7476 time: 234.9058s\n",
            "Epoch: 0429 loss_train: 0.3640 acc_train: 0.8869 loss_val: 1.5244 acc_val: 0.7520 time: 235.4331s\n",
            "Epoch: 0430 loss_train: 0.3721 acc_train: 0.8891 loss_val: 1.5182 acc_val: 0.7360 time: 235.9592s\n",
            "Epoch: 0431 loss_train: 0.3846 acc_train: 0.8789 loss_val: 1.5251 acc_val: 0.7484 time: 236.4891s\n",
            "Epoch: 0432 loss_train: 0.3665 acc_train: 0.8905 loss_val: 1.5208 acc_val: 0.7484 time: 237.0169s\n",
            "Epoch: 0433 loss_train: 0.3704 acc_train: 0.8869 loss_val: 1.5117 acc_val: 0.7342 time: 237.5340s\n",
            "Epoch: 0434 loss_train: 0.3768 acc_train: 0.8801 loss_val: 1.5156 acc_val: 0.7513 time: 238.0633s\n",
            "Epoch: 0435 loss_train: 0.3834 acc_train: 0.8816 loss_val: 1.5160 acc_val: 0.7298 time: 238.5968s\n",
            "Epoch: 0436 loss_train: 0.3839 acc_train: 0.8723 loss_val: 1.5259 acc_val: 0.7465 time: 239.1404s\n",
            "Epoch: 0437 loss_train: 0.3648 acc_train: 0.8841 loss_val: 1.5354 acc_val: 0.7531 time: 239.6716s\n",
            "Epoch: 0438 loss_train: 0.4017 acc_train: 0.8834 loss_val: 1.5273 acc_val: 0.7091 time: 240.1951s\n",
            "Epoch: 0439 loss_train: 0.4213 acc_train: 0.8605 loss_val: 1.5344 acc_val: 0.7367 time: 240.7245s\n",
            "Epoch: 0440 loss_train: 0.3745 acc_train: 0.8857 loss_val: 1.5423 acc_val: 0.7356 time: 241.2440s\n",
            "Epoch: 0441 loss_train: 0.4233 acc_train: 0.8774 loss_val: 1.5330 acc_val: 0.7222 time: 241.7859s\n",
            "Epoch: 0442 loss_train: 0.4388 acc_train: 0.8533 loss_val: 1.5146 acc_val: 0.7444 time: 242.3225s\n",
            "Epoch: 0443 loss_train: 0.3789 acc_train: 0.8777 loss_val: 1.5262 acc_val: 0.7502 time: 242.8627s\n",
            "Epoch: 0444 loss_train: 0.4323 acc_train: 0.8679 loss_val: 1.5197 acc_val: 0.7189 time: 243.3951s\n",
            "Epoch: 0445 loss_train: 0.4257 acc_train: 0.8531 loss_val: 1.5402 acc_val: 0.7236 time: 243.9358s\n",
            "Epoch: 0446 loss_train: 0.4188 acc_train: 0.8630 loss_val: 1.5348 acc_val: 0.7469 time: 244.4657s\n",
            "Epoch: 0447 loss_train: 0.4083 acc_train: 0.8773 loss_val: 1.5162 acc_val: 0.7484 time: 245.0086s\n",
            "Epoch: 0448 loss_train: 0.3916 acc_train: 0.8804 loss_val: 1.5039 acc_val: 0.7349 time: 245.5472s\n",
            "Epoch: 0449 loss_train: 0.3991 acc_train: 0.8709 loss_val: 1.5095 acc_val: 0.7215 time: 246.0792s\n",
            "Epoch: 0450 loss_train: 0.4140 acc_train: 0.8716 loss_val: 1.5155 acc_val: 0.7589 time: 246.6192s\n",
            "Epoch: 0451 loss_train: 0.3816 acc_train: 0.8853 loss_val: 1.5200 acc_val: 0.7382 time: 247.1470s\n",
            "Epoch: 0452 loss_train: 0.4274 acc_train: 0.8671 loss_val: 1.5209 acc_val: 0.7444 time: 247.6887s\n",
            "Epoch: 0453 loss_train: 0.3732 acc_train: 0.8804 loss_val: 1.5265 acc_val: 0.7313 time: 248.2088s\n",
            "Epoch: 0454 loss_train: 0.3936 acc_train: 0.8814 loss_val: 1.5107 acc_val: 0.7491 time: 248.7414s\n",
            "Epoch: 0455 loss_train: 0.3918 acc_train: 0.8776 loss_val: 1.5042 acc_val: 0.7524 time: 249.2727s\n",
            "Epoch: 0456 loss_train: 0.3761 acc_train: 0.8833 loss_val: 1.5083 acc_val: 0.7495 time: 249.8063s\n",
            "Epoch: 0457 loss_train: 0.3895 acc_train: 0.8736 loss_val: 1.5111 acc_val: 0.7476 time: 250.3348s\n",
            "Epoch: 0458 loss_train: 0.3938 acc_train: 0.8724 loss_val: 1.5003 acc_val: 0.7509 time: 250.8714s\n",
            "Epoch: 0459 loss_train: 0.3787 acc_train: 0.8815 loss_val: 1.5002 acc_val: 0.7502 time: 251.3874s\n",
            "Epoch: 0460 loss_train: 0.3719 acc_train: 0.8831 loss_val: 1.5062 acc_val: 0.7407 time: 251.9218s\n",
            "Epoch: 0461 loss_train: 0.3864 acc_train: 0.8809 loss_val: 1.5015 acc_val: 0.7458 time: 252.4459s\n",
            "Epoch: 0462 loss_train: 0.3636 acc_train: 0.8853 loss_val: 1.4987 acc_val: 0.7509 time: 253.0012s\n",
            "Epoch: 0463 loss_train: 0.3875 acc_train: 0.8742 loss_val: 1.5067 acc_val: 0.7655 time: 253.5430s\n",
            "Epoch: 0464 loss_train: 0.3755 acc_train: 0.8868 loss_val: 1.5012 acc_val: 0.7553 time: 254.1040s\n",
            "Epoch: 0465 loss_train: 0.3712 acc_train: 0.8805 loss_val: 1.4980 acc_val: 0.7444 time: 254.6217s\n",
            "Epoch: 0466 loss_train: 0.3717 acc_train: 0.8797 loss_val: 1.5035 acc_val: 0.7480 time: 255.1486s\n",
            "Epoch: 0467 loss_train: 0.3885 acc_train: 0.8780 loss_val: 1.5021 acc_val: 0.7465 time: 255.6734s\n",
            "Epoch: 0468 loss_train: 0.3669 acc_train: 0.8856 loss_val: 1.5110 acc_val: 0.7542 time: 256.1885s\n",
            "Epoch: 0469 loss_train: 0.3824 acc_train: 0.8761 loss_val: 1.5072 acc_val: 0.7618 time: 256.7164s\n",
            "Epoch: 0470 loss_train: 0.3716 acc_train: 0.8891 loss_val: 1.4898 acc_val: 0.7476 time: 257.2327s\n",
            "Epoch: 0471 loss_train: 0.3656 acc_train: 0.8878 loss_val: 1.4866 acc_val: 0.7338 time: 257.7591s\n",
            "Epoch: 0472 loss_train: 0.3733 acc_train: 0.8803 loss_val: 1.5000 acc_val: 0.7542 time: 258.2800s\n",
            "Epoch: 0473 loss_train: 0.3701 acc_train: 0.8857 loss_val: 1.5109 acc_val: 0.7640 time: 258.8115s\n",
            "Epoch: 0474 loss_train: 0.3662 acc_train: 0.8902 loss_val: 1.5025 acc_val: 0.7505 time: 259.3380s\n",
            "Epoch: 0475 loss_train: 0.3611 acc_train: 0.8870 loss_val: 1.5007 acc_val: 0.7469 time: 259.8805s\n",
            "Epoch: 0476 loss_train: 0.3578 acc_train: 0.8904 loss_val: 1.5001 acc_val: 0.7440 time: 260.4033s\n",
            "Epoch: 0477 loss_train: 0.3702 acc_train: 0.8828 loss_val: 1.4979 acc_val: 0.7524 time: 260.9365s\n",
            "Epoch: 0478 loss_train: 0.3592 acc_train: 0.8898 loss_val: 1.5018 acc_val: 0.7611 time: 261.4670s\n",
            "Epoch: 0479 loss_train: 0.3608 acc_train: 0.8907 loss_val: 1.4975 acc_val: 0.7582 time: 262.0045s\n",
            "Epoch: 0480 loss_train: 0.3547 acc_train: 0.8932 loss_val: 1.4876 acc_val: 0.7400 time: 262.5281s\n",
            "Epoch: 0481 loss_train: 0.3666 acc_train: 0.8806 loss_val: 1.4903 acc_val: 0.7462 time: 263.0792s\n",
            "Epoch: 0482 loss_train: 0.3678 acc_train: 0.8891 loss_val: 1.4991 acc_val: 0.7527 time: 263.6055s\n",
            "Epoch: 0483 loss_train: 0.3635 acc_train: 0.8898 loss_val: 1.5106 acc_val: 0.7524 time: 264.1572s\n",
            "Epoch: 0484 loss_train: 0.3776 acc_train: 0.8836 loss_val: 1.5119 acc_val: 0.7516 time: 264.6835s\n",
            "Epoch: 0485 loss_train: 0.3590 acc_train: 0.8907 loss_val: 1.5065 acc_val: 0.7436 time: 265.2194s\n",
            "Epoch: 0486 loss_train: 0.3622 acc_train: 0.8875 loss_val: 1.5007 acc_val: 0.7345 time: 265.7484s\n",
            "Epoch: 0487 loss_train: 0.3755 acc_train: 0.8817 loss_val: 1.4990 acc_val: 0.7345 time: 266.2746s\n",
            "Epoch: 0488 loss_train: 0.3711 acc_train: 0.8840 loss_val: 1.5037 acc_val: 0.7531 time: 266.8064s\n",
            "Epoch: 0489 loss_train: 0.3570 acc_train: 0.8916 loss_val: 1.5038 acc_val: 0.7604 time: 267.3285s\n",
            "Epoch: 0490 loss_train: 0.3605 acc_train: 0.8907 loss_val: 1.4909 acc_val: 0.7538 time: 267.8621s\n",
            "Epoch: 0491 loss_train: 0.3542 acc_train: 0.8869 loss_val: 1.4884 acc_val: 0.7389 time: 268.3911s\n",
            "Epoch: 0492 loss_train: 0.3672 acc_train: 0.8834 loss_val: 1.4934 acc_val: 0.7393 time: 268.9335s\n",
            "Epoch: 0493 loss_train: 0.3600 acc_train: 0.8893 loss_val: 1.5015 acc_val: 0.7440 time: 269.4597s\n",
            "Epoch: 0494 loss_train: 0.3553 acc_train: 0.8888 loss_val: 1.5082 acc_val: 0.7545 time: 270.0061s\n",
            "Epoch: 0495 loss_train: 0.3608 acc_train: 0.8893 loss_val: 1.5014 acc_val: 0.7455 time: 270.5221s\n",
            "Epoch: 0496 loss_train: 0.3511 acc_train: 0.8929 loss_val: 1.4964 acc_val: 0.7415 time: 271.0558s\n",
            "Epoch: 0497 loss_train: 0.3595 acc_train: 0.8885 loss_val: 1.4942 acc_val: 0.7476 time: 271.5784s\n",
            "Epoch: 0498 loss_train: 0.3493 acc_train: 0.8899 loss_val: 1.4942 acc_val: 0.7567 time: 272.1217s\n",
            "Epoch: 0499 loss_train: 0.3504 acc_train: 0.8915 loss_val: 1.4931 acc_val: 0.7585 time: 272.6510s\n",
            "Epoch: 0500 loss_train: 0.3658 acc_train: 0.8872 loss_val: 1.4949 acc_val: 0.7502 time: 273.1789s\n",
            "Epoch: 0501 loss_train: 0.3593 acc_train: 0.8897 loss_val: 1.4951 acc_val: 0.7393 time: 273.7031s\n",
            "Epoch: 0502 loss_train: 0.3632 acc_train: 0.8874 loss_val: 1.4993 acc_val: 0.7516 time: 274.2495s\n",
            "Epoch: 0503 loss_train: 0.3627 acc_train: 0.8892 loss_val: 1.5016 acc_val: 0.7491 time: 274.7789s\n",
            "Epoch: 0504 loss_train: 0.3643 acc_train: 0.8852 loss_val: 1.5084 acc_val: 0.7513 time: 275.3057s\n",
            "Epoch: 0505 loss_train: 0.3619 acc_train: 0.8920 loss_val: 1.4973 acc_val: 0.7378 time: 275.8506s\n",
            "Epoch: 0506 loss_train: 0.3606 acc_train: 0.8846 loss_val: 1.4917 acc_val: 0.7418 time: 276.3960s\n",
            "Epoch: 0507 loss_train: 0.3676 acc_train: 0.8843 loss_val: 1.4946 acc_val: 0.7484 time: 276.9314s\n",
            "Epoch: 0508 loss_train: 0.3596 acc_train: 0.8889 loss_val: 1.5027 acc_val: 0.7527 time: 277.4505s\n",
            "Epoch: 0509 loss_train: 0.3639 acc_train: 0.8870 loss_val: 1.5025 acc_val: 0.7571 time: 277.9876s\n",
            "Epoch: 0510 loss_train: 0.3579 acc_train: 0.8918 loss_val: 1.4913 acc_val: 0.7425 time: 278.5241s\n",
            "Epoch: 0511 loss_train: 0.3566 acc_train: 0.8856 loss_val: 1.4905 acc_val: 0.7491 time: 279.0805s\n",
            "Epoch: 0512 loss_train: 0.3587 acc_train: 0.8868 loss_val: 1.4945 acc_val: 0.7498 time: 279.6017s\n",
            "Epoch: 0513 loss_train: 0.3522 acc_train: 0.8937 loss_val: 1.4975 acc_val: 0.7458 time: 280.1462s\n",
            "Epoch: 0514 loss_train: 0.3641 acc_train: 0.8854 loss_val: 1.4944 acc_val: 0.7531 time: 280.6805s\n",
            "Epoch: 0515 loss_train: 0.3520 acc_train: 0.8926 loss_val: 1.4891 acc_val: 0.7458 time: 281.2087s\n",
            "Epoch: 0516 loss_train: 0.3559 acc_train: 0.8872 loss_val: 1.4902 acc_val: 0.7480 time: 281.7298s\n",
            "Epoch: 0517 loss_train: 0.3476 acc_train: 0.8915 loss_val: 1.4974 acc_val: 0.7542 time: 282.2654s\n",
            "Epoch: 0518 loss_train: 0.3616 acc_train: 0.8885 loss_val: 1.4971 acc_val: 0.7578 time: 282.7968s\n",
            "Epoch: 0519 loss_train: 0.3566 acc_train: 0.8928 loss_val: 1.4930 acc_val: 0.7451 time: 283.3367s\n",
            "Epoch: 0520 loss_train: 0.3566 acc_train: 0.8892 loss_val: 1.4971 acc_val: 0.7433 time: 283.8746s\n",
            "Epoch: 0521 loss_train: 0.3502 acc_train: 0.8905 loss_val: 1.4976 acc_val: 0.7469 time: 284.4249s\n",
            "Epoch: 0522 loss_train: 0.3559 acc_train: 0.8910 loss_val: 1.4900 acc_val: 0.7498 time: 284.9489s\n",
            "Epoch: 0523 loss_train: 0.3685 acc_train: 0.8861 loss_val: 1.4894 acc_val: 0.7465 time: 285.4814s\n",
            "Epoch: 0524 loss_train: 0.3635 acc_train: 0.8885 loss_val: 1.4933 acc_val: 0.7415 time: 286.0063s\n",
            "Epoch: 0525 loss_train: 0.3503 acc_train: 0.8916 loss_val: 1.4949 acc_val: 0.7480 time: 286.5511s\n",
            "Epoch: 0526 loss_train: 0.3557 acc_train: 0.8902 loss_val: 1.4989 acc_val: 0.7538 time: 287.0804s\n",
            "Epoch: 0527 loss_train: 0.3480 acc_train: 0.8942 loss_val: 1.5002 acc_val: 0.7524 time: 287.6108s\n",
            "Epoch: 0528 loss_train: 0.3517 acc_train: 0.8904 loss_val: 1.4972 acc_val: 0.7371 time: 288.1416s\n",
            "Epoch: 0529 loss_train: 0.3576 acc_train: 0.8913 loss_val: 1.4868 acc_val: 0.7396 time: 288.6683s\n",
            "Epoch: 0530 loss_train: 0.3487 acc_train: 0.8896 loss_val: 1.4858 acc_val: 0.7465 time: 289.2093s\n",
            "Epoch: 0531 loss_train: 0.3719 acc_train: 0.8848 loss_val: 1.4884 acc_val: 0.7415 time: 289.7410s\n",
            "Epoch: 0532 loss_train: 0.3507 acc_train: 0.8883 loss_val: 1.4959 acc_val: 0.7433 time: 290.2862s\n",
            "Epoch: 0533 loss_train: 0.3676 acc_train: 0.8848 loss_val: 1.4954 acc_val: 0.7491 time: 290.8268s\n",
            "Epoch: 0534 loss_train: 0.3488 acc_train: 0.8939 loss_val: 1.4944 acc_val: 0.7462 time: 291.3662s\n",
            "Epoch: 0535 loss_train: 0.3576 acc_train: 0.8872 loss_val: 1.4979 acc_val: 0.7458 time: 291.8950s\n",
            "Epoch: 0536 loss_train: 0.3483 acc_train: 0.8947 loss_val: 1.4952 acc_val: 0.7360 time: 292.4313s\n",
            "Epoch: 0537 loss_train: 0.3601 acc_train: 0.8849 loss_val: 1.4874 acc_val: 0.7465 time: 292.9649s\n",
            "Epoch: 0538 loss_train: 0.3571 acc_train: 0.8861 loss_val: 1.4858 acc_val: 0.7505 time: 293.5037s\n",
            "Epoch: 0539 loss_train: 0.3670 acc_train: 0.8891 loss_val: 1.4918 acc_val: 0.7473 time: 294.0484s\n",
            "Epoch: 0540 loss_train: 0.3528 acc_train: 0.8924 loss_val: 1.4964 acc_val: 0.7385 time: 294.5963s\n",
            "Epoch: 0541 loss_train: 0.3590 acc_train: 0.8869 loss_val: 1.4948 acc_val: 0.7491 time: 295.1227s\n",
            "Epoch: 0542 loss_train: 0.3549 acc_train: 0.8927 loss_val: 1.4889 acc_val: 0.7444 time: 295.6562s\n",
            "Epoch: 0543 loss_train: 0.3530 acc_train: 0.8893 loss_val: 1.4864 acc_val: 0.7309 time: 296.1967s\n",
            "Epoch: 0544 loss_train: 0.3679 acc_train: 0.8829 loss_val: 1.5014 acc_val: 0.7436 time: 296.7364s\n",
            "Epoch: 0545 loss_train: 0.3760 acc_train: 0.8873 loss_val: 1.4935 acc_val: 0.7444 time: 297.2690s\n",
            "Epoch: 0546 loss_train: 0.3554 acc_train: 0.8896 loss_val: 1.4900 acc_val: 0.7375 time: 297.8100s\n",
            "Epoch: 0547 loss_train: 0.3698 acc_train: 0.8862 loss_val: 1.4877 acc_val: 0.7451 time: 298.3431s\n",
            "Epoch: 0548 loss_train: 0.3583 acc_train: 0.8875 loss_val: 1.4915 acc_val: 0.7429 time: 298.8810s\n",
            "Epoch: 0549 loss_train: 0.3701 acc_train: 0.8836 loss_val: 1.4922 acc_val: 0.7396 time: 299.4100s\n",
            "Epoch: 0550 loss_train: 0.3680 acc_train: 0.8812 loss_val: 1.5075 acc_val: 0.7585 time: 299.9477s\n",
            "Epoch: 0551 loss_train: 0.3847 acc_train: 0.8921 loss_val: 1.4895 acc_val: 0.7276 time: 300.5021s\n",
            "Epoch: 0552 loss_train: 0.3613 acc_train: 0.8822 loss_val: 1.4848 acc_val: 0.7324 time: 301.0307s\n",
            "Epoch: 0553 loss_train: 0.3574 acc_train: 0.8857 loss_val: 1.4915 acc_val: 0.7633 time: 301.5536s\n",
            "Epoch: 0554 loss_train: 0.3727 acc_train: 0.8873 loss_val: 1.4895 acc_val: 0.7455 time: 302.0816s\n",
            "Epoch: 0555 loss_train: 0.3715 acc_train: 0.8792 loss_val: 1.4915 acc_val: 0.7484 time: 302.6221s\n",
            "Epoch: 0556 loss_train: 0.3549 acc_train: 0.8891 loss_val: 1.4887 acc_val: 0.7418 time: 303.1490s\n",
            "Epoch: 0557 loss_train: 0.3563 acc_train: 0.8866 loss_val: 1.4847 acc_val: 0.7425 time: 303.6754s\n",
            "Epoch: 0558 loss_train: 0.3515 acc_train: 0.8908 loss_val: 1.4860 acc_val: 0.7433 time: 304.1975s\n",
            "Epoch: 0559 loss_train: 0.3538 acc_train: 0.8877 loss_val: 1.4889 acc_val: 0.7495 time: 304.7302s\n",
            "Epoch: 0560 loss_train: 0.3513 acc_train: 0.8909 loss_val: 1.4805 acc_val: 0.7444 time: 305.2742s\n",
            "Epoch: 0561 loss_train: 0.3569 acc_train: 0.8850 loss_val: 1.4802 acc_val: 0.7433 time: 305.8106s\n",
            "Epoch: 0562 loss_train: 0.3484 acc_train: 0.8912 loss_val: 1.4938 acc_val: 0.7604 time: 306.3396s\n",
            "Epoch: 0563 loss_train: 0.3539 acc_train: 0.8896 loss_val: 1.4924 acc_val: 0.7469 time: 306.8832s\n",
            "Epoch: 0564 loss_train: 0.3561 acc_train: 0.8895 loss_val: 1.4829 acc_val: 0.7360 time: 307.4006s\n",
            "Epoch: 0565 loss_train: 0.3601 acc_train: 0.8814 loss_val: 1.4807 acc_val: 0.7509 time: 307.9290s\n",
            "Epoch: 0566 loss_train: 0.3523 acc_train: 0.8918 loss_val: 1.4828 acc_val: 0.7560 time: 308.4509s\n",
            "Epoch: 0567 loss_train: 0.3476 acc_train: 0.8908 loss_val: 1.4866 acc_val: 0.7404 time: 308.9917s\n",
            "Epoch: 0568 loss_train: 0.3545 acc_train: 0.8855 loss_val: 1.4826 acc_val: 0.7404 time: 309.5154s\n",
            "Epoch: 0569 loss_train: 0.3544 acc_train: 0.8912 loss_val: 1.4826 acc_val: 0.7444 time: 310.0469s\n",
            "Epoch: 0570 loss_train: 0.3565 acc_train: 0.8912 loss_val: 1.4833 acc_val: 0.7400 time: 310.5644s\n",
            "Epoch: 0571 loss_train: 0.3569 acc_train: 0.8872 loss_val: 1.4963 acc_val: 0.7556 time: 311.0826s\n",
            "Epoch: 0572 loss_train: 0.3543 acc_train: 0.8935 loss_val: 1.4847 acc_val: 0.7415 time: 311.6179s\n",
            "Epoch: 0573 loss_train: 0.3458 acc_train: 0.8932 loss_val: 1.4788 acc_val: 0.7320 time: 312.1503s\n",
            "Epoch: 0574 loss_train: 0.3639 acc_train: 0.8825 loss_val: 1.4876 acc_val: 0.7542 time: 312.6814s\n",
            "Epoch: 0575 loss_train: 0.3551 acc_train: 0.8915 loss_val: 1.4901 acc_val: 0.7429 time: 313.2072s\n",
            "Epoch: 0576 loss_train: 0.3553 acc_train: 0.8912 loss_val: 1.4847 acc_val: 0.7404 time: 313.7345s\n",
            "Epoch: 0577 loss_train: 0.3576 acc_train: 0.8850 loss_val: 1.4912 acc_val: 0.7505 time: 314.2505s\n",
            "Epoch: 0578 loss_train: 0.3668 acc_train: 0.8932 loss_val: 1.4802 acc_val: 0.7273 time: 314.7743s\n",
            "Epoch: 0579 loss_train: 0.3646 acc_train: 0.8806 loss_val: 1.4879 acc_val: 0.7491 time: 315.3076s\n",
            "Epoch: 0580 loss_train: 0.3563 acc_train: 0.8856 loss_val: 1.4889 acc_val: 0.7571 time: 315.8315s\n",
            "Epoch: 0581 loss_train: 0.3477 acc_train: 0.8958 loss_val: 1.4797 acc_val: 0.7185 time: 316.3431s\n",
            "Epoch: 0582 loss_train: 0.3618 acc_train: 0.8815 loss_val: 1.4812 acc_val: 0.7400 time: 316.8694s\n",
            "Epoch: 0583 loss_train: 0.3528 acc_train: 0.8888 loss_val: 1.4861 acc_val: 0.7560 time: 317.3934s\n",
            "Epoch: 0584 loss_train: 0.3526 acc_train: 0.8931 loss_val: 1.4866 acc_val: 0.7422 time: 317.9261s\n",
            "Epoch: 0585 loss_train: 0.3601 acc_train: 0.8872 loss_val: 1.4792 acc_val: 0.7469 time: 318.4447s\n",
            "Epoch: 0586 loss_train: 0.3514 acc_train: 0.8900 loss_val: 1.4833 acc_val: 0.7433 time: 318.9866s\n",
            "Epoch: 0587 loss_train: 0.3741 acc_train: 0.8876 loss_val: 1.4830 acc_val: 0.7025 time: 319.4994s\n",
            "Epoch: 0588 loss_train: 0.3994 acc_train: 0.8655 loss_val: 1.5069 acc_val: 0.7516 time: 320.0310s\n",
            "Epoch: 0589 loss_train: 0.3772 acc_train: 0.8888 loss_val: 1.5034 acc_val: 0.7593 time: 320.5415s\n",
            "Epoch: 0590 loss_train: 0.3823 acc_train: 0.8897 loss_val: 1.4891 acc_val: 0.7113 time: 321.0787s\n",
            "Epoch: 0591 loss_train: 0.3988 acc_train: 0.8649 loss_val: 1.4912 acc_val: 0.7385 time: 321.5997s\n",
            "Epoch: 0592 loss_train: 0.3668 acc_train: 0.8823 loss_val: 1.4964 acc_val: 0.7607 time: 322.1242s\n",
            "Epoch: 0593 loss_train: 0.3902 acc_train: 0.8813 loss_val: 1.4822 acc_val: 0.7338 time: 322.6469s\n",
            "Epoch: 0594 loss_train: 0.3841 acc_train: 0.8771 loss_val: 1.4748 acc_val: 0.7255 time: 323.1742s\n",
            "Epoch: 0595 loss_train: 0.3691 acc_train: 0.8815 loss_val: 1.4868 acc_val: 0.7422 time: 323.6957s\n",
            "Epoch: 0596 loss_train: 0.3941 acc_train: 0.8827 loss_val: 1.4806 acc_val: 0.7225 time: 324.2228s\n",
            "Epoch: 0597 loss_train: 0.3870 acc_train: 0.8748 loss_val: 1.4767 acc_val: 0.7338 time: 324.7465s\n",
            "Epoch: 0598 loss_train: 0.3877 acc_train: 0.8739 loss_val: 1.4908 acc_val: 0.7658 time: 325.2848s\n",
            "Epoch: 0599 loss_train: 0.3816 acc_train: 0.8884 loss_val: 1.4809 acc_val: 0.7531 time: 325.8017s\n",
            "Epoch: 0600 loss_train: 0.3655 acc_train: 0.8871 loss_val: 1.4754 acc_val: 0.7215 time: 326.3258s\n",
            "Epoch: 0601 loss_train: 0.3894 acc_train: 0.8715 loss_val: 1.4727 acc_val: 0.7411 time: 326.8518s\n",
            "Epoch: 0602 loss_train: 0.3512 acc_train: 0.8891 loss_val: 1.4753 acc_val: 0.7571 time: 327.3587s\n",
            "Epoch: 0603 loss_train: 0.3639 acc_train: 0.8901 loss_val: 1.4656 acc_val: 0.7476 time: 327.8884s\n",
            "Epoch: 0604 loss_train: 0.3626 acc_train: 0.8857 loss_val: 1.4612 acc_val: 0.7433 time: 328.4286s\n",
            "Epoch: 0605 loss_train: 0.3752 acc_train: 0.8805 loss_val: 1.4731 acc_val: 0.7571 time: 328.9716s\n",
            "Epoch: 0606 loss_train: 0.3685 acc_train: 0.8906 loss_val: 1.4787 acc_val: 0.7367 time: 329.4957s\n",
            "Epoch: 0607 loss_train: 0.3666 acc_train: 0.8823 loss_val: 1.4782 acc_val: 0.7324 time: 330.0208s\n",
            "Epoch: 0608 loss_train: 0.3624 acc_train: 0.8851 loss_val: 1.4827 acc_val: 0.7455 time: 330.5361s\n",
            "Epoch: 0609 loss_train: 0.3595 acc_train: 0.8888 loss_val: 1.4786 acc_val: 0.7596 time: 331.0613s\n",
            "Epoch: 0610 loss_train: 0.3583 acc_train: 0.8932 loss_val: 1.4644 acc_val: 0.7418 time: 331.5900s\n",
            "Epoch: 0611 loss_train: 0.3635 acc_train: 0.8819 loss_val: 1.4499 acc_val: 0.7458 time: 332.1175s\n",
            "Epoch: 0612 loss_train: 0.3434 acc_train: 0.8901 loss_val: 1.4502 acc_val: 0.7509 time: 332.6296s\n",
            "Epoch: 0613 loss_train: 0.3578 acc_train: 0.8872 loss_val: 1.4578 acc_val: 0.7425 time: 333.1556s\n",
            "Epoch: 0614 loss_train: 0.3472 acc_train: 0.8900 loss_val: 1.4699 acc_val: 0.7378 time: 333.6798s\n",
            "Epoch: 0615 loss_train: 0.3647 acc_train: 0.8836 loss_val: 1.4857 acc_val: 0.7585 time: 334.2013s\n",
            "Epoch: 0616 loss_train: 0.3544 acc_train: 0.8959 loss_val: 1.4847 acc_val: 0.7498 time: 334.7245s\n",
            "Epoch: 0617 loss_train: 0.3573 acc_train: 0.8923 loss_val: 1.4749 acc_val: 0.7429 time: 335.2703s\n",
            "Epoch: 0618 loss_train: 0.3477 acc_train: 0.8925 loss_val: 1.4634 acc_val: 0.7407 time: 335.7978s\n",
            "Epoch: 0619 loss_train: 0.3528 acc_train: 0.8861 loss_val: 1.4622 acc_val: 0.7505 time: 336.3347s\n",
            "Epoch: 0620 loss_train: 0.3552 acc_train: 0.8918 loss_val: 1.4637 acc_val: 0.7433 time: 336.8614s\n",
            "Epoch: 0621 loss_train: 0.3516 acc_train: 0.8869 loss_val: 1.4590 acc_val: 0.7375 time: 337.3786s\n",
            "Epoch: 0622 loss_train: 0.3527 acc_train: 0.8880 loss_val: 1.4654 acc_val: 0.7451 time: 337.9121s\n",
            "Epoch: 0623 loss_train: 0.3517 acc_train: 0.8938 loss_val: 1.4703 acc_val: 0.7436 time: 338.4515s\n",
            "Epoch: 0624 loss_train: 0.3381 acc_train: 0.8957 loss_val: 1.4724 acc_val: 0.7309 time: 339.0105s\n",
            "Epoch: 0625 loss_train: 0.3667 acc_train: 0.8829 loss_val: 1.4818 acc_val: 0.7556 time: 340.0833s\n",
            "Epoch: 0626 loss_train: 0.3435 acc_train: 0.8954 loss_val: 1.4773 acc_val: 0.7476 time: 340.8393s\n",
            "Epoch: 0627 loss_train: 0.3528 acc_train: 0.8953 loss_val: 1.4643 acc_val: 0.7215 time: 341.3954s\n",
            "Epoch: 0628 loss_train: 0.3601 acc_train: 0.8813 loss_val: 1.4617 acc_val: 0.7451 time: 341.9520s\n",
            "Epoch: 0629 loss_train: 0.3357 acc_train: 0.8923 loss_val: 1.4699 acc_val: 0.7625 time: 342.5054s\n",
            "Epoch: 0630 loss_train: 0.3580 acc_train: 0.8947 loss_val: 1.4652 acc_val: 0.7393 time: 343.0761s\n",
            "Epoch: 0631 loss_train: 0.3646 acc_train: 0.8803 loss_val: 1.4675 acc_val: 0.7385 time: 343.6675s\n",
            "Epoch: 0632 loss_train: 0.3503 acc_train: 0.8863 loss_val: 1.4762 acc_val: 0.7455 time: 344.3527s\n",
            "Epoch: 0633 loss_train: 0.3686 acc_train: 0.8892 loss_val: 1.4753 acc_val: 0.7389 time: 345.0681s\n",
            "Epoch: 0634 loss_train: 0.3468 acc_train: 0.8921 loss_val: 1.4735 acc_val: 0.7327 time: 345.9217s\n",
            "Epoch: 0635 loss_train: 0.3496 acc_train: 0.8889 loss_val: 1.4712 acc_val: 0.7455 time: 346.5664s\n",
            "Epoch: 0636 loss_train: 0.3444 acc_train: 0.8933 loss_val: 1.4657 acc_val: 0.7440 time: 347.2395s\n",
            "Epoch: 0637 loss_train: 0.3474 acc_train: 0.8928 loss_val: 1.4594 acc_val: 0.7353 time: 348.1301s\n",
            "Epoch: 0638 loss_train: 0.3587 acc_train: 0.8854 loss_val: 1.4669 acc_val: 0.7556 time: 348.9435s\n",
            "Epoch: 0639 loss_train: 0.3430 acc_train: 0.8945 loss_val: 1.4738 acc_val: 0.7556 time: 349.6562s\n",
            "Epoch: 0640 loss_train: 0.3460 acc_train: 0.8947 loss_val: 1.4679 acc_val: 0.7382 time: 350.2947s\n",
            "Epoch: 0641 loss_train: 0.3476 acc_train: 0.8900 loss_val: 1.4618 acc_val: 0.7364 time: 350.9645s\n",
            "Epoch: 0642 loss_train: 0.3507 acc_train: 0.8896 loss_val: 1.4683 acc_val: 0.7556 time: 351.6457s\n",
            "Epoch: 0643 loss_train: 0.3514 acc_train: 0.8938 loss_val: 1.4670 acc_val: 0.7476 time: 352.3559s\n",
            "Epoch: 0644 loss_train: 0.3505 acc_train: 0.8877 loss_val: 1.4673 acc_val: 0.7349 time: 353.0653s\n",
            "Epoch: 0645 loss_train: 0.3454 acc_train: 0.8889 loss_val: 1.4719 acc_val: 0.7418 time: 353.8501s\n",
            "Epoch: 0646 loss_train: 0.3427 acc_train: 0.8980 loss_val: 1.4688 acc_val: 0.7480 time: 354.5272s\n",
            "Epoch: 0647 loss_train: 0.3388 acc_train: 0.8958 loss_val: 1.4638 acc_val: 0.7313 time: 355.1600s\n",
            "Epoch: 0648 loss_train: 0.3450 acc_train: 0.8900 loss_val: 1.4677 acc_val: 0.7535 time: 355.7081s\n",
            "Epoch: 0649 loss_train: 0.3488 acc_train: 0.8924 loss_val: 1.4685 acc_val: 0.7451 time: 356.2385s\n",
            "Epoch: 0650 loss_train: 0.3497 acc_train: 0.8926 loss_val: 1.4659 acc_val: 0.7316 time: 356.7749s\n",
            "Epoch: 0651 loss_train: 0.3387 acc_train: 0.8882 loss_val: 1.4709 acc_val: 0.7407 time: 357.3037s\n",
            "Epoch: 0652 loss_train: 0.3427 acc_train: 0.8940 loss_val: 1.4771 acc_val: 0.7578 time: 357.8314s\n",
            "Epoch: 0653 loss_train: 0.3437 acc_train: 0.8990 loss_val: 1.4646 acc_val: 0.7295 time: 358.3539s\n",
            "Epoch: 0654 loss_train: 0.3469 acc_train: 0.8884 loss_val: 1.4619 acc_val: 0.7342 time: 358.9021s\n",
            "Epoch: 0655 loss_train: 0.3513 acc_train: 0.8838 loss_val: 1.4707 acc_val: 0.7520 time: 359.4339s\n",
            "Epoch: 0656 loss_train: 0.3597 acc_train: 0.8911 loss_val: 1.4721 acc_val: 0.7320 time: 359.9695s\n",
            "Epoch: 0657 loss_train: 0.3550 acc_train: 0.8833 loss_val: 1.4823 acc_val: 0.7473 time: 360.4933s\n",
            "Epoch: 0658 loss_train: 0.3540 acc_train: 0.8913 loss_val: 1.4778 acc_val: 0.7465 time: 361.0335s\n",
            "Epoch: 0659 loss_train: 0.3532 acc_train: 0.8916 loss_val: 1.4668 acc_val: 0.7331 time: 361.5549s\n",
            "Epoch: 0660 loss_train: 0.3461 acc_train: 0.8936 loss_val: 1.4604 acc_val: 0.7138 time: 362.1072s\n",
            "Epoch: 0661 loss_train: 0.3621 acc_train: 0.8789 loss_val: 1.4716 acc_val: 0.7549 time: 362.6583s\n",
            "Epoch: 0662 loss_train: 0.3527 acc_train: 0.8961 loss_val: 1.4730 acc_val: 0.7607 time: 363.1977s\n",
            "Epoch: 0663 loss_train: 0.3491 acc_train: 0.8948 loss_val: 1.4651 acc_val: 0.7276 time: 363.7251s\n",
            "Epoch: 0664 loss_train: 0.3512 acc_train: 0.8891 loss_val: 1.4709 acc_val: 0.7378 time: 364.2594s\n",
            "Epoch: 0665 loss_train: 0.3413 acc_train: 0.8913 loss_val: 1.4754 acc_val: 0.7458 time: 364.7892s\n",
            "Epoch: 0666 loss_train: 0.3449 acc_train: 0.8968 loss_val: 1.4709 acc_val: 0.7509 time: 365.3265s\n",
            "Epoch: 0667 loss_train: 0.3407 acc_train: 0.8968 loss_val: 1.4653 acc_val: 0.7349 time: 365.8785s\n",
            "Epoch: 0668 loss_train: 0.3526 acc_train: 0.8860 loss_val: 1.4626 acc_val: 0.7502 time: 366.4127s\n",
            "Epoch: 0669 loss_train: 0.3456 acc_train: 0.8923 loss_val: 1.4604 acc_val: 0.7469 time: 366.9494s\n",
            "Epoch: 0670 loss_train: 0.3552 acc_train: 0.8907 loss_val: 1.4581 acc_val: 0.7367 time: 367.4798s\n",
            "Epoch: 0671 loss_train: 0.3372 acc_train: 0.8903 loss_val: 1.4616 acc_val: 0.7356 time: 368.0194s\n",
            "Epoch: 0672 loss_train: 0.3491 acc_train: 0.8868 loss_val: 1.4785 acc_val: 0.7633 time: 368.5474s\n",
            "Epoch: 0673 loss_train: 0.3684 acc_train: 0.8920 loss_val: 1.4703 acc_val: 0.7429 time: 369.0729s\n",
            "Epoch: 0674 loss_train: 0.3522 acc_train: 0.8900 loss_val: 1.4661 acc_val: 0.7284 time: 369.6112s\n",
            "Epoch: 0675 loss_train: 0.3490 acc_train: 0.8877 loss_val: 1.4693 acc_val: 0.7447 time: 370.1503s\n",
            "Epoch: 0676 loss_train: 0.3424 acc_train: 0.8965 loss_val: 1.4625 acc_val: 0.7393 time: 370.6805s\n",
            "Epoch: 0677 loss_train: 0.3433 acc_train: 0.8879 loss_val: 1.4581 acc_val: 0.7364 time: 371.2123s\n",
            "Epoch: 0678 loss_train: 0.3445 acc_train: 0.8891 loss_val: 1.4655 acc_val: 0.7629 time: 371.7407s\n",
            "Epoch: 0679 loss_train: 0.3456 acc_train: 0.8968 loss_val: 1.4620 acc_val: 0.7425 time: 372.2684s\n",
            "Epoch: 0680 loss_train: 0.3421 acc_train: 0.8897 loss_val: 1.4628 acc_val: 0.7262 time: 372.7867s\n",
            "Epoch: 0681 loss_train: 0.3469 acc_train: 0.8863 loss_val: 1.4687 acc_val: 0.7371 time: 373.3058s\n",
            "Epoch: 0682 loss_train: 0.3540 acc_train: 0.8895 loss_val: 1.4730 acc_val: 0.7542 time: 373.8238s\n",
            "Epoch: 0683 loss_train: 0.3423 acc_train: 0.8972 loss_val: 1.4676 acc_val: 0.7385 time: 374.3361s\n",
            "Epoch: 0684 loss_train: 0.3351 acc_train: 0.8939 loss_val: 1.4631 acc_val: 0.7385 time: 374.8664s\n",
            "Epoch: 0685 loss_train: 0.3396 acc_train: 0.8943 loss_val: 1.4615 acc_val: 0.7451 time: 375.3966s\n",
            "Epoch: 0686 loss_train: 0.3470 acc_train: 0.8916 loss_val: 1.4618 acc_val: 0.7433 time: 375.9145s\n",
            "Epoch: 0687 loss_train: 0.3403 acc_train: 0.8927 loss_val: 1.4698 acc_val: 0.7436 time: 376.4551s\n",
            "Epoch: 0688 loss_train: 0.3533 acc_train: 0.8902 loss_val: 1.4679 acc_val: 0.7400 time: 377.0030s\n",
            "Epoch: 0689 loss_train: 0.3416 acc_train: 0.8928 loss_val: 1.4623 acc_val: 0.7353 time: 377.5294s\n",
            "Epoch: 0690 loss_train: 0.3472 acc_train: 0.8898 loss_val: 1.4625 acc_val: 0.7389 time: 378.0633s\n",
            "Epoch: 0691 loss_train: 0.3426 acc_train: 0.8940 loss_val: 1.4711 acc_val: 0.7462 time: 378.5838s\n",
            "Epoch: 0692 loss_train: 0.3423 acc_train: 0.8957 loss_val: 1.4678 acc_val: 0.7342 time: 379.1085s\n",
            "Epoch: 0693 loss_train: 0.3414 acc_train: 0.8904 loss_val: 1.4693 acc_val: 0.7331 time: 379.6333s\n",
            "Epoch: 0694 loss_train: 0.3464 acc_train: 0.8881 loss_val: 1.4729 acc_val: 0.7520 time: 380.1623s\n",
            "Epoch: 0695 loss_train: 0.3466 acc_train: 0.8933 loss_val: 1.4657 acc_val: 0.7393 time: 380.6923s\n",
            "Epoch: 0696 loss_train: 0.3484 acc_train: 0.8904 loss_val: 1.4573 acc_val: 0.7302 time: 381.2022s\n",
            "Epoch: 0697 loss_train: 0.3288 acc_train: 0.8935 loss_val: 1.4582 acc_val: 0.7382 time: 381.7220s\n",
            "Epoch: 0698 loss_train: 0.3415 acc_train: 0.8947 loss_val: 1.4649 acc_val: 0.7502 time: 382.2800s\n",
            "Epoch: 0699 loss_train: 0.3546 acc_train: 0.8901 loss_val: 1.4653 acc_val: 0.7189 time: 382.8257s\n",
            "Epoch: 0700 loss_train: 0.3503 acc_train: 0.8858 loss_val: 1.4688 acc_val: 0.7411 time: 383.3607s\n",
            "Epoch: 0701 loss_train: 0.3439 acc_train: 0.8927 loss_val: 1.4736 acc_val: 0.7531 time: 383.9014s\n",
            "Epoch: 0702 loss_train: 0.3596 acc_train: 0.8941 loss_val: 1.4603 acc_val: 0.7196 time: 384.4284s\n",
            "Epoch: 0703 loss_train: 0.3417 acc_train: 0.8902 loss_val: 1.4585 acc_val: 0.7262 time: 384.9628s\n",
            "Epoch: 0704 loss_train: 0.3493 acc_train: 0.8893 loss_val: 1.4678 acc_val: 0.7593 time: 385.4957s\n",
            "Epoch: 0705 loss_train: 0.3660 acc_train: 0.8937 loss_val: 1.4595 acc_val: 0.7255 time: 386.0216s\n",
            "Epoch: 0706 loss_train: 0.3524 acc_train: 0.8853 loss_val: 1.4644 acc_val: 0.7385 time: 386.5630s\n",
            "Epoch: 0707 loss_train: 0.3424 acc_train: 0.8922 loss_val: 1.4690 acc_val: 0.7542 time: 387.1130s\n",
            "Epoch: 0708 loss_train: 0.3489 acc_train: 0.8949 loss_val: 1.4580 acc_val: 0.7411 time: 387.6571s\n",
            "Epoch: 0709 loss_train: 0.3453 acc_train: 0.8935 loss_val: 1.4563 acc_val: 0.7433 time: 388.1887s\n",
            "Epoch: 0710 loss_train: 0.3399 acc_train: 0.8958 loss_val: 1.4575 acc_val: 0.7400 time: 388.7225s\n",
            "Epoch: 0711 loss_train: 0.3431 acc_train: 0.8923 loss_val: 1.4558 acc_val: 0.7385 time: 389.2523s\n",
            "Epoch: 0712 loss_train: 0.3478 acc_train: 0.8889 loss_val: 1.4560 acc_val: 0.7313 time: 389.7891s\n",
            "Epoch: 0713 loss_train: 0.3344 acc_train: 0.8918 loss_val: 1.4597 acc_val: 0.7415 time: 390.3106s\n",
            "Epoch: 0714 loss_train: 0.3424 acc_train: 0.8943 loss_val: 1.4616 acc_val: 0.7509 time: 390.8459s\n",
            "Epoch: 0715 loss_train: 0.3496 acc_train: 0.8938 loss_val: 1.4544 acc_val: 0.7313 time: 391.3765s\n",
            "Epoch: 0716 loss_train: 0.3481 acc_train: 0.8867 loss_val: 1.4565 acc_val: 0.7345 time: 391.8997s\n",
            "Epoch: 0717 loss_train: 0.3413 acc_train: 0.8912 loss_val: 1.4691 acc_val: 0.7567 time: 392.4315s\n",
            "Epoch: 0718 loss_train: 0.3515 acc_train: 0.8966 loss_val: 1.4543 acc_val: 0.7287 time: 392.9596s\n",
            "Epoch: 0719 loss_train: 0.3367 acc_train: 0.8908 loss_val: 1.4520 acc_val: 0.7087 time: 393.4772s\n",
            "Epoch: 0720 loss_train: 0.3504 acc_train: 0.8860 loss_val: 1.4587 acc_val: 0.7425 time: 394.0089s\n",
            "Epoch: 0721 loss_train: 0.3370 acc_train: 0.8943 loss_val: 1.4639 acc_val: 0.7531 time: 394.5275s\n",
            "Epoch: 0722 loss_train: 0.3440 acc_train: 0.8935 loss_val: 1.4573 acc_val: 0.7349 time: 395.0642s\n",
            "Epoch: 0723 loss_train: 0.3479 acc_train: 0.8922 loss_val: 1.4565 acc_val: 0.7429 time: 395.5858s\n",
            "Epoch: 0724 loss_train: 0.3364 acc_train: 0.8969 loss_val: 1.4541 acc_val: 0.7389 time: 396.1307s\n",
            "Epoch: 0725 loss_train: 0.3481 acc_train: 0.8900 loss_val: 1.4536 acc_val: 0.7247 time: 396.6742s\n",
            "Epoch: 0726 loss_train: 0.3414 acc_train: 0.8892 loss_val: 1.4637 acc_val: 0.7491 time: 397.1990s\n",
            "Epoch: 0727 loss_train: 0.3391 acc_train: 0.8975 loss_val: 1.4589 acc_val: 0.7436 time: 397.7281s\n",
            "Epoch: 0728 loss_train: 0.3457 acc_train: 0.8950 loss_val: 1.4518 acc_val: 0.7098 time: 398.2494s\n",
            "Epoch: 0729 loss_train: 0.3456 acc_train: 0.8853 loss_val: 1.4572 acc_val: 0.7393 time: 398.7806s\n",
            "Epoch: 0730 loss_train: 0.3290 acc_train: 0.8969 loss_val: 1.4671 acc_val: 0.7556 time: 399.3087s\n",
            "Epoch: 0731 loss_train: 0.3535 acc_train: 0.8930 loss_val: 1.4565 acc_val: 0.7291 time: 399.8347s\n",
            "Epoch: 0732 loss_train: 0.3462 acc_train: 0.8896 loss_val: 1.4547 acc_val: 0.7269 time: 400.3628s\n",
            "Epoch: 0733 loss_train: 0.3443 acc_train: 0.8908 loss_val: 1.4594 acc_val: 0.7302 time: 400.8980s\n",
            "Epoch: 0734 loss_train: 0.3434 acc_train: 0.8924 loss_val: 1.4674 acc_val: 0.7382 time: 401.4219s\n",
            "Epoch: 0735 loss_train: 0.3449 acc_train: 0.8936 loss_val: 1.4638 acc_val: 0.7385 time: 401.9473s\n",
            "Epoch: 0736 loss_train: 0.3347 acc_train: 0.8943 loss_val: 1.4579 acc_val: 0.7367 time: 402.4728s\n",
            "Epoch: 0737 loss_train: 0.3428 acc_train: 0.8950 loss_val: 1.4543 acc_val: 0.7313 time: 403.0110s\n",
            "Epoch: 0738 loss_train: 0.3392 acc_train: 0.8935 loss_val: 1.4533 acc_val: 0.7342 time: 403.5406s\n",
            "Epoch: 0739 loss_train: 0.3402 acc_train: 0.8931 loss_val: 1.4524 acc_val: 0.7378 time: 404.0643s\n",
            "Epoch: 0740 loss_train: 0.3413 acc_train: 0.8918 loss_val: 1.4550 acc_val: 0.7396 time: 404.5781s\n",
            "Epoch: 0741 loss_train: 0.3394 acc_train: 0.8970 loss_val: 1.4562 acc_val: 0.7324 time: 405.1079s\n",
            "Epoch: 0742 loss_train: 0.3356 acc_train: 0.8936 loss_val: 1.4594 acc_val: 0.7345 time: 405.6374s\n",
            "Epoch: 0743 loss_train: 0.3352 acc_train: 0.8911 loss_val: 1.4612 acc_val: 0.7436 time: 406.1660s\n",
            "Epoch: 0744 loss_train: 0.3358 acc_train: 0.8964 loss_val: 1.4559 acc_val: 0.7305 time: 406.7008s\n",
            "Epoch: 0745 loss_train: 0.3378 acc_train: 0.8932 loss_val: 1.4598 acc_val: 0.7313 time: 407.2354s\n",
            "Epoch: 0746 loss_train: 0.3404 acc_train: 0.8920 loss_val: 1.4593 acc_val: 0.7378 time: 407.7649s\n",
            "Epoch: 0747 loss_train: 0.3386 acc_train: 0.9001 loss_val: 1.4547 acc_val: 0.7338 time: 408.3050s\n",
            "Epoch: 0748 loss_train: 0.3362 acc_train: 0.8946 loss_val: 1.4528 acc_val: 0.7378 time: 408.8317s\n",
            "Epoch: 0749 loss_train: 0.3338 acc_train: 0.8941 loss_val: 1.4550 acc_val: 0.7389 time: 409.3513s\n",
            "Epoch: 0750 loss_train: 0.3400 acc_train: 0.8981 loss_val: 1.4535 acc_val: 0.7120 time: 409.8747s\n",
            "Epoch: 0751 loss_train: 0.3533 acc_train: 0.8830 loss_val: 1.4673 acc_val: 0.7556 time: 410.4068s\n",
            "Epoch: 0752 loss_train: 0.3361 acc_train: 0.9005 loss_val: 1.4645 acc_val: 0.7455 time: 410.9302s\n",
            "Epoch: 0753 loss_train: 0.3320 acc_train: 0.8967 loss_val: 1.4534 acc_val: 0.7262 time: 411.4616s\n",
            "Epoch: 0754 loss_train: 0.3439 acc_train: 0.8897 loss_val: 1.4550 acc_val: 0.7411 time: 411.9886s\n",
            "Epoch: 0755 loss_train: 0.3439 acc_train: 0.8957 loss_val: 1.4541 acc_val: 0.7164 time: 412.5055s\n",
            "Epoch: 0756 loss_train: 0.3474 acc_train: 0.8863 loss_val: 1.4629 acc_val: 0.7505 time: 413.0340s\n",
            "Epoch: 0757 loss_train: 0.3364 acc_train: 0.8960 loss_val: 1.4587 acc_val: 0.7407 time: 413.5496s\n",
            "Epoch: 0758 loss_train: 0.3456 acc_train: 0.8919 loss_val: 1.4540 acc_val: 0.7084 time: 414.0840s\n",
            "Epoch: 0759 loss_train: 0.3663 acc_train: 0.8774 loss_val: 1.4667 acc_val: 0.7495 time: 414.6083s\n",
            "Epoch: 0760 loss_train: 0.3452 acc_train: 0.8986 loss_val: 1.4668 acc_val: 0.7433 time: 415.1382s\n",
            "Epoch: 0761 loss_train: 0.3395 acc_train: 0.8932 loss_val: 1.4533 acc_val: 0.7251 time: 415.6537s\n",
            "Epoch: 0762 loss_train: 0.3379 acc_train: 0.8925 loss_val: 1.4527 acc_val: 0.7382 time: 416.1779s\n",
            "Epoch: 0763 loss_train: 0.3459 acc_train: 0.8929 loss_val: 1.4517 acc_val: 0.7236 time: 416.7147s\n",
            "Epoch: 0764 loss_train: 0.3482 acc_train: 0.8877 loss_val: 1.4582 acc_val: 0.7433 time: 417.2324s\n",
            "Epoch: 0765 loss_train: 0.3297 acc_train: 0.8995 loss_val: 1.4618 acc_val: 0.7473 time: 417.7432s\n",
            "Epoch: 0766 loss_train: 0.3418 acc_train: 0.8935 loss_val: 1.4535 acc_val: 0.7302 time: 418.2738s\n",
            "Epoch: 0767 loss_train: 0.3377 acc_train: 0.8909 loss_val: 1.4512 acc_val: 0.7262 time: 418.8013s\n",
            "Epoch: 0768 loss_train: 0.3385 acc_train: 0.8920 loss_val: 1.4519 acc_val: 0.7498 time: 419.3312s\n",
            "Epoch: 0769 loss_train: 0.3342 acc_train: 0.8947 loss_val: 1.4541 acc_val: 0.7502 time: 419.8621s\n",
            "Epoch: 0770 loss_train: 0.3348 acc_train: 0.9011 loss_val: 1.4521 acc_val: 0.7305 time: 420.3841s\n",
            "Epoch: 0771 loss_train: 0.3442 acc_train: 0.8889 loss_val: 1.4581 acc_val: 0.7415 time: 420.9176s\n",
            "Epoch: 0772 loss_train: 0.3400 acc_train: 0.8967 loss_val: 1.4619 acc_val: 0.7451 time: 421.4447s\n",
            "Epoch: 0773 loss_train: 0.3467 acc_train: 0.8925 loss_val: 1.4551 acc_val: 0.7116 time: 421.9862s\n",
            "Epoch: 0774 loss_train: 0.3544 acc_train: 0.8854 loss_val: 1.4577 acc_val: 0.7404 time: 422.5214s\n",
            "Epoch: 0775 loss_train: 0.3309 acc_train: 0.8987 loss_val: 1.4639 acc_val: 0.7575 time: 423.0603s\n",
            "Epoch: 0776 loss_train: 0.3621 acc_train: 0.8894 loss_val: 1.4482 acc_val: 0.7145 time: 423.5815s\n",
            "Epoch: 0777 loss_train: 0.3519 acc_train: 0.8855 loss_val: 1.4501 acc_val: 0.7367 time: 424.0970s\n",
            "Epoch: 0778 loss_train: 0.3453 acc_train: 0.8913 loss_val: 1.4607 acc_val: 0.7480 time: 424.6205s\n",
            "Epoch: 0779 loss_train: 0.3552 acc_train: 0.8938 loss_val: 1.4615 acc_val: 0.7062 time: 425.1537s\n",
            "Epoch: 0780 loss_train: 0.3674 acc_train: 0.8800 loss_val: 1.4633 acc_val: 0.7389 time: 425.6742s\n",
            "Epoch: 0781 loss_train: 0.3387 acc_train: 0.8928 loss_val: 1.4711 acc_val: 0.7615 time: 426.1991s\n",
            "Epoch: 0782 loss_train: 0.3645 acc_train: 0.8980 loss_val: 1.4493 acc_val: 0.7167 time: 426.7229s\n",
            "Epoch: 0783 loss_train: 0.3527 acc_train: 0.8859 loss_val: 1.4480 acc_val: 0.7189 time: 427.2552s\n",
            "Epoch: 0784 loss_train: 0.3339 acc_train: 0.8921 loss_val: 1.4553 acc_val: 0.7513 time: 427.7818s\n",
            "Epoch: 0785 loss_train: 0.3503 acc_train: 0.8928 loss_val: 1.4519 acc_val: 0.7295 time: 428.3096s\n",
            "Epoch: 0786 loss_train: 0.3453 acc_train: 0.8894 loss_val: 1.4504 acc_val: 0.7298 time: 428.8381s\n",
            "Epoch: 0787 loss_train: 0.3411 acc_train: 0.8922 loss_val: 1.4562 acc_val: 0.7509 time: 429.3751s\n",
            "Epoch: 0788 loss_train: 0.3437 acc_train: 0.8937 loss_val: 1.4537 acc_val: 0.7458 time: 429.9069s\n",
            "Epoch: 0789 loss_train: 0.3399 acc_train: 0.8935 loss_val: 1.4530 acc_val: 0.7276 time: 430.4245s\n",
            "Epoch: 0790 loss_train: 0.3425 acc_train: 0.8903 loss_val: 1.4562 acc_val: 0.7400 time: 430.9548s\n",
            "Epoch: 0791 loss_train: 0.3485 acc_train: 0.8907 loss_val: 1.4458 acc_val: 0.7425 time: 431.4722s\n",
            "Epoch: 0792 loss_train: 0.3340 acc_train: 0.8939 loss_val: 1.4382 acc_val: 0.7353 time: 432.0044s\n",
            "Epoch: 0793 loss_train: 0.3392 acc_train: 0.8923 loss_val: 1.4368 acc_val: 0.7265 time: 432.5282s\n",
            "Epoch: 0794 loss_train: 0.3390 acc_train: 0.8912 loss_val: 1.4471 acc_val: 0.7371 time: 433.0579s\n",
            "Epoch: 0795 loss_train: 0.3433 acc_train: 0.8946 loss_val: 1.4558 acc_val: 0.7324 time: 433.5878s\n",
            "Epoch: 0796 loss_train: 0.3387 acc_train: 0.8892 loss_val: 1.4639 acc_val: 0.7378 time: 434.1157s\n",
            "Epoch: 0797 loss_train: 0.3418 acc_train: 0.8952 loss_val: 1.4606 acc_val: 0.7418 time: 434.6396s\n",
            "Epoch: 0798 loss_train: 0.3410 acc_train: 0.8961 loss_val: 1.4492 acc_val: 0.7156 time: 435.1599s\n",
            "Epoch: 0799 loss_train: 0.3570 acc_train: 0.8826 loss_val: 1.4442 acc_val: 0.7324 time: 435.6793s\n",
            "Epoch: 0800 loss_train: 0.3379 acc_train: 0.8937 loss_val: 1.4586 acc_val: 0.7684 time: 436.2065s\n",
            "Epoch: 0801 loss_train: 0.3452 acc_train: 0.9023 loss_val: 1.4453 acc_val: 0.7247 time: 436.7323s\n",
            "Epoch: 0802 loss_train: 0.3497 acc_train: 0.8849 loss_val: 1.4415 acc_val: 0.7207 time: 437.2639s\n",
            "Epoch: 0803 loss_train: 0.3377 acc_train: 0.8906 loss_val: 1.4496 acc_val: 0.7404 time: 437.7813s\n",
            "Epoch: 0804 loss_train: 0.3582 acc_train: 0.8889 loss_val: 1.4481 acc_val: 0.7153 time: 438.3019s\n",
            "Epoch: 0805 loss_train: 0.3555 acc_train: 0.8824 loss_val: 1.4574 acc_val: 0.7473 time: 438.8348s\n",
            "Epoch: 0806 loss_train: 0.3365 acc_train: 0.8951 loss_val: 1.4560 acc_val: 0.7531 time: 439.3570s\n",
            "Epoch: 0807 loss_train: 0.3391 acc_train: 0.8996 loss_val: 1.4418 acc_val: 0.7222 time: 439.8787s\n",
            "Epoch: 0808 loss_train: 0.3530 acc_train: 0.8857 loss_val: 1.4416 acc_val: 0.7335 time: 440.3928s\n",
            "Epoch: 0809 loss_train: 0.3371 acc_train: 0.8948 loss_val: 1.4493 acc_val: 0.7465 time: 440.9143s\n",
            "Epoch: 0810 loss_train: 0.3451 acc_train: 0.8949 loss_val: 1.4463 acc_val: 0.7225 time: 441.4222s\n",
            "Epoch: 0811 loss_train: 0.3469 acc_train: 0.8899 loss_val: 1.4441 acc_val: 0.7324 time: 441.9619s\n",
            "Epoch: 0812 loss_train: 0.3249 acc_train: 0.8960 loss_val: 1.4522 acc_val: 0.7542 time: 442.4708s\n",
            "Epoch: 0813 loss_train: 0.3551 acc_train: 0.8920 loss_val: 1.4479 acc_val: 0.7240 time: 443.0103s\n",
            "Epoch: 0814 loss_train: 0.3521 acc_train: 0.8877 loss_val: 1.4502 acc_val: 0.7342 time: 443.5307s\n",
            "Epoch: 0815 loss_train: 0.3320 acc_train: 0.8999 loss_val: 1.4511 acc_val: 0.7411 time: 444.0597s\n",
            "Epoch: 0816 loss_train: 0.3566 acc_train: 0.8908 loss_val: 1.4423 acc_val: 0.7313 time: 444.5849s\n",
            "Epoch: 0817 loss_train: 0.3449 acc_train: 0.8909 loss_val: 1.4398 acc_val: 0.7280 time: 445.1153s\n",
            "Epoch: 0818 loss_train: 0.3298 acc_train: 0.8961 loss_val: 1.4424 acc_val: 0.7345 time: 445.6379s\n",
            "Epoch: 0819 loss_train: 0.3397 acc_train: 0.8891 loss_val: 1.4484 acc_val: 0.7487 time: 446.1563s\n",
            "Epoch: 0820 loss_train: 0.3275 acc_train: 0.9003 loss_val: 1.4454 acc_val: 0.7385 time: 446.6791s\n",
            "Epoch: 0821 loss_train: 0.3317 acc_train: 0.8963 loss_val: 1.4424 acc_val: 0.7295 time: 447.1872s\n",
            "Epoch: 0822 loss_train: 0.3279 acc_train: 0.8945 loss_val: 1.4404 acc_val: 0.7236 time: 447.7220s\n",
            "Epoch: 0823 loss_train: 0.3332 acc_train: 0.8942 loss_val: 1.4416 acc_val: 0.7342 time: 448.2414s\n",
            "Epoch: 0824 loss_train: 0.3399 acc_train: 0.8945 loss_val: 1.4472 acc_val: 0.7375 time: 448.7629s\n",
            "Epoch: 0825 loss_train: 0.3383 acc_train: 0.8929 loss_val: 1.4503 acc_val: 0.7411 time: 449.2794s\n",
            "Epoch: 0826 loss_train: 0.3391 acc_train: 0.8943 loss_val: 1.4416 acc_val: 0.7287 time: 449.8037s\n",
            "Epoch: 0827 loss_train: 0.3350 acc_train: 0.8946 loss_val: 1.4425 acc_val: 0.7276 time: 450.3175s\n",
            "Epoch: 0828 loss_train: 0.3351 acc_train: 0.8930 loss_val: 1.4440 acc_val: 0.7269 time: 450.8382s\n",
            "Epoch: 0829 loss_train: 0.3328 acc_train: 0.8950 loss_val: 1.4476 acc_val: 0.7378 time: 451.3662s\n",
            "Epoch: 0830 loss_train: 0.3393 acc_train: 0.8930 loss_val: 1.4440 acc_val: 0.7407 time: 451.8896s\n",
            "Epoch: 0831 loss_train: 0.3315 acc_train: 0.8960 loss_val: 1.4401 acc_val: 0.7200 time: 452.4132s\n",
            "Epoch: 0832 loss_train: 0.3376 acc_train: 0.8916 loss_val: 1.4442 acc_val: 0.7313 time: 452.9435s\n",
            "Epoch: 0833 loss_train: 0.3309 acc_train: 0.8948 loss_val: 1.4465 acc_val: 0.7400 time: 453.4583s\n",
            "Epoch: 0834 loss_train: 0.3349 acc_train: 0.8965 loss_val: 1.4455 acc_val: 0.7211 time: 453.9839s\n",
            "Epoch: 0835 loss_train: 0.3456 acc_train: 0.8858 loss_val: 1.4439 acc_val: 0.7338 time: 454.4884s\n",
            "Epoch: 0836 loss_train: 0.3324 acc_train: 0.8923 loss_val: 1.4486 acc_val: 0.7473 time: 455.0290s\n",
            "Epoch: 0837 loss_train: 0.3480 acc_train: 0.8972 loss_val: 1.4432 acc_val: 0.7000 time: 455.5517s\n",
            "Epoch: 0838 loss_train: 0.3563 acc_train: 0.8826 loss_val: 1.4473 acc_val: 0.7265 time: 456.0668s\n",
            "Epoch: 0839 loss_train: 0.3283 acc_train: 0.8978 loss_val: 1.4577 acc_val: 0.7491 time: 456.5861s\n",
            "Epoch: 0840 loss_train: 0.3502 acc_train: 0.8942 loss_val: 1.4462 acc_val: 0.7116 time: 457.1180s\n",
            "Epoch: 0841 loss_train: 0.3341 acc_train: 0.8886 loss_val: 1.4448 acc_val: 0.7225 time: 457.6483s\n",
            "Epoch: 0842 loss_train: 0.3338 acc_train: 0.8948 loss_val: 1.4504 acc_val: 0.7560 time: 458.1694s\n",
            "Epoch: 0843 loss_train: 0.3457 acc_train: 0.8958 loss_val: 1.4412 acc_val: 0.7280 time: 458.6909s\n",
            "Epoch: 0844 loss_train: 0.3443 acc_train: 0.8902 loss_val: 1.4418 acc_val: 0.7251 time: 459.2219s\n",
            "Epoch: 0845 loss_train: 0.3432 acc_train: 0.8863 loss_val: 1.4464 acc_val: 0.7360 time: 459.7526s\n",
            "Epoch: 0846 loss_train: 0.3293 acc_train: 0.8980 loss_val: 1.4430 acc_val: 0.7149 time: 460.2750s\n",
            "Epoch: 0847 loss_train: 0.3410 acc_train: 0.8896 loss_val: 1.4441 acc_val: 0.7385 time: 460.7960s\n",
            "Epoch: 0848 loss_train: 0.3397 acc_train: 0.8946 loss_val: 1.4453 acc_val: 0.7335 time: 461.3133s\n",
            "Epoch: 0849 loss_train: 0.3369 acc_train: 0.8924 loss_val: 1.4473 acc_val: 0.7382 time: 461.8330s\n",
            "Epoch: 0850 loss_train: 0.3381 acc_train: 0.8921 loss_val: 1.4451 acc_val: 0.7425 time: 462.3506s\n",
            "Epoch: 0851 loss_train: 0.3392 acc_train: 0.8950 loss_val: 1.4421 acc_val: 0.7378 time: 462.8805s\n",
            "Epoch: 0852 loss_train: 0.3249 acc_train: 0.8974 loss_val: 1.4403 acc_val: 0.7258 time: 463.3964s\n",
            "Epoch: 0853 loss_train: 0.3284 acc_train: 0.8933 loss_val: 1.4407 acc_val: 0.7313 time: 463.9333s\n",
            "Epoch: 0854 loss_train: 0.3372 acc_train: 0.8954 loss_val: 1.4381 acc_val: 0.7320 time: 464.4476s\n",
            "Epoch: 0855 loss_train: 0.3307 acc_train: 0.8949 loss_val: 1.4362 acc_val: 0.7327 time: 464.9727s\n",
            "Epoch: 0856 loss_train: 0.3294 acc_train: 0.8978 loss_val: 1.4395 acc_val: 0.7342 time: 465.4892s\n",
            "Epoch: 0857 loss_train: 0.3383 acc_train: 0.8922 loss_val: 1.4469 acc_val: 0.7407 time: 466.0187s\n",
            "Epoch: 0858 loss_train: 0.3290 acc_train: 0.8976 loss_val: 1.4429 acc_val: 0.7320 time: 466.5277s\n",
            "Epoch: 0859 loss_train: 0.3418 acc_train: 0.8888 loss_val: 1.4467 acc_val: 0.7425 time: 467.0687s\n",
            "Epoch: 0860 loss_train: 0.3322 acc_train: 0.8982 loss_val: 1.4473 acc_val: 0.7378 time: 467.5968s\n",
            "Epoch: 0861 loss_train: 0.3320 acc_train: 0.9001 loss_val: 1.4426 acc_val: 0.7145 time: 468.1242s\n",
            "Epoch: 0862 loss_train: 0.3362 acc_train: 0.8945 loss_val: 1.4392 acc_val: 0.7153 time: 468.6410s\n",
            "Epoch: 0863 loss_train: 0.3397 acc_train: 0.8914 loss_val: 1.4467 acc_val: 0.7458 time: 469.1659s\n",
            "Epoch: 0864 loss_train: 0.3324 acc_train: 0.9006 loss_val: 1.4427 acc_val: 0.7389 time: 469.6843s\n",
            "Epoch: 0865 loss_train: 0.3265 acc_train: 0.8981 loss_val: 1.4361 acc_val: 0.7218 time: 470.2171s\n",
            "Epoch: 0866 loss_train: 0.3312 acc_train: 0.8941 loss_val: 1.4406 acc_val: 0.7407 time: 470.7405s\n",
            "Epoch: 0867 loss_train: 0.3410 acc_train: 0.8950 loss_val: 1.4401 acc_val: 0.7244 time: 471.2611s\n",
            "Epoch: 0868 loss_train: 0.3289 acc_train: 0.8948 loss_val: 1.4464 acc_val: 0.7233 time: 471.7939s\n",
            "Epoch: 0869 loss_train: 0.3312 acc_train: 0.8957 loss_val: 1.4448 acc_val: 0.7313 time: 472.3146s\n",
            "Epoch: 0870 loss_train: 0.3251 acc_train: 0.8986 loss_val: 1.4376 acc_val: 0.7215 time: 472.8354s\n",
            "Epoch: 0871 loss_train: 0.3260 acc_train: 0.8961 loss_val: 1.4354 acc_val: 0.7284 time: 473.3647s\n",
            "Epoch: 0872 loss_train: 0.3287 acc_train: 0.8972 loss_val: 1.4407 acc_val: 0.7440 time: 473.8890s\n",
            "Epoch: 0873 loss_train: 0.3249 acc_train: 0.9018 loss_val: 1.4348 acc_val: 0.7233 time: 474.4140s\n",
            "Epoch: 0874 loss_train: 0.3288 acc_train: 0.8938 loss_val: 1.4403 acc_val: 0.7287 time: 474.9309s\n",
            "Epoch: 0875 loss_train: 0.3331 acc_train: 0.8957 loss_val: 1.4490 acc_val: 0.7455 time: 475.4485s\n",
            "Epoch: 0876 loss_train: 0.3364 acc_train: 0.8990 loss_val: 1.4430 acc_val: 0.7196 time: 475.9723s\n",
            "Epoch: 0877 loss_train: 0.3308 acc_train: 0.8933 loss_val: 1.4408 acc_val: 0.7204 time: 476.4990s\n",
            "Epoch: 0878 loss_train: 0.3404 acc_train: 0.8900 loss_val: 1.4478 acc_val: 0.7415 time: 477.0149s\n",
            "Epoch: 0879 loss_train: 0.3363 acc_train: 0.8983 loss_val: 1.4388 acc_val: 0.7196 time: 477.5333s\n",
            "Epoch: 0880 loss_train: 0.3360 acc_train: 0.8930 loss_val: 1.4351 acc_val: 0.7233 time: 478.0644s\n",
            "Epoch: 0881 loss_train: 0.3335 acc_train: 0.8954 loss_val: 1.4401 acc_val: 0.7371 time: 478.5886s\n",
            "Epoch: 0882 loss_train: 0.3390 acc_train: 0.8981 loss_val: 1.4379 acc_val: 0.7124 time: 479.1288s\n",
            "Epoch: 0883 loss_train: 0.3435 acc_train: 0.8863 loss_val: 1.4460 acc_val: 0.7360 time: 479.6616s\n",
            "Epoch: 0884 loss_train: 0.3312 acc_train: 0.8999 loss_val: 1.4476 acc_val: 0.7418 time: 480.1983s\n",
            "Epoch: 0885 loss_train: 0.3437 acc_train: 0.8934 loss_val: 1.4448 acc_val: 0.7211 time: 480.7195s\n",
            "Epoch: 0886 loss_train: 0.3373 acc_train: 0.8920 loss_val: 1.4453 acc_val: 0.7338 time: 481.2365s\n",
            "Epoch: 0887 loss_train: 0.3290 acc_train: 0.8964 loss_val: 1.4428 acc_val: 0.7411 time: 481.7624s\n",
            "Epoch: 0888 loss_train: 0.3361 acc_train: 0.8975 loss_val: 1.4317 acc_val: 0.7164 time: 482.2942s\n",
            "Epoch: 0889 loss_train: 0.3423 acc_train: 0.8887 loss_val: 1.4322 acc_val: 0.7276 time: 482.8172s\n",
            "Epoch: 0890 loss_train: 0.3300 acc_train: 0.8973 loss_val: 1.4331 acc_val: 0.7236 time: 483.3346s\n",
            "Epoch: 0891 loss_train: 0.3306 acc_train: 0.8921 loss_val: 1.4389 acc_val: 0.7211 time: 483.8541s\n",
            "Epoch: 0892 loss_train: 0.3349 acc_train: 0.8922 loss_val: 1.4524 acc_val: 0.7447 time: 484.3779s\n",
            "Epoch: 0893 loss_train: 0.3329 acc_train: 0.8976 loss_val: 1.4463 acc_val: 0.7345 time: 484.9032s\n",
            "Epoch: 0894 loss_train: 0.3302 acc_train: 0.8976 loss_val: 1.4354 acc_val: 0.7200 time: 485.4260s\n",
            "Epoch: 0895 loss_train: 0.3355 acc_train: 0.8929 loss_val: 1.4314 acc_val: 0.7269 time: 485.9629s\n",
            "Epoch: 0896 loss_train: 0.3252 acc_train: 0.8975 loss_val: 1.4346 acc_val: 0.7425 time: 486.4945s\n",
            "Epoch: 0897 loss_train: 0.3370 acc_train: 0.8961 loss_val: 1.4330 acc_val: 0.7218 time: 487.0223s\n",
            "Epoch: 0898 loss_train: 0.3247 acc_train: 0.8968 loss_val: 1.4384 acc_val: 0.7182 time: 487.5430s\n",
            "Epoch: 0899 loss_train: 0.3273 acc_train: 0.8984 loss_val: 1.4391 acc_val: 0.7356 time: 488.1008s\n",
            "Epoch: 0900 loss_train: 0.3338 acc_train: 0.8966 loss_val: 1.4400 acc_val: 0.7313 time: 488.6345s\n",
            "Epoch: 0901 loss_train: 0.3266 acc_train: 0.8963 loss_val: 1.4445 acc_val: 0.7385 time: 489.1728s\n",
            "Epoch: 0902 loss_train: 0.3289 acc_train: 0.8982 loss_val: 1.4411 acc_val: 0.7327 time: 489.6960s\n",
            "Epoch: 0903 loss_train: 0.3272 acc_train: 0.8958 loss_val: 1.4343 acc_val: 0.7247 time: 490.2246s\n",
            "Epoch: 0904 loss_train: 0.3269 acc_train: 0.8923 loss_val: 1.4341 acc_val: 0.7335 time: 490.7480s\n",
            "Epoch: 0905 loss_train: 0.3320 acc_train: 0.8923 loss_val: 1.4377 acc_val: 0.7298 time: 491.2628s\n",
            "Epoch: 0906 loss_train: 0.3240 acc_train: 0.8977 loss_val: 1.4376 acc_val: 0.7233 time: 491.7863s\n",
            "Epoch: 0907 loss_train: 0.3355 acc_train: 0.8952 loss_val: 1.4430 acc_val: 0.7375 time: 492.3119s\n",
            "Epoch: 0908 loss_train: 0.3206 acc_train: 0.9014 loss_val: 1.4367 acc_val: 0.7229 time: 492.8438s\n",
            "Epoch: 0909 loss_train: 0.3226 acc_train: 0.8940 loss_val: 1.4379 acc_val: 0.7251 time: 493.3681s\n",
            "Epoch: 0910 loss_train: 0.3287 acc_train: 0.8929 loss_val: 1.4444 acc_val: 0.7444 time: 493.8986s\n",
            "Epoch: 0911 loss_train: 0.3349 acc_train: 0.8979 loss_val: 1.4350 acc_val: 0.7153 time: 494.4124s\n",
            "Epoch: 0912 loss_train: 0.3349 acc_train: 0.8893 loss_val: 1.4401 acc_val: 0.7291 time: 494.9350s\n",
            "Epoch: 0913 loss_train: 0.3250 acc_train: 0.8964 loss_val: 1.4440 acc_val: 0.7324 time: 495.4420s\n",
            "Epoch: 0914 loss_train: 0.3330 acc_train: 0.8993 loss_val: 1.4464 acc_val: 0.7273 time: 495.9818s\n",
            "Epoch: 0915 loss_train: 0.3327 acc_train: 0.8957 loss_val: 1.4453 acc_val: 0.7320 time: 496.5029s\n",
            "Epoch: 0916 loss_train: 0.3360 acc_train: 0.8967 loss_val: 1.4409 acc_val: 0.7262 time: 497.0230s\n",
            "Epoch: 0917 loss_train: 0.3382 acc_train: 0.8919 loss_val: 1.4354 acc_val: 0.7189 time: 497.5464s\n",
            "Epoch: 0918 loss_train: 0.3317 acc_train: 0.8954 loss_val: 1.4371 acc_val: 0.7396 time: 498.0827s\n",
            "Epoch: 0919 loss_train: 0.3280 acc_train: 0.8969 loss_val: 1.4384 acc_val: 0.7425 time: 498.6065s\n",
            "Epoch: 0920 loss_train: 0.3332 acc_train: 0.8968 loss_val: 1.4378 acc_val: 0.7153 time: 499.1294s\n",
            "Epoch: 0921 loss_train: 0.3456 acc_train: 0.8839 loss_val: 1.4487 acc_val: 0.7331 time: 499.6541s\n",
            "Epoch: 0922 loss_train: 0.3350 acc_train: 0.8960 loss_val: 1.4455 acc_val: 0.7244 time: 500.1791s\n",
            "Epoch: 0923 loss_train: 0.3281 acc_train: 0.8963 loss_val: 1.4391 acc_val: 0.7160 time: 500.7107s\n",
            "Epoch: 0924 loss_train: 0.3398 acc_train: 0.8886 loss_val: 1.4401 acc_val: 0.7404 time: 501.2268s\n",
            "Epoch: 0925 loss_train: 0.3348 acc_train: 0.8957 loss_val: 1.4349 acc_val: 0.7167 time: 501.7596s\n",
            "Epoch: 0926 loss_train: 0.3493 acc_train: 0.8818 loss_val: 1.4401 acc_val: 0.7385 time: 502.2777s\n",
            "Epoch: 0927 loss_train: 0.3257 acc_train: 0.8983 loss_val: 1.4415 acc_val: 0.7295 time: 502.8152s\n",
            "Epoch: 0928 loss_train: 0.3238 acc_train: 0.8981 loss_val: 1.4418 acc_val: 0.7273 time: 503.3358s\n",
            "Epoch: 0929 loss_train: 0.3381 acc_train: 0.8966 loss_val: 1.4404 acc_val: 0.7284 time: 503.8765s\n",
            "Epoch: 0930 loss_train: 0.3278 acc_train: 0.8960 loss_val: 1.4429 acc_val: 0.7425 time: 504.3974s\n",
            "Epoch: 0931 loss_train: 0.3414 acc_train: 0.8992 loss_val: 1.4300 acc_val: 0.7036 time: 504.9300s\n",
            "Epoch: 0932 loss_train: 0.3475 acc_train: 0.8853 loss_val: 1.4405 acc_val: 0.7324 time: 505.4487s\n",
            "Epoch: 0933 loss_train: 0.3250 acc_train: 0.9004 loss_val: 1.4410 acc_val: 0.7240 time: 505.9819s\n",
            "Epoch: 0934 loss_train: 0.3368 acc_train: 0.8957 loss_val: 1.4356 acc_val: 0.7244 time: 506.5005s\n",
            "Epoch: 0935 loss_train: 0.3277 acc_train: 0.8969 loss_val: 1.4379 acc_val: 0.7338 time: 507.0248s\n",
            "Epoch: 0936 loss_train: 0.3316 acc_train: 0.8973 loss_val: 1.4326 acc_val: 0.7244 time: 507.5391s\n",
            "Epoch: 0937 loss_train: 0.3233 acc_train: 0.8958 loss_val: 1.4344 acc_val: 0.7236 time: 508.0688s\n",
            "Epoch: 0938 loss_train: 0.3261 acc_train: 0.8973 loss_val: 1.4337 acc_val: 0.7342 time: 508.5983s\n",
            "Epoch: 0939 loss_train: 0.3261 acc_train: 0.8969 loss_val: 1.4325 acc_val: 0.7316 time: 509.1288s\n",
            "Epoch: 0940 loss_train: 0.3251 acc_train: 0.8983 loss_val: 1.4319 acc_val: 0.7265 time: 509.6470s\n",
            "Epoch: 0941 loss_train: 0.3280 acc_train: 0.8937 loss_val: 1.4366 acc_val: 0.7291 time: 510.1819s\n",
            "Epoch: 0942 loss_train: 0.3207 acc_train: 0.9015 loss_val: 1.4378 acc_val: 0.7265 time: 510.7043s\n",
            "Epoch: 0943 loss_train: 0.3313 acc_train: 0.8992 loss_val: 1.4350 acc_val: 0.7244 time: 511.2250s\n",
            "Epoch: 0944 loss_train: 0.3258 acc_train: 0.8974 loss_val: 1.4394 acc_val: 0.7309 time: 511.7576s\n",
            "Epoch: 0945 loss_train: 0.3270 acc_train: 0.8981 loss_val: 1.4361 acc_val: 0.7178 time: 512.2880s\n",
            "Epoch: 0946 loss_train: 0.3280 acc_train: 0.8932 loss_val: 1.4354 acc_val: 0.7345 time: 512.8158s\n",
            "Epoch: 0947 loss_train: 0.3279 acc_train: 0.8950 loss_val: 1.4326 acc_val: 0.7204 time: 513.3469s\n",
            "Epoch: 0948 loss_train: 0.3320 acc_train: 0.8935 loss_val: 1.4372 acc_val: 0.7353 time: 513.8741s\n",
            "Epoch: 0949 loss_train: 0.3197 acc_train: 0.9007 loss_val: 1.4372 acc_val: 0.7269 time: 514.3952s\n",
            "Epoch: 0950 loss_train: 0.3278 acc_train: 0.8977 loss_val: 1.4372 acc_val: 0.7204 time: 514.9267s\n",
            "Epoch: 0951 loss_train: 0.3319 acc_train: 0.8962 loss_val: 1.4397 acc_val: 0.7225 time: 515.4488s\n",
            "Epoch: 0952 loss_train: 0.3268 acc_train: 0.8975 loss_val: 1.4364 acc_val: 0.7142 time: 515.9925s\n",
            "Epoch: 0953 loss_train: 0.3292 acc_train: 0.8951 loss_val: 1.4409 acc_val: 0.7320 time: 516.5289s\n",
            "Epoch: 0954 loss_train: 0.3156 acc_train: 0.9039 loss_val: 1.4421 acc_val: 0.7324 time: 517.0634s\n",
            "Epoch: 0955 loss_train: 0.3346 acc_train: 0.8947 loss_val: 1.4388 acc_val: 0.7273 time: 517.5735s\n",
            "Epoch: 0956 loss_train: 0.3241 acc_train: 0.8966 loss_val: 1.4383 acc_val: 0.7273 time: 518.1021s\n",
            "Epoch: 0957 loss_train: 0.3239 acc_train: 0.8978 loss_val: 1.4347 acc_val: 0.7149 time: 518.6350s\n",
            "Epoch: 0958 loss_train: 0.3299 acc_train: 0.8983 loss_val: 1.4412 acc_val: 0.7342 time: 519.1790s\n",
            "Epoch: 0959 loss_train: 0.3216 acc_train: 0.9014 loss_val: 1.4359 acc_val: 0.7200 time: 519.7029s\n",
            "Epoch: 0960 loss_train: 0.3263 acc_train: 0.8983 loss_val: 1.4298 acc_val: 0.7018 time: 520.2249s\n",
            "Epoch: 0961 loss_train: 0.3335 acc_train: 0.8916 loss_val: 1.4404 acc_val: 0.7367 time: 520.7386s\n",
            "Epoch: 0962 loss_train: 0.3345 acc_train: 0.8977 loss_val: 1.4428 acc_val: 0.7225 time: 521.2571s\n",
            "Epoch: 0963 loss_train: 0.3172 acc_train: 0.9016 loss_val: 1.4401 acc_val: 0.7200 time: 521.7793s\n",
            "Epoch: 0964 loss_train: 0.3334 acc_train: 0.8886 loss_val: 1.4465 acc_val: 0.7407 time: 522.3084s\n",
            "Epoch: 0965 loss_train: 0.3450 acc_train: 0.8998 loss_val: 1.4363 acc_val: 0.6895 time: 522.8307s\n",
            "Epoch: 0966 loss_train: 0.3633 acc_train: 0.8808 loss_val: 1.4452 acc_val: 0.7356 time: 523.3548s\n",
            "Epoch: 0967 loss_train: 0.3283 acc_train: 0.9000 loss_val: 1.4399 acc_val: 0.7258 time: 523.8889s\n",
            "Epoch: 0968 loss_train: 0.3481 acc_train: 0.8950 loss_val: 1.4279 acc_val: 0.6862 time: 524.3984s\n",
            "Epoch: 0969 loss_train: 0.3725 acc_train: 0.8765 loss_val: 1.4422 acc_val: 0.7549 time: 524.9148s\n",
            "Epoch: 0970 loss_train: 0.3439 acc_train: 0.8962 loss_val: 1.4401 acc_val: 0.7353 time: 525.4346s\n",
            "Epoch: 0971 loss_train: 0.3479 acc_train: 0.8961 loss_val: 1.4388 acc_val: 0.6698 time: 525.9555s\n",
            "Epoch: 0972 loss_train: 0.3818 acc_train: 0.8731 loss_val: 1.4525 acc_val: 0.7236 time: 526.4910s\n",
            "Epoch: 0973 loss_train: 0.3544 acc_train: 0.8903 loss_val: 1.4494 acc_val: 0.7418 time: 527.0317s\n",
            "Epoch: 0974 loss_train: 0.3489 acc_train: 0.8946 loss_val: 1.4318 acc_val: 0.7309 time: 527.5580s\n",
            "Epoch: 0975 loss_train: 0.3481 acc_train: 0.8868 loss_val: 1.4067 acc_val: 0.7185 time: 528.0828s\n",
            "Epoch: 0976 loss_train: 0.3343 acc_train: 0.8897 loss_val: 1.4261 acc_val: 0.7338 time: 528.6092s\n",
            "Epoch: 0977 loss_train: 0.4035 acc_train: 0.8767 loss_val: 1.4361 acc_val: 0.6695 time: 529.1473s\n",
            "Epoch: 0978 loss_train: 0.4348 acc_train: 0.8634 loss_val: 1.4907 acc_val: 0.7509 time: 529.6617s\n",
            "Epoch: 0979 loss_train: 0.4600 acc_train: 0.8773 loss_val: 1.4737 acc_val: 0.6738 time: 530.1980s\n",
            "Epoch: 0980 loss_train: 0.4544 acc_train: 0.8552 loss_val: 1.4578 acc_val: 0.6873 time: 530.7297s\n",
            "Epoch: 0981 loss_train: 0.4278 acc_train: 0.8558 loss_val: 1.4760 acc_val: 0.6898 time: 531.2555s\n",
            "Epoch: 0982 loss_train: 0.4863 acc_train: 0.8380 loss_val: 1.4917 acc_val: 0.7040 time: 531.7807s\n",
            "Epoch: 0983 loss_train: 0.4861 acc_train: 0.8430 loss_val: 1.4860 acc_val: 0.6862 time: 532.3061s\n",
            "Epoch: 0984 loss_train: 0.4499 acc_train: 0.8535 loss_val: 1.5250 acc_val: 0.6945 time: 532.8357s\n",
            "Epoch: 0985 loss_train: 0.4425 acc_train: 0.8562 loss_val: 1.5487 acc_val: 0.6971 time: 533.3613s\n",
            "Epoch: 0986 loss_train: 0.4692 acc_train: 0.8546 loss_val: 1.5264 acc_val: 0.6727 time: 533.8956s\n",
            "Epoch: 0987 loss_train: 0.4825 acc_train: 0.8390 loss_val: 1.5262 acc_val: 0.7236 time: 534.4287s\n",
            "Epoch: 0988 loss_train: 0.4371 acc_train: 0.8700 loss_val: 1.5015 acc_val: 0.7080 time: 534.9489s\n",
            "Epoch: 0989 loss_train: 0.4204 acc_train: 0.8631 loss_val: 1.4866 acc_val: 0.7062 time: 535.4781s\n",
            "Epoch: 0990 loss_train: 0.4396 acc_train: 0.8505 loss_val: 1.4914 acc_val: 0.7327 time: 536.0058s\n",
            "Epoch: 0991 loss_train: 0.4331 acc_train: 0.8618 loss_val: 1.4791 acc_val: 0.7240 time: 536.5371s\n",
            "Epoch: 0992 loss_train: 0.4017 acc_train: 0.8700 loss_val: 1.4832 acc_val: 0.7080 time: 537.0627s\n",
            "Epoch: 0993 loss_train: 0.4061 acc_train: 0.8616 loss_val: 1.4989 acc_val: 0.7345 time: 537.5889s\n",
            "Epoch: 0994 loss_train: 0.4116 acc_train: 0.8704 loss_val: 1.4845 acc_val: 0.7345 time: 538.1333s\n",
            "Epoch: 0995 loss_train: 0.3975 acc_train: 0.8778 loss_val: 1.4469 acc_val: 0.7062 time: 538.6858s\n",
            "Epoch: 0996 loss_train: 0.4072 acc_train: 0.8572 loss_val: 1.4369 acc_val: 0.7284 time: 539.2188s\n",
            "Epoch: 0997 loss_train: 0.3819 acc_train: 0.8756 loss_val: 1.4452 acc_val: 0.7418 time: 539.7519s\n",
            "Epoch: 0998 loss_train: 0.3871 acc_train: 0.8748 loss_val: 1.4517 acc_val: 0.7327 time: 540.2645s\n",
            "Epoch: 0999 loss_train: 0.3992 acc_train: 0.8661 loss_val: 1.4600 acc_val: 0.7396 time: 540.7898s\n",
            "Epoch: 1000 loss_train: 0.3876 acc_train: 0.8725 loss_val: 1.4580 acc_val: 0.7491 time: 541.3144s\n",
            "Epoch: 1001 loss_train: 0.3810 acc_train: 0.8804 loss_val: 1.4444 acc_val: 0.7375 time: 541.8387s\n",
            "Epoch: 1002 loss_train: 0.3788 acc_train: 0.8759 loss_val: 1.4539 acc_val: 0.7327 time: 542.3776s\n",
            "Epoch: 1003 loss_train: 0.3724 acc_train: 0.8781 loss_val: 1.4691 acc_val: 0.7364 time: 542.8994s\n",
            "Epoch: 1004 loss_train: 0.3927 acc_train: 0.8760 loss_val: 1.4737 acc_val: 0.7498 time: 543.4200s\n",
            "Epoch: 1005 loss_train: 0.3865 acc_train: 0.8832 loss_val: 1.4644 acc_val: 0.7389 time: 543.9508s\n",
            "Epoch: 1006 loss_train: 0.3722 acc_train: 0.8786 loss_val: 1.4625 acc_val: 0.7462 time: 544.4784s\n",
            "Epoch: 1007 loss_train: 0.3694 acc_train: 0.8826 loss_val: 1.4625 acc_val: 0.7535 time: 544.9942s\n",
            "Epoch: 1008 loss_train: 0.3613 acc_train: 0.8849 loss_val: 1.4558 acc_val: 0.7498 time: 545.5223s\n",
            "Epoch: 1009 loss_train: 0.3759 acc_train: 0.8796 loss_val: 1.4533 acc_val: 0.7502 time: 546.0514s\n",
            "Epoch: 1010 loss_train: 0.3711 acc_train: 0.8803 loss_val: 1.4559 acc_val: 0.7513 time: 546.5871s\n",
            "Epoch: 1011 loss_train: 0.3592 acc_train: 0.8834 loss_val: 1.4670 acc_val: 0.7495 time: 547.1189s\n",
            "Epoch: 1012 loss_train: 0.3684 acc_train: 0.8842 loss_val: 1.4720 acc_val: 0.7545 time: 547.6458s\n",
            "Epoch: 1013 loss_train: 0.3631 acc_train: 0.8849 loss_val: 1.4807 acc_val: 0.7600 time: 548.1705s\n",
            "Epoch: 1014 loss_train: 0.3677 acc_train: 0.8862 loss_val: 1.4772 acc_val: 0.7560 time: 548.7053s\n",
            "Epoch: 1015 loss_train: 0.3671 acc_train: 0.8835 loss_val: 1.4716 acc_val: 0.7425 time: 549.2495s\n",
            "Epoch: 1016 loss_train: 0.3675 acc_train: 0.8796 loss_val: 1.4681 acc_val: 0.7495 time: 549.7792s\n",
            "Epoch: 1017 loss_train: 0.3577 acc_train: 0.8828 loss_val: 1.4748 acc_val: 0.7673 time: 550.3050s\n",
            "Epoch: 1018 loss_train: 0.3753 acc_train: 0.8889 loss_val: 1.4698 acc_val: 0.7545 time: 550.8327s\n",
            "Epoch: 1019 loss_train: 0.3719 acc_train: 0.8808 loss_val: 1.4725 acc_val: 0.7600 time: 551.3611s\n",
            "Epoch: 1020 loss_train: 0.3526 acc_train: 0.8883 loss_val: 1.4733 acc_val: 0.7589 time: 551.8867s\n",
            "Epoch: 1021 loss_train: 0.3711 acc_train: 0.8857 loss_val: 1.4677 acc_val: 0.7444 time: 552.3940s\n",
            "Epoch: 1022 loss_train: 0.3643 acc_train: 0.8811 loss_val: 1.4741 acc_val: 0.7542 time: 552.9265s\n",
            "Epoch: 1023 loss_train: 0.3572 acc_train: 0.8824 loss_val: 1.4881 acc_val: 0.7655 time: 553.4443s\n",
            "Epoch: 1024 loss_train: 0.3664 acc_train: 0.8860 loss_val: 1.4806 acc_val: 0.7593 time: 553.9836s\n",
            "Epoch: 1025 loss_train: 0.3672 acc_train: 0.8846 loss_val: 1.4737 acc_val: 0.7505 time: 554.4988s\n",
            "Epoch: 1026 loss_train: 0.3686 acc_train: 0.8772 loss_val: 1.4805 acc_val: 0.7713 time: 555.0330s\n",
            "Epoch: 1027 loss_train: 0.3551 acc_train: 0.8897 loss_val: 1.4779 acc_val: 0.7705 time: 555.5441s\n",
            "Epoch: 1028 loss_train: 0.3540 acc_train: 0.8876 loss_val: 1.4690 acc_val: 0.7593 time: 556.0680s\n",
            "Epoch: 1029 loss_train: 0.3554 acc_train: 0.8860 loss_val: 1.4664 acc_val: 0.7575 time: 556.5867s\n",
            "Epoch: 1030 loss_train: 0.3568 acc_train: 0.8870 loss_val: 1.4715 acc_val: 0.7589 time: 557.1218s\n",
            "Epoch: 1031 loss_train: 0.3496 acc_train: 0.8906 loss_val: 1.4785 acc_val: 0.7593 time: 557.6498s\n",
            "Epoch: 1032 loss_train: 0.3539 acc_train: 0.8877 loss_val: 1.4797 acc_val: 0.7593 time: 558.1750s\n",
            "Epoch: 1033 loss_train: 0.3559 acc_train: 0.8872 loss_val: 1.4776 acc_val: 0.7593 time: 558.6959s\n",
            "Epoch: 1034 loss_train: 0.3598 acc_train: 0.8870 loss_val: 1.4756 acc_val: 0.7593 time: 559.2217s\n",
            "Epoch: 1035 loss_train: 0.3546 acc_train: 0.8866 loss_val: 1.4758 acc_val: 0.7618 time: 559.7336s\n",
            "Epoch: 1036 loss_train: 0.3535 acc_train: 0.8891 loss_val: 1.4759 acc_val: 0.7625 time: 560.2562s\n",
            "Epoch: 1037 loss_train: 0.3441 acc_train: 0.8897 loss_val: 1.4727 acc_val: 0.7556 time: 560.7850s\n",
            "Epoch: 1038 loss_train: 0.3495 acc_train: 0.8848 loss_val: 1.4827 acc_val: 0.7629 time: 561.2942s\n",
            "Epoch: 1039 loss_train: 0.3615 acc_train: 0.8861 loss_val: 1.4809 acc_val: 0.7640 time: 561.8237s\n",
            "Epoch: 1040 loss_train: 0.3489 acc_train: 0.8915 loss_val: 1.4738 acc_val: 0.7538 time: 562.3418s\n",
            "Epoch: 1041 loss_train: 0.3442 acc_train: 0.8869 loss_val: 1.4805 acc_val: 0.7578 time: 562.8741s\n",
            "Epoch: 1042 loss_train: 0.3519 acc_train: 0.8892 loss_val: 1.4836 acc_val: 0.7607 time: 563.4053s\n",
            "Epoch: 1043 loss_train: 0.3552 acc_train: 0.8888 loss_val: 1.4827 acc_val: 0.7571 time: 563.9303s\n",
            "Epoch: 1044 loss_train: 0.3512 acc_train: 0.8863 loss_val: 1.4841 acc_val: 0.7604 time: 564.4535s\n",
            "Epoch: 1045 loss_train: 0.3551 acc_train: 0.8868 loss_val: 1.4812 acc_val: 0.7673 time: 564.9799s\n",
            "Epoch: 1046 loss_train: 0.3581 acc_train: 0.8843 loss_val: 1.4763 acc_val: 0.7578 time: 565.5072s\n",
            "Epoch: 1047 loss_train: 0.3523 acc_train: 0.8863 loss_val: 1.4815 acc_val: 0.7596 time: 566.0308s\n",
            "Epoch: 1048 loss_train: 0.3493 acc_train: 0.8851 loss_val: 1.4954 acc_val: 0.7716 time: 566.5534s\n",
            "Epoch: 1049 loss_train: 0.3484 acc_train: 0.8959 loss_val: 1.4910 acc_val: 0.7604 time: 567.0820s\n",
            "Epoch: 1050 loss_train: 0.3445 acc_train: 0.8931 loss_val: 1.4795 acc_val: 0.7560 time: 567.6105s\n",
            "Epoch: 1051 loss_train: 0.3539 acc_train: 0.8872 loss_val: 1.4796 acc_val: 0.7615 time: 568.1434s\n",
            "Epoch: 1052 loss_train: 0.3507 acc_train: 0.8858 loss_val: 1.4829 acc_val: 0.7647 time: 568.6660s\n",
            "Epoch: 1053 loss_train: 0.3581 acc_train: 0.8885 loss_val: 1.4888 acc_val: 0.7615 time: 569.1863s\n",
            "Epoch: 1054 loss_train: 0.3492 acc_train: 0.8915 loss_val: 1.4938 acc_val: 0.7625 time: 569.7254s\n",
            "Epoch: 1055 loss_train: 0.3450 acc_train: 0.8887 loss_val: 1.4913 acc_val: 0.7615 time: 570.2456s\n",
            "Epoch: 1056 loss_train: 0.3490 acc_train: 0.8904 loss_val: 1.4832 acc_val: 0.7542 time: 570.7614s\n",
            "Epoch: 1057 loss_train: 0.3461 acc_train: 0.8858 loss_val: 1.4843 acc_val: 0.7567 time: 571.2832s\n",
            "Epoch: 1058 loss_train: 0.3507 acc_train: 0.8877 loss_val: 1.4884 acc_val: 0.7658 time: 571.8078s\n",
            "Epoch: 1059 loss_train: 0.3557 acc_train: 0.8872 loss_val: 1.4898 acc_val: 0.7618 time: 572.3510s\n",
            "Epoch: 1060 loss_train: 0.3488 acc_train: 0.8883 loss_val: 1.4914 acc_val: 0.7589 time: 572.8667s\n",
            "Epoch: 1061 loss_train: 0.3431 acc_train: 0.8895 loss_val: 1.4934 acc_val: 0.7593 time: 573.3870s\n",
            "Epoch: 1062 loss_train: 0.3553 acc_train: 0.8874 loss_val: 1.4896 acc_val: 0.7622 time: 573.9028s\n",
            "Epoch: 1063 loss_train: 0.3462 acc_train: 0.8893 loss_val: 1.4824 acc_val: 0.7531 time: 574.4249s\n",
            "Epoch: 1064 loss_train: 0.3512 acc_train: 0.8850 loss_val: 1.4837 acc_val: 0.7535 time: 574.9656s\n",
            "Epoch: 1065 loss_train: 0.3532 acc_train: 0.8853 loss_val: 1.5006 acc_val: 0.7709 time: 575.4958s\n",
            "Epoch: 1066 loss_train: 0.3479 acc_train: 0.8955 loss_val: 1.5039 acc_val: 0.7582 time: 576.0281s\n",
            "Epoch: 1067 loss_train: 0.3639 acc_train: 0.8846 loss_val: 1.4876 acc_val: 0.7560 time: 576.5644s\n",
            "Epoch: 1068 loss_train: 0.3490 acc_train: 0.8866 loss_val: 1.4848 acc_val: 0.7560 time: 577.1024s\n",
            "Epoch: 1069 loss_train: 0.3766 acc_train: 0.8827 loss_val: 1.4854 acc_val: 0.7527 time: 577.6172s\n",
            "Epoch: 1070 loss_train: 0.3508 acc_train: 0.8859 loss_val: 1.5066 acc_val: 0.7640 time: 578.1481s\n",
            "Epoch: 1071 loss_train: 0.3570 acc_train: 0.8883 loss_val: 1.5084 acc_val: 0.7709 time: 578.6786s\n",
            "Epoch: 1072 loss_train: 0.3504 acc_train: 0.8925 loss_val: 1.4882 acc_val: 0.7618 time: 579.1973s\n",
            "Epoch: 1073 loss_train: 0.3463 acc_train: 0.8913 loss_val: 1.4742 acc_val: 0.7545 time: 579.7272s\n",
            "Epoch: 1074 loss_train: 0.3609 acc_train: 0.8749 loss_val: 1.4864 acc_val: 0.7644 time: 580.2441s\n",
            "Epoch: 1075 loss_train: 0.3589 acc_train: 0.8892 loss_val: 1.4951 acc_val: 0.7567 time: 580.7611s\n",
            "Epoch: 1076 loss_train: 0.3649 acc_train: 0.8830 loss_val: 1.5034 acc_val: 0.7593 time: 581.2803s\n",
            "Epoch: 1077 loss_train: 0.3460 acc_train: 0.8907 loss_val: 1.4982 acc_val: 0.7575 time: 581.7983s\n",
            "Epoch: 1078 loss_train: 0.3647 acc_train: 0.8885 loss_val: 1.4844 acc_val: 0.7524 time: 582.3235s\n",
            "Epoch: 1079 loss_train: 0.3508 acc_train: 0.8887 loss_val: 1.4802 acc_val: 0.7607 time: 582.8519s\n",
            "Epoch: 1080 loss_train: 0.3652 acc_train: 0.8786 loss_val: 1.4951 acc_val: 0.7720 time: 583.3848s\n",
            "Epoch: 1081 loss_train: 0.3536 acc_train: 0.8911 loss_val: 1.4907 acc_val: 0.7589 time: 583.9267s\n",
            "Epoch: 1082 loss_train: 0.3518 acc_train: 0.8875 loss_val: 1.4936 acc_val: 0.7527 time: 584.4427s\n",
            "Epoch: 1083 loss_train: 0.3667 acc_train: 0.8768 loss_val: 1.4997 acc_val: 0.7669 time: 584.9615s\n",
            "Epoch: 1084 loss_train: 0.3523 acc_train: 0.8950 loss_val: 1.4837 acc_val: 0.7629 time: 585.4931s\n",
            "Epoch: 1085 loss_train: 0.3564 acc_train: 0.8897 loss_val: 1.4852 acc_val: 0.7625 time: 586.0136s\n",
            "Epoch: 1086 loss_train: 0.3572 acc_train: 0.8847 loss_val: 1.4897 acc_val: 0.7640 time: 586.5376s\n",
            "Epoch: 1087 loss_train: 0.3515 acc_train: 0.8911 loss_val: 1.4897 acc_val: 0.7625 time: 587.0571s\n",
            "Epoch: 1088 loss_train: 0.3431 acc_train: 0.8923 loss_val: 1.4883 acc_val: 0.7607 time: 587.5760s\n",
            "Epoch: 1089 loss_train: 0.3526 acc_train: 0.8871 loss_val: 1.4848 acc_val: 0.7549 time: 588.1003s\n",
            "Epoch: 1090 loss_train: 0.3520 acc_train: 0.8908 loss_val: 1.4809 acc_val: 0.7498 time: 588.6233s\n",
            "Epoch: 1091 loss_train: 0.3598 acc_train: 0.8781 loss_val: 1.4981 acc_val: 0.7680 time: 589.1429s\n",
            "Epoch: 1092 loss_train: 0.3527 acc_train: 0.8892 loss_val: 1.5067 acc_val: 0.7753 time: 589.6755s\n",
            "Epoch: 1093 loss_train: 0.3608 acc_train: 0.8935 loss_val: 1.4871 acc_val: 0.7589 time: 590.1904s\n",
            "Epoch: 1094 loss_train: 0.3480 acc_train: 0.8854 loss_val: 1.4815 acc_val: 0.7473 time: 590.7069s\n",
            "Epoch: 1095 loss_train: 0.3514 acc_train: 0.8853 loss_val: 1.4873 acc_val: 0.7545 time: 591.2307s\n",
            "Epoch: 1096 loss_train: 0.3472 acc_train: 0.8902 loss_val: 1.4927 acc_val: 0.7589 time: 591.7578s\n",
            "Epoch: 1097 loss_train: 0.3462 acc_train: 0.8919 loss_val: 1.4938 acc_val: 0.7556 time: 592.2882s\n",
            "Epoch: 1098 loss_train: 0.3605 acc_train: 0.8860 loss_val: 1.4999 acc_val: 0.7716 time: 592.8205s\n",
            "Epoch: 1099 loss_train: 0.3565 acc_train: 0.8912 loss_val: 1.4891 acc_val: 0.7640 time: 593.3617s\n",
            "Epoch: 1100 loss_train: 0.3464 acc_train: 0.8942 loss_val: 1.4869 acc_val: 0.7520 time: 593.8894s\n",
            "Epoch: 1101 loss_train: 0.3502 acc_train: 0.8878 loss_val: 1.4913 acc_val: 0.7564 time: 594.4185s\n",
            "Epoch: 1102 loss_train: 0.3470 acc_train: 0.8943 loss_val: 1.4862 acc_val: 0.7545 time: 594.9435s\n",
            "Epoch: 1103 loss_train: 0.3501 acc_train: 0.8884 loss_val: 1.4914 acc_val: 0.7655 time: 595.4812s\n",
            "Epoch: 1104 loss_train: 0.3513 acc_train: 0.8891 loss_val: 1.4867 acc_val: 0.7698 time: 596.0043s\n",
            "Epoch: 1105 loss_train: 0.3518 acc_train: 0.8949 loss_val: 1.4741 acc_val: 0.7484 time: 596.5199s\n",
            "Epoch: 1106 loss_train: 0.3480 acc_train: 0.8860 loss_val: 1.4823 acc_val: 0.7520 time: 597.0474s\n",
            "Epoch: 1107 loss_train: 0.3544 acc_train: 0.8827 loss_val: 1.5114 acc_val: 0.7556 time: 597.5734s\n",
            "Epoch: 1108 loss_train: 0.3826 acc_train: 0.8878 loss_val: 1.5027 acc_val: 0.7356 time: 598.1034s\n",
            "Epoch: 1109 loss_train: 0.3765 acc_train: 0.8731 loss_val: 1.4953 acc_val: 0.7440 time: 598.6412s\n",
            "Epoch: 1110 loss_train: 0.3678 acc_train: 0.8770 loss_val: 1.5079 acc_val: 0.7578 time: 599.1811s\n",
            "Epoch: 1111 loss_train: 0.4082 acc_train: 0.8823 loss_val: 1.4917 acc_val: 0.7484 time: 599.7331s\n",
            "Epoch: 1112 loss_train: 0.3600 acc_train: 0.8804 loss_val: 1.4882 acc_val: 0.7425 time: 600.2803s\n",
            "Epoch: 1113 loss_train: 0.3754 acc_train: 0.8771 loss_val: 1.4863 acc_val: 0.7596 time: 600.8259s\n",
            "Epoch: 1114 loss_train: 0.3729 acc_train: 0.8870 loss_val: 1.4748 acc_val: 0.7625 time: 601.3715s\n",
            "Epoch: 1115 loss_train: 0.3501 acc_train: 0.8858 loss_val: 1.4829 acc_val: 0.7647 time: 601.9211s\n",
            "Epoch: 1116 loss_train: 0.3602 acc_train: 0.8804 loss_val: 1.5012 acc_val: 0.7673 time: 602.4668s\n",
            "Epoch: 1117 loss_train: 0.3501 acc_train: 0.8909 loss_val: 1.5000 acc_val: 0.7618 time: 602.9996s\n",
            "Epoch: 1118 loss_train: 0.3592 acc_train: 0.8885 loss_val: 1.4856 acc_val: 0.7455 time: 603.5304s\n",
            "Epoch: 1119 loss_train: 0.3495 acc_train: 0.8863 loss_val: 1.4756 acc_val: 0.7527 time: 604.0626s\n",
            "Epoch: 1120 loss_train: 0.3590 acc_train: 0.8834 loss_val: 1.4773 acc_val: 0.7687 time: 604.5972s\n",
            "Epoch: 1121 loss_train: 0.3396 acc_train: 0.8922 loss_val: 1.4852 acc_val: 0.7716 time: 605.1346s\n",
            "Epoch: 1122 loss_train: 0.3491 acc_train: 0.8931 loss_val: 1.4881 acc_val: 0.7618 time: 605.6847s\n",
            "Epoch: 1123 loss_train: 0.3434 acc_train: 0.8909 loss_val: 1.4825 acc_val: 0.7553 time: 606.2196s\n",
            "Epoch: 1124 loss_train: 0.3507 acc_train: 0.8855 loss_val: 1.4896 acc_val: 0.7578 time: 606.7595s\n",
            "Epoch: 1125 loss_train: 0.3495 acc_train: 0.8880 loss_val: 1.4860 acc_val: 0.7604 time: 607.2867s\n",
            "Epoch: 1126 loss_train: 0.3423 acc_train: 0.8930 loss_val: 1.4825 acc_val: 0.7611 time: 607.8243s\n",
            "Epoch: 1127 loss_train: 0.3481 acc_train: 0.8873 loss_val: 1.4826 acc_val: 0.7662 time: 608.3451s\n",
            "Epoch: 1128 loss_train: 0.3437 acc_train: 0.8938 loss_val: 1.4833 acc_val: 0.7640 time: 608.8848s\n",
            "Epoch: 1129 loss_train: 0.3432 acc_train: 0.8914 loss_val: 1.4804 acc_val: 0.7585 time: 609.4211s\n",
            "Epoch: 1130 loss_train: 0.3416 acc_train: 0.8896 loss_val: 1.4769 acc_val: 0.7538 time: 609.9661s\n",
            "Epoch: 1131 loss_train: 0.3460 acc_train: 0.8878 loss_val: 1.4822 acc_val: 0.7575 time: 610.4867s\n",
            "Epoch: 1132 loss_train: 0.3473 acc_train: 0.8883 loss_val: 1.4921 acc_val: 0.7651 time: 611.0273s\n",
            "Epoch: 1133 loss_train: 0.3400 acc_train: 0.8943 loss_val: 1.4938 acc_val: 0.7647 time: 611.5585s\n",
            "Epoch: 1134 loss_train: 0.3493 acc_train: 0.8898 loss_val: 1.4944 acc_val: 0.7589 time: 612.1022s\n",
            "Epoch: 1135 loss_train: 0.3450 acc_train: 0.8915 loss_val: 1.4876 acc_val: 0.7571 time: 612.6387s\n",
            "Epoch: 1136 loss_train: 0.3372 acc_train: 0.8915 loss_val: 1.4738 acc_val: 0.7582 time: 613.1667s\n",
            "Epoch: 1137 loss_train: 0.3495 acc_train: 0.8888 loss_val: 1.4729 acc_val: 0.7607 time: 613.7085s\n",
            "Epoch: 1138 loss_train: 0.3383 acc_train: 0.8908 loss_val: 1.4805 acc_val: 0.7564 time: 614.2378s\n",
            "Epoch: 1139 loss_train: 0.3376 acc_train: 0.8931 loss_val: 1.4962 acc_val: 0.7604 time: 614.7685s\n",
            "Epoch: 1140 loss_train: 0.3494 acc_train: 0.8926 loss_val: 1.4912 acc_val: 0.7600 time: 615.2892s\n",
            "Epoch: 1141 loss_train: 0.3373 acc_train: 0.8947 loss_val: 1.4837 acc_val: 0.7509 time: 615.8150s\n",
            "Epoch: 1142 loss_train: 0.3376 acc_train: 0.8922 loss_val: 1.4885 acc_val: 0.7618 time: 616.3354s\n",
            "Epoch: 1143 loss_train: 0.3396 acc_train: 0.8931 loss_val: 1.4874 acc_val: 0.7644 time: 616.8564s\n",
            "Epoch: 1144 loss_train: 0.3473 acc_train: 0.8915 loss_val: 1.4836 acc_val: 0.7564 time: 617.3845s\n",
            "Epoch: 1145 loss_train: 0.3427 acc_train: 0.8899 loss_val: 1.4892 acc_val: 0.7542 time: 617.9248s\n",
            "Epoch: 1146 loss_train: 0.3459 acc_train: 0.8881 loss_val: 1.4918 acc_val: 0.7589 time: 618.4388s\n",
            "Epoch: 1147 loss_train: 0.3520 acc_train: 0.8898 loss_val: 1.4807 acc_val: 0.7553 time: 618.9741s\n",
            "Epoch: 1148 loss_train: 0.3420 acc_train: 0.8906 loss_val: 1.4784 acc_val: 0.7556 time: 619.5039s\n",
            "Epoch: 1149 loss_train: 0.3357 acc_train: 0.8920 loss_val: 1.4907 acc_val: 0.7567 time: 620.0375s\n",
            "Epoch: 1150 loss_train: 0.3494 acc_train: 0.8934 loss_val: 1.4915 acc_val: 0.7527 time: 620.5884s\n",
            "Epoch: 1151 loss_train: 0.3433 acc_train: 0.8908 loss_val: 1.4909 acc_val: 0.7560 time: 621.1235s\n",
            "Epoch: 1152 loss_train: 0.3430 acc_train: 0.8903 loss_val: 1.4854 acc_val: 0.7582 time: 621.6573s\n",
            "Epoch: 1153 loss_train: 0.3350 acc_train: 0.8923 loss_val: 1.4776 acc_val: 0.7527 time: 622.1790s\n",
            "Epoch: 1154 loss_train: 0.3465 acc_train: 0.8894 loss_val: 1.4823 acc_val: 0.7582 time: 622.7067s\n",
            "Epoch: 1155 loss_train: 0.3391 acc_train: 0.8897 loss_val: 1.4932 acc_val: 0.7633 time: 623.2417s\n",
            "Epoch: 1156 loss_train: 0.3335 acc_train: 0.8988 loss_val: 1.4854 acc_val: 0.7545 time: 623.7682s\n",
            "Epoch: 1157 loss_train: 0.3486 acc_train: 0.8871 loss_val: 1.4800 acc_val: 0.7542 time: 624.3103s\n",
            "Epoch: 1158 loss_train: 0.3425 acc_train: 0.8875 loss_val: 1.4808 acc_val: 0.7556 time: 624.8502s\n",
            "Epoch: 1159 loss_train: 0.3423 acc_train: 0.8886 loss_val: 1.4829 acc_val: 0.7513 time: 625.3822s\n",
            "Epoch: 1160 loss_train: 0.3502 acc_train: 0.8875 loss_val: 1.4964 acc_val: 0.7527 time: 625.9211s\n",
            "Epoch: 1161 loss_train: 0.3484 acc_train: 0.8923 loss_val: 1.4995 acc_val: 0.7571 time: 626.4431s\n",
            "Epoch: 1162 loss_train: 0.3562 acc_train: 0.8960 loss_val: 1.4811 acc_val: 0.7498 time: 626.9753s\n",
            "Epoch: 1163 loss_train: 0.3617 acc_train: 0.8793 loss_val: 1.4884 acc_val: 0.7633 time: 627.4998s\n",
            "Epoch: 1164 loss_train: 0.3411 acc_train: 0.8935 loss_val: 1.4883 acc_val: 0.7596 time: 628.0376s\n",
            "Epoch: 1165 loss_train: 0.3493 acc_train: 0.8918 loss_val: 1.4874 acc_val: 0.7535 time: 628.5732s\n",
            "Epoch: 1166 loss_train: 0.3402 acc_train: 0.8921 loss_val: 1.4853 acc_val: 0.7465 time: 629.1172s\n",
            "Epoch: 1167 loss_train: 0.3449 acc_train: 0.8855 loss_val: 1.4888 acc_val: 0.7560 time: 629.6400s\n",
            "Epoch: 1168 loss_train: 0.3501 acc_train: 0.8905 loss_val: 1.4949 acc_val: 0.7625 time: 630.1660s\n",
            "Epoch: 1169 loss_train: 0.3533 acc_train: 0.8915 loss_val: 1.4844 acc_val: 0.7582 time: 630.7050s\n",
            "Epoch: 1170 loss_train: 0.3431 acc_train: 0.8928 loss_val: 1.4849 acc_val: 0.7505 time: 631.2454s\n",
            "Epoch: 1171 loss_train: 0.3486 acc_train: 0.8885 loss_val: 1.4872 acc_val: 0.7502 time: 631.7948s\n",
            "Epoch: 1172 loss_train: 0.3479 acc_train: 0.8929 loss_val: 1.4835 acc_val: 0.7505 time: 632.3322s\n",
            "Epoch: 1173 loss_train: 0.3459 acc_train: 0.8869 loss_val: 1.4926 acc_val: 0.7542 time: 632.8622s\n",
            "Epoch: 1174 loss_train: 0.3331 acc_train: 0.8960 loss_val: 1.4898 acc_val: 0.7629 time: 633.3873s\n",
            "Epoch: 1175 loss_train: 0.3516 acc_train: 0.8930 loss_val: 1.4721 acc_val: 0.7516 time: 633.9161s\n",
            "Epoch: 1176 loss_train: 0.3464 acc_train: 0.8873 loss_val: 1.4731 acc_val: 0.7509 time: 634.4446s\n",
            "Epoch: 1177 loss_train: 0.3420 acc_train: 0.8912 loss_val: 1.4917 acc_val: 0.7538 time: 634.9708s\n",
            "Epoch: 1178 loss_train: 0.3519 acc_train: 0.8896 loss_val: 1.4897 acc_val: 0.7502 time: 635.4948s\n",
            "Epoch: 1179 loss_train: 0.3479 acc_train: 0.8874 loss_val: 1.4952 acc_val: 0.7556 time: 636.0249s\n",
            "Epoch: 1180 loss_train: 0.3360 acc_train: 0.8990 loss_val: 1.4884 acc_val: 0.7662 time: 636.5507s\n",
            "Epoch: 1181 loss_train: 0.3488 acc_train: 0.8921 loss_val: 1.4720 acc_val: 0.7578 time: 637.0740s\n",
            "Epoch: 1182 loss_train: 0.3356 acc_train: 0.8927 loss_val: 1.4673 acc_val: 0.7553 time: 637.6006s\n",
            "Epoch: 1183 loss_train: 0.3394 acc_train: 0.8883 loss_val: 1.4728 acc_val: 0.7564 time: 638.1306s\n",
            "Epoch: 1184 loss_train: 0.3430 acc_train: 0.8897 loss_val: 1.4812 acc_val: 0.7524 time: 638.6613s\n",
            "Epoch: 1185 loss_train: 0.3416 acc_train: 0.8904 loss_val: 1.4795 acc_val: 0.7615 time: 639.1914s\n",
            "Epoch: 1186 loss_train: 0.3460 acc_train: 0.8913 loss_val: 1.4788 acc_val: 0.7611 time: 639.7246s\n",
            "Epoch: 1187 loss_train: 0.3445 acc_train: 0.8916 loss_val: 1.4893 acc_val: 0.7655 time: 640.2525s\n",
            "Epoch: 1188 loss_train: 0.3427 acc_train: 0.8934 loss_val: 1.4813 acc_val: 0.7527 time: 640.7907s\n",
            "Epoch: 1189 loss_train: 0.3453 acc_train: 0.8876 loss_val: 1.4743 acc_val: 0.7589 time: 641.3153s\n",
            "Epoch: 1190 loss_train: 0.3311 acc_train: 0.8929 loss_val: 1.4771 acc_val: 0.7604 time: 641.8530s\n",
            "Epoch: 1191 loss_train: 0.3449 acc_train: 0.8932 loss_val: 1.4719 acc_val: 0.7531 time: 642.3862s\n",
            "Epoch: 1192 loss_train: 0.3460 acc_train: 0.8871 loss_val: 1.4771 acc_val: 0.7615 time: 642.9324s\n",
            "Epoch: 1193 loss_train: 0.3371 acc_train: 0.8904 loss_val: 1.4796 acc_val: 0.7629 time: 643.4585s\n",
            "Epoch: 1194 loss_train: 0.3472 acc_train: 0.8924 loss_val: 1.4746 acc_val: 0.7531 time: 643.9991s\n",
            "Epoch: 1195 loss_train: 0.3375 acc_train: 0.8912 loss_val: 1.4781 acc_val: 0.7556 time: 644.5244s\n",
            "Epoch: 1196 loss_train: 0.3316 acc_train: 0.8930 loss_val: 1.4814 acc_val: 0.7596 time: 645.0534s\n",
            "Epoch: 1197 loss_train: 0.3345 acc_train: 0.8969 loss_val: 1.4717 acc_val: 0.7589 time: 645.5848s\n",
            "Epoch: 1198 loss_train: 0.3364 acc_train: 0.8956 loss_val: 1.4661 acc_val: 0.7505 time: 646.1077s\n",
            "Epoch: 1199 loss_train: 0.3441 acc_train: 0.8894 loss_val: 1.4791 acc_val: 0.7604 time: 646.6318s\n",
            "Epoch: 1200 loss_train: 0.3412 acc_train: 0.8955 loss_val: 1.4768 acc_val: 0.7545 time: 647.1650s\n",
            "Epoch: 1201 loss_train: 0.3343 acc_train: 0.8948 loss_val: 1.4742 acc_val: 0.7545 time: 647.6942s\n",
            "Epoch: 1202 loss_train: 0.3421 acc_train: 0.8907 loss_val: 1.4771 acc_val: 0.7633 time: 648.2098s\n",
            "Epoch: 1203 loss_train: 0.3313 acc_train: 0.8959 loss_val: 1.4770 acc_val: 0.7625 time: 648.7363s\n",
            "Epoch: 1204 loss_train: 0.3294 acc_train: 0.8962 loss_val: 1.4739 acc_val: 0.7436 time: 649.2536s\n",
            "Epoch: 1205 loss_train: 0.3359 acc_train: 0.8912 loss_val: 1.4820 acc_val: 0.7571 time: 649.7798s\n",
            "Epoch: 1206 loss_train: 0.3480 acc_train: 0.8925 loss_val: 1.4769 acc_val: 0.7567 time: 650.3099s\n",
            "Epoch: 1207 loss_train: 0.3385 acc_train: 0.8951 loss_val: 1.4712 acc_val: 0.7567 time: 650.8527s\n",
            "Epoch: 1208 loss_train: 0.3388 acc_train: 0.8888 loss_val: 1.4718 acc_val: 0.7633 time: 651.3796s\n",
            "Epoch: 1209 loss_train: 0.3340 acc_train: 0.8921 loss_val: 1.4719 acc_val: 0.7607 time: 651.9176s\n",
            "Epoch: 1210 loss_train: 0.3384 acc_train: 0.8899 loss_val: 1.4775 acc_val: 0.7505 time: 652.4423s\n",
            "Epoch: 1211 loss_train: 0.3304 acc_train: 0.8916 loss_val: 1.4816 acc_val: 0.7545 time: 652.9677s\n",
            "Epoch: 1212 loss_train: 0.3380 acc_train: 0.8920 loss_val: 1.4818 acc_val: 0.7607 time: 653.4856s\n",
            "Epoch: 1213 loss_train: 0.3375 acc_train: 0.8926 loss_val: 1.4750 acc_val: 0.7535 time: 654.0349s\n",
            "Epoch: 1214 loss_train: 0.3440 acc_train: 0.8870 loss_val: 1.4769 acc_val: 0.7607 time: 654.5722s\n",
            "Epoch: 1215 loss_train: 0.3236 acc_train: 0.8964 loss_val: 1.4759 acc_val: 0.7607 time: 655.1052s\n",
            "Epoch: 1216 loss_train: 0.3419 acc_train: 0.8935 loss_val: 1.4738 acc_val: 0.7545 time: 655.6260s\n",
            "Epoch: 1217 loss_train: 0.3403 acc_train: 0.8931 loss_val: 1.4729 acc_val: 0.7487 time: 656.1663s\n",
            "Epoch: 1218 loss_train: 0.3434 acc_train: 0.8899 loss_val: 1.4748 acc_val: 0.7560 time: 656.7053s\n",
            "Epoch: 1219 loss_train: 0.3359 acc_train: 0.8918 loss_val: 1.4782 acc_val: 0.7636 time: 657.2257s\n",
            "Epoch: 1220 loss_train: 0.3416 acc_train: 0.8940 loss_val: 1.4708 acc_val: 0.7578 time: 657.7680s\n",
            "Epoch: 1221 loss_train: 0.3300 acc_train: 0.8942 loss_val: 1.4720 acc_val: 0.7542 time: 658.3009s\n",
            "Epoch: 1222 loss_train: 0.3434 acc_train: 0.8907 loss_val: 1.4776 acc_val: 0.7524 time: 658.8369s\n",
            "Epoch: 1223 loss_train: 0.3340 acc_train: 0.8927 loss_val: 1.4804 acc_val: 0.7578 time: 659.3590s\n",
            "Epoch: 1224 loss_train: 0.3430 acc_train: 0.8925 loss_val: 1.4787 acc_val: 0.7658 time: 659.8999s\n",
            "Epoch: 1225 loss_train: 0.3375 acc_train: 0.8927 loss_val: 1.4675 acc_val: 0.7578 time: 660.4261s\n",
            "Epoch: 1226 loss_train: 0.3365 acc_train: 0.8939 loss_val: 1.4629 acc_val: 0.7513 time: 660.9732s\n",
            "Epoch: 1227 loss_train: 0.3437 acc_train: 0.8831 loss_val: 1.4772 acc_val: 0.7622 time: 661.4853s\n",
            "Epoch: 1228 loss_train: 0.3275 acc_train: 0.8973 loss_val: 1.4811 acc_val: 0.7615 time: 662.0374s\n",
            "Epoch: 1229 loss_train: 0.3342 acc_train: 0.8996 loss_val: 1.4716 acc_val: 0.7531 time: 662.5866s\n",
            "Epoch: 1230 loss_train: 0.3451 acc_train: 0.8891 loss_val: 1.4756 acc_val: 0.7607 time: 663.1224s\n",
            "Epoch: 1231 loss_train: 0.3514 acc_train: 0.8844 loss_val: 1.4763 acc_val: 0.7611 time: 663.6601s\n",
            "Epoch: 1232 loss_train: 0.3359 acc_train: 0.8947 loss_val: 1.4693 acc_val: 0.7585 time: 664.1929s\n",
            "Epoch: 1233 loss_train: 0.3521 acc_train: 0.8879 loss_val: 1.4669 acc_val: 0.7553 time: 664.7173s\n",
            "Epoch: 1234 loss_train: 0.3410 acc_train: 0.8905 loss_val: 1.4677 acc_val: 0.7556 time: 665.2436s\n",
            "Epoch: 1235 loss_train: 0.3378 acc_train: 0.8920 loss_val: 1.4711 acc_val: 0.7560 time: 665.7850s\n",
            "Epoch: 1236 loss_train: 0.3504 acc_train: 0.8858 loss_val: 1.4885 acc_val: 0.7669 time: 666.3233s\n",
            "Epoch: 1237 loss_train: 0.3502 acc_train: 0.8910 loss_val: 1.4771 acc_val: 0.7618 time: 666.8703s\n",
            "Epoch: 1238 loss_train: 0.3489 acc_train: 0.8937 loss_val: 1.4568 acc_val: 0.7309 time: 667.3974s\n",
            "Epoch: 1239 loss_train: 0.3674 acc_train: 0.8781 loss_val: 1.4753 acc_val: 0.7571 time: 667.9440s\n",
            "Epoch: 1240 loss_train: 0.3585 acc_train: 0.8892 loss_val: 1.4826 acc_val: 0.7462 time: 668.4724s\n",
            "Epoch: 1241 loss_train: 0.3566 acc_train: 0.8863 loss_val: 1.4746 acc_val: 0.7585 time: 669.0106s\n",
            "Epoch: 1242 loss_train: 0.3371 acc_train: 0.8938 loss_val: 1.4671 acc_val: 0.7553 time: 669.5385s\n",
            "Epoch: 1243 loss_train: 0.3652 acc_train: 0.8885 loss_val: 1.4575 acc_val: 0.7229 time: 670.0828s\n",
            "Epoch: 1244 loss_train: 0.3963 acc_train: 0.8625 loss_val: 1.4901 acc_val: 0.7640 time: 670.6079s\n",
            "Epoch: 1245 loss_train: 0.3589 acc_train: 0.8882 loss_val: 1.5048 acc_val: 0.7705 time: 671.1379s\n",
            "Epoch: 1246 loss_train: 0.3821 acc_train: 0.8913 loss_val: 1.4676 acc_val: 0.7324 time: 671.6714s\n",
            "Epoch: 1247 loss_train: 0.3778 acc_train: 0.8781 loss_val: 1.4665 acc_val: 0.7313 time: 672.2145s\n",
            "Epoch: 1248 loss_train: 0.3669 acc_train: 0.8777 loss_val: 1.4718 acc_val: 0.7487 time: 672.7503s\n",
            "Epoch: 1249 loss_train: 0.3911 acc_train: 0.8831 loss_val: 1.4791 acc_val: 0.7233 time: 673.2692s\n",
            "Epoch: 1250 loss_train: 0.4245 acc_train: 0.8575 loss_val: 1.4966 acc_val: 0.7469 time: 673.8017s\n",
            "Epoch: 1251 loss_train: 0.4006 acc_train: 0.8727 loss_val: 1.4924 acc_val: 0.7589 time: 674.3284s\n",
            "Epoch: 1252 loss_train: 0.3739 acc_train: 0.8868 loss_val: 1.4803 acc_val: 0.7396 time: 674.8740s\n",
            "Epoch: 1253 loss_train: 0.4004 acc_train: 0.8702 loss_val: 1.4737 acc_val: 0.7480 time: 675.3915s\n",
            "Epoch: 1254 loss_train: 0.3861 acc_train: 0.8683 loss_val: 1.4744 acc_val: 0.7680 time: 675.9288s\n",
            "Epoch: 1255 loss_train: 0.3458 acc_train: 0.8936 loss_val: 1.4863 acc_val: 0.7669 time: 676.4547s\n",
            "Epoch: 1256 loss_train: 0.3615 acc_train: 0.8942 loss_val: 1.4572 acc_val: 0.7582 time: 677.0001s\n",
            "Epoch: 1257 loss_train: 0.3688 acc_train: 0.8797 loss_val: 1.4385 acc_val: 0.7600 time: 677.5229s\n",
            "Epoch: 1258 loss_train: 0.3731 acc_train: 0.8809 loss_val: 1.4325 acc_val: 0.7647 time: 678.0609s\n",
            "Epoch: 1259 loss_train: 0.3459 acc_train: 0.8875 loss_val: 1.4458 acc_val: 0.7625 time: 678.5848s\n",
            "Epoch: 1260 loss_train: 0.3423 acc_train: 0.8889 loss_val: 1.4898 acc_val: 0.7680 time: 679.1288s\n",
            "Epoch: 1261 loss_train: 0.3624 acc_train: 0.8874 loss_val: 1.4952 acc_val: 0.7658 time: 679.6541s\n",
            "Epoch: 1262 loss_train: 0.3538 acc_train: 0.8935 loss_val: 1.4626 acc_val: 0.7538 time: 680.1784s\n",
            "Epoch: 1263 loss_train: 0.3583 acc_train: 0.8797 loss_val: 1.4524 acc_val: 0.7676 time: 680.7068s\n",
            "Epoch: 1264 loss_train: 0.3583 acc_train: 0.8887 loss_val: 1.4426 acc_val: 0.7625 time: 681.2329s\n",
            "Epoch: 1265 loss_train: 0.3375 acc_train: 0.8914 loss_val: 1.4454 acc_val: 0.7615 time: 681.7824s\n",
            "Epoch: 1266 loss_train: 0.3448 acc_train: 0.8879 loss_val: 1.4555 acc_val: 0.7702 time: 682.3079s\n",
            "Epoch: 1267 loss_train: 0.3513 acc_train: 0.8885 loss_val: 1.4560 acc_val: 0.7731 time: 682.8503s\n",
            "Epoch: 1268 loss_train: 0.3540 acc_train: 0.8932 loss_val: 1.4463 acc_val: 0.7655 time: 683.3713s\n",
            "Epoch: 1269 loss_train: 0.3415 acc_train: 0.8913 loss_val: 1.4410 acc_val: 0.7607 time: 683.9021s\n",
            "Epoch: 1270 loss_train: 0.3441 acc_train: 0.8903 loss_val: 1.4571 acc_val: 0.7655 time: 684.4247s\n",
            "Epoch: 1271 loss_train: 0.3345 acc_train: 0.8921 loss_val: 1.4716 acc_val: 0.7684 time: 684.9705s\n",
            "Epoch: 1272 loss_train: 0.3421 acc_train: 0.8988 loss_val: 1.4630 acc_val: 0.7589 time: 685.4984s\n",
            "Epoch: 1273 loss_train: 0.3547 acc_train: 0.8870 loss_val: 1.4535 acc_val: 0.7611 time: 686.0308s\n",
            "Epoch: 1274 loss_train: 0.3447 acc_train: 0.8866 loss_val: 1.4572 acc_val: 0.7607 time: 686.5513s\n",
            "Epoch: 1275 loss_train: 0.3445 acc_train: 0.8935 loss_val: 1.4519 acc_val: 0.7578 time: 687.0819s\n",
            "Epoch: 1276 loss_train: 0.3380 acc_train: 0.8872 loss_val: 1.4485 acc_val: 0.7615 time: 687.6114s\n",
            "Epoch: 1277 loss_train: 0.3403 acc_train: 0.8862 loss_val: 1.4556 acc_val: 0.7698 time: 688.1419s\n",
            "Epoch: 1278 loss_train: 0.3530 acc_train: 0.8930 loss_val: 1.4457 acc_val: 0.7596 time: 688.6730s\n",
            "Epoch: 1279 loss_train: 0.3373 acc_train: 0.8900 loss_val: 1.4485 acc_val: 0.7444 time: 689.2016s\n",
            "Epoch: 1280 loss_train: 0.3439 acc_train: 0.8883 loss_val: 1.4565 acc_val: 0.7629 time: 689.7343s\n",
            "Epoch: 1281 loss_train: 0.3361 acc_train: 0.8942 loss_val: 1.4651 acc_val: 0.7615 time: 690.2528s\n",
            "Epoch: 1282 loss_train: 0.3305 acc_train: 0.8929 loss_val: 1.4676 acc_val: 0.7607 time: 690.7824s\n",
            "Epoch: 1283 loss_train: 0.3382 acc_train: 0.8932 loss_val: 1.4602 acc_val: 0.7647 time: 691.3059s\n",
            "Epoch: 1284 loss_train: 0.3301 acc_train: 0.8969 loss_val: 1.4530 acc_val: 0.7684 time: 691.8435s\n",
            "Epoch: 1285 loss_train: 0.3310 acc_train: 0.8918 loss_val: 1.4529 acc_val: 0.7662 time: 692.4015s\n",
            "Epoch: 1286 loss_train: 0.3312 acc_train: 0.8922 loss_val: 1.4518 acc_val: 0.7625 time: 692.9346s\n",
            "Epoch: 1287 loss_train: 0.3426 acc_train: 0.8907 loss_val: 1.4581 acc_val: 0.7578 time: 693.4665s\n",
            "Epoch: 1288 loss_train: 0.3370 acc_train: 0.8928 loss_val: 1.4601 acc_val: 0.7589 time: 693.9968s\n",
            "Epoch: 1289 loss_train: 0.3436 acc_train: 0.8902 loss_val: 1.4554 acc_val: 0.7593 time: 694.5307s\n",
            "Epoch: 1290 loss_train: 0.3339 acc_train: 0.8922 loss_val: 1.4544 acc_val: 0.7647 time: 695.0542s\n",
            "Epoch: 1291 loss_train: 0.3302 acc_train: 0.8995 loss_val: 1.4511 acc_val: 0.7615 time: 695.5858s\n",
            "Epoch: 1292 loss_train: 0.3394 acc_train: 0.8904 loss_val: 1.4513 acc_val: 0.7560 time: 696.1269s\n",
            "Epoch: 1293 loss_train: 0.3354 acc_train: 0.8907 loss_val: 1.4648 acc_val: 0.7589 time: 696.6667s\n",
            "Epoch: 1294 loss_train: 0.3397 acc_train: 0.8962 loss_val: 1.4589 acc_val: 0.7582 time: 697.2052s\n",
            "Epoch: 1295 loss_train: 0.3367 acc_train: 0.8949 loss_val: 1.4539 acc_val: 0.7611 time: 697.7339s\n",
            "Epoch: 1296 loss_train: 0.3313 acc_train: 0.8943 loss_val: 1.4554 acc_val: 0.7622 time: 698.2653s\n",
            "Epoch: 1297 loss_train: 0.3416 acc_train: 0.8961 loss_val: 1.4516 acc_val: 0.7575 time: 698.8088s\n",
            "Epoch: 1298 loss_train: 0.3245 acc_train: 0.8960 loss_val: 1.4543 acc_val: 0.7545 time: 699.3398s\n",
            "Epoch: 1299 loss_train: 0.3303 acc_train: 0.8909 loss_val: 1.4636 acc_val: 0.7564 time: 699.8647s\n",
            "Epoch: 1300 loss_train: 0.3446 acc_train: 0.8885 loss_val: 1.4634 acc_val: 0.7589 time: 700.3879s\n",
            "Epoch: 1301 loss_train: 0.3369 acc_train: 0.8980 loss_val: 1.4512 acc_val: 0.7436 time: 700.9198s\n",
            "Epoch: 1302 loss_train: 0.3372 acc_train: 0.8852 loss_val: 1.4551 acc_val: 0.7644 time: 701.4440s\n",
            "Epoch: 1303 loss_train: 0.3352 acc_train: 0.8925 loss_val: 1.4630 acc_val: 0.7698 time: 701.9747s\n",
            "Epoch: 1304 loss_train: 0.3488 acc_train: 0.8961 loss_val: 1.4539 acc_val: 0.7473 time: 702.5108s\n",
            "Epoch: 1305 loss_train: 0.3331 acc_train: 0.8924 loss_val: 1.4562 acc_val: 0.7447 time: 703.0489s\n",
            "Epoch: 1306 loss_train: 0.3468 acc_train: 0.8850 loss_val: 1.4612 acc_val: 0.7567 time: 703.5835s\n",
            "Epoch: 1307 loss_train: 0.3395 acc_train: 0.8949 loss_val: 1.4573 acc_val: 0.7582 time: 704.1217s\n",
            "Epoch: 1308 loss_train: 0.3306 acc_train: 0.8962 loss_val: 1.4568 acc_val: 0.7491 time: 704.6587s\n",
            "Epoch: 1309 loss_train: 0.3303 acc_train: 0.8942 loss_val: 1.4650 acc_val: 0.7596 time: 705.1911s\n",
            "Epoch: 1310 loss_train: 0.3319 acc_train: 0.8975 loss_val: 1.4619 acc_val: 0.7585 time: 705.7208s\n",
            "Epoch: 1311 loss_train: 0.3462 acc_train: 0.8916 loss_val: 1.4558 acc_val: 0.7415 time: 706.2548s\n",
            "Epoch: 1312 loss_train: 0.3482 acc_train: 0.8848 loss_val: 1.4663 acc_val: 0.7585 time: 706.7900s\n",
            "Epoch: 1313 loss_train: 0.3342 acc_train: 0.8945 loss_val: 1.4630 acc_val: 0.7662 time: 707.3196s\n",
            "Epoch: 1314 loss_train: 0.3408 acc_train: 0.8973 loss_val: 1.4454 acc_val: 0.7425 time: 707.8554s\n",
            "Epoch: 1315 loss_train: 0.3329 acc_train: 0.8892 loss_val: 1.4461 acc_val: 0.7447 time: 708.3922s\n",
            "Epoch: 1316 loss_train: 0.3301 acc_train: 0.8941 loss_val: 1.4648 acc_val: 0.7625 time: 708.9330s\n",
            "Epoch: 1317 loss_train: 0.3485 acc_train: 0.8941 loss_val: 1.4618 acc_val: 0.7465 time: 709.4794s\n",
            "Epoch: 1318 loss_train: 0.3338 acc_train: 0.8938 loss_val: 1.4601 acc_val: 0.7502 time: 710.0140s\n",
            "Epoch: 1319 loss_train: 0.3395 acc_train: 0.8901 loss_val: 1.4654 acc_val: 0.7615 time: 710.5651s\n",
            "Epoch: 1320 loss_train: 0.3396 acc_train: 0.8959 loss_val: 1.4533 acc_val: 0.7473 time: 711.0947s\n",
            "Epoch: 1321 loss_train: 0.3382 acc_train: 0.8878 loss_val: 1.4489 acc_val: 0.7484 time: 711.6318s\n",
            "Epoch: 1322 loss_train: 0.3346 acc_train: 0.8898 loss_val: 1.4577 acc_val: 0.7680 time: 712.1978s\n",
            "Epoch: 1323 loss_train: 0.3362 acc_train: 0.8981 loss_val: 1.4538 acc_val: 0.7615 time: 712.7646s\n",
            "Epoch: 1324 loss_train: 0.3283 acc_train: 0.8929 loss_val: 1.4493 acc_val: 0.7404 time: 713.3064s\n",
            "Epoch: 1325 loss_train: 0.3491 acc_train: 0.8878 loss_val: 1.4505 acc_val: 0.7585 time: 713.8585s\n",
            "Epoch: 1326 loss_train: 0.3430 acc_train: 0.8911 loss_val: 1.4590 acc_val: 0.7655 time: 714.4099s\n",
            "Epoch: 1327 loss_train: 0.3448 acc_train: 0.8922 loss_val: 1.4547 acc_val: 0.7451 time: 714.9376s\n",
            "Epoch: 1328 loss_train: 0.3451 acc_train: 0.8860 loss_val: 1.4620 acc_val: 0.7607 time: 715.4497s\n",
            "Epoch: 1329 loss_train: 0.3313 acc_train: 0.8940 loss_val: 1.4650 acc_val: 0.7604 time: 715.9905s\n",
            "Epoch: 1330 loss_train: 0.3545 acc_train: 0.8927 loss_val: 1.4502 acc_val: 0.7316 time: 716.5285s\n",
            "Epoch: 1331 loss_train: 0.3548 acc_train: 0.8798 loss_val: 1.4528 acc_val: 0.7520 time: 717.0638s\n",
            "Epoch: 1332 loss_train: 0.3416 acc_train: 0.8884 loss_val: 1.4618 acc_val: 0.7749 time: 717.5998s\n",
            "Epoch: 1333 loss_train: 0.3425 acc_train: 0.8994 loss_val: 1.4513 acc_val: 0.7629 time: 718.1366s\n",
            "Epoch: 1334 loss_train: 0.3382 acc_train: 0.8942 loss_val: 1.4420 acc_val: 0.7367 time: 718.6660s\n",
            "Epoch: 1335 loss_train: 0.3388 acc_train: 0.8872 loss_val: 1.4540 acc_val: 0.7567 time: 719.2165s\n",
            "Epoch: 1336 loss_train: 0.3463 acc_train: 0.8898 loss_val: 1.4615 acc_val: 0.7607 time: 719.7538s\n",
            "Epoch: 1337 loss_train: 0.3356 acc_train: 0.8950 loss_val: 1.4622 acc_val: 0.7582 time: 720.2915s\n",
            "Epoch: 1338 loss_train: 0.3377 acc_train: 0.8931 loss_val: 1.4634 acc_val: 0.7724 time: 720.8359s\n",
            "Epoch: 1339 loss_train: 0.3365 acc_train: 0.8963 loss_val: 1.4548 acc_val: 0.7676 time: 721.3657s\n",
            "Epoch: 1340 loss_train: 0.3370 acc_train: 0.8945 loss_val: 1.4434 acc_val: 0.7560 time: 721.9239s\n",
            "Epoch: 1341 loss_train: 0.3316 acc_train: 0.8937 loss_val: 1.4418 acc_val: 0.7462 time: 722.4781s\n",
            "Epoch: 1342 loss_train: 0.3285 acc_train: 0.8924 loss_val: 1.4502 acc_val: 0.7611 time: 723.0262s\n",
            "Epoch: 1343 loss_train: 0.3340 acc_train: 0.8911 loss_val: 1.4596 acc_val: 0.7665 time: 723.5538s\n",
            "Epoch: 1344 loss_train: 0.3419 acc_train: 0.8954 loss_val: 1.4524 acc_val: 0.7473 time: 724.1066s\n",
            "Epoch: 1345 loss_train: 0.3257 acc_train: 0.8939 loss_val: 1.4507 acc_val: 0.7524 time: 724.6355s\n",
            "Epoch: 1346 loss_train: 0.3332 acc_train: 0.8924 loss_val: 1.4656 acc_val: 0.7655 time: 725.1759s\n",
            "Epoch: 1347 loss_train: 0.3484 acc_train: 0.8977 loss_val: 1.4570 acc_val: 0.7451 time: 725.7108s\n",
            "Epoch: 1348 loss_train: 0.3379 acc_train: 0.8903 loss_val: 1.4518 acc_val: 0.7400 time: 726.2607s\n",
            "Epoch: 1349 loss_train: 0.3382 acc_train: 0.8894 loss_val: 1.4534 acc_val: 0.7629 time: 726.7965s\n",
            "Epoch: 1350 loss_train: 0.3455 acc_train: 0.8949 loss_val: 1.4475 acc_val: 0.7553 time: 727.3288s\n",
            "Epoch: 1351 loss_train: 0.3472 acc_train: 0.8900 loss_val: 1.4402 acc_val: 0.7367 time: 727.8924s\n",
            "Epoch: 1352 loss_train: 0.3506 acc_train: 0.8863 loss_val: 1.4522 acc_val: 0.7582 time: 728.4276s\n",
            "Epoch: 1353 loss_train: 0.3286 acc_train: 0.8953 loss_val: 1.4685 acc_val: 0.7629 time: 729.0060s\n",
            "Epoch: 1354 loss_train: 0.3513 acc_train: 0.8984 loss_val: 1.4571 acc_val: 0.7378 time: 729.5487s\n",
            "Epoch: 1355 loss_train: 0.3432 acc_train: 0.8849 loss_val: 1.4471 acc_val: 0.7349 time: 730.1147s\n",
            "Epoch: 1356 loss_train: 0.3415 acc_train: 0.8877 loss_val: 1.4548 acc_val: 0.7687 time: 730.6481s\n",
            "Epoch: 1357 loss_train: 0.3442 acc_train: 0.8949 loss_val: 1.4476 acc_val: 0.7658 time: 731.1870s\n",
            "Epoch: 1358 loss_train: 0.3357 acc_train: 0.8932 loss_val: 1.4380 acc_val: 0.7404 time: 731.7189s\n",
            "Epoch: 1359 loss_train: 0.3355 acc_train: 0.8923 loss_val: 1.4485 acc_val: 0.7498 time: 732.2607s\n",
            "Epoch: 1360 loss_train: 0.3351 acc_train: 0.8937 loss_val: 1.4628 acc_val: 0.7535 time: 732.8069s\n",
            "Epoch: 1361 loss_train: 0.3358 acc_train: 0.8954 loss_val: 1.4588 acc_val: 0.7505 time: 733.3296s\n",
            "Epoch: 1362 loss_train: 0.3332 acc_train: 0.8940 loss_val: 1.4473 acc_val: 0.7567 time: 733.8600s\n",
            "Epoch: 1363 loss_train: 0.3376 acc_train: 0.8945 loss_val: 1.4403 acc_val: 0.7545 time: 734.3939s\n",
            "Epoch: 1364 loss_train: 0.3383 acc_train: 0.8951 loss_val: 1.4396 acc_val: 0.7527 time: 734.9289s\n",
            "Epoch: 1365 loss_train: 0.3361 acc_train: 0.8963 loss_val: 1.4485 acc_val: 0.7495 time: 735.4464s\n",
            "Epoch: 1366 loss_train: 0.3338 acc_train: 0.8927 loss_val: 1.4589 acc_val: 0.7538 time: 735.9869s\n",
            "Epoch: 1367 loss_train: 0.3383 acc_train: 0.8954 loss_val: 1.4556 acc_val: 0.7513 time: 736.5089s\n",
            "Epoch: 1368 loss_train: 0.3313 acc_train: 0.8956 loss_val: 1.4458 acc_val: 0.7520 time: 737.0485s\n",
            "Epoch: 1369 loss_train: 0.3301 acc_train: 0.8945 loss_val: 1.4430 acc_val: 0.7505 time: 737.5688s\n",
            "Epoch: 1370 loss_train: 0.3359 acc_train: 0.8899 loss_val: 1.4466 acc_val: 0.7665 time: 738.0955s\n",
            "Epoch: 1371 loss_train: 0.3271 acc_train: 0.8976 loss_val: 1.4475 acc_val: 0.7596 time: 738.6058s\n",
            "Epoch: 1372 loss_train: 0.3352 acc_train: 0.8947 loss_val: 1.4486 acc_val: 0.7527 time: 739.1471s\n",
            "Epoch: 1373 loss_train: 0.3303 acc_train: 0.8946 loss_val: 1.4504 acc_val: 0.7480 time: 739.6680s\n",
            "Epoch: 1374 loss_train: 0.3348 acc_train: 0.8896 loss_val: 1.4500 acc_val: 0.7578 time: 740.1972s\n",
            "Epoch: 1375 loss_train: 0.3373 acc_train: 0.8924 loss_val: 1.4520 acc_val: 0.7585 time: 740.7272s\n",
            "Epoch: 1376 loss_train: 0.3394 acc_train: 0.8983 loss_val: 1.4480 acc_val: 0.7415 time: 741.2546s\n",
            "Epoch: 1377 loss_train: 0.3212 acc_train: 0.8956 loss_val: 1.4514 acc_val: 0.7535 time: 741.7999s\n",
            "Epoch: 1378 loss_train: 0.3333 acc_train: 0.8969 loss_val: 1.4473 acc_val: 0.7527 time: 742.3419s\n",
            "Epoch: 1379 loss_train: 0.3234 acc_train: 0.8991 loss_val: 1.4444 acc_val: 0.7505 time: 742.8856s\n",
            "Epoch: 1380 loss_train: 0.3226 acc_train: 0.8972 loss_val: 1.4414 acc_val: 0.7505 time: 743.4276s\n",
            "Epoch: 1381 loss_train: 0.3245 acc_train: 0.8957 loss_val: 1.4451 acc_val: 0.7535 time: 743.9656s\n",
            "Epoch: 1382 loss_train: 0.3301 acc_train: 0.8976 loss_val: 1.4487 acc_val: 0.7495 time: 744.5039s\n",
            "Epoch: 1383 loss_train: 0.3326 acc_train: 0.8937 loss_val: 1.4570 acc_val: 0.7538 time: 745.0322s\n",
            "Epoch: 1384 loss_train: 0.3315 acc_train: 0.8976 loss_val: 1.4577 acc_val: 0.7422 time: 745.5642s\n",
            "Epoch: 1385 loss_train: 0.3308 acc_train: 0.8925 loss_val: 1.4511 acc_val: 0.7465 time: 746.1114s\n",
            "Epoch: 1386 loss_train: 0.3268 acc_train: 0.8977 loss_val: 1.4491 acc_val: 0.7535 time: 746.6480s\n",
            "Epoch: 1387 loss_train: 0.3336 acc_train: 0.8964 loss_val: 1.4431 acc_val: 0.7345 time: 747.1849s\n",
            "Epoch: 1388 loss_train: 0.3314 acc_train: 0.8922 loss_val: 1.4498 acc_val: 0.7429 time: 747.7247s\n",
            "Epoch: 1389 loss_train: 0.3307 acc_train: 0.8923 loss_val: 1.4562 acc_val: 0.7535 time: 748.2631s\n",
            "Epoch: 1390 loss_train: 0.3409 acc_train: 0.9004 loss_val: 1.4488 acc_val: 0.7225 time: 748.7984s\n",
            "Epoch: 1391 loss_train: 0.3381 acc_train: 0.8882 loss_val: 1.4544 acc_val: 0.7418 time: 749.3257s\n",
            "Epoch: 1392 loss_train: 0.3289 acc_train: 0.8969 loss_val: 1.4585 acc_val: 0.7600 time: 749.8570s\n",
            "Epoch: 1393 loss_train: 0.3384 acc_train: 0.8974 loss_val: 1.4467 acc_val: 0.7233 time: 750.3896s\n",
            "Epoch: 1394 loss_train: 0.3447 acc_train: 0.8819 loss_val: 1.4539 acc_val: 0.7476 time: 750.9208s\n",
            "Epoch: 1395 loss_train: 0.3233 acc_train: 0.8963 loss_val: 1.4593 acc_val: 0.7618 time: 751.4566s\n",
            "Epoch: 1396 loss_train: 0.3359 acc_train: 0.8963 loss_val: 1.4446 acc_val: 0.7353 time: 752.0066s\n",
            "Epoch: 1397 loss_train: 0.3305 acc_train: 0.8929 loss_val: 1.4397 acc_val: 0.7316 time: 752.5407s\n",
            "Epoch: 1398 loss_train: 0.3356 acc_train: 0.8932 loss_val: 1.4474 acc_val: 0.7571 time: 753.0748s\n",
            "Epoch: 1399 loss_train: 0.3298 acc_train: 0.8958 loss_val: 1.4530 acc_val: 0.7578 time: 753.6232s\n",
            "Epoch: 1400 loss_train: 0.3350 acc_train: 0.9006 loss_val: 1.4467 acc_val: 0.7364 time: 754.1529s\n",
            "Epoch: 1401 loss_train: 0.3356 acc_train: 0.8929 loss_val: 1.4470 acc_val: 0.7389 time: 754.6812s\n",
            "Epoch: 1402 loss_train: 0.3202 acc_train: 0.8969 loss_val: 1.4469 acc_val: 0.7527 time: 755.1978s\n",
            "Epoch: 1403 loss_train: 0.3254 acc_train: 0.8958 loss_val: 1.4474 acc_val: 0.7513 time: 755.7292s\n",
            "Epoch: 1404 loss_train: 0.3263 acc_train: 0.8980 loss_val: 1.4402 acc_val: 0.7316 time: 756.2503s\n",
            "Epoch: 1405 loss_train: 0.3267 acc_train: 0.8930 loss_val: 1.4480 acc_val: 0.7516 time: 756.7764s\n",
            "Epoch: 1406 loss_train: 0.3269 acc_train: 0.8982 loss_val: 1.4499 acc_val: 0.7524 time: 757.3109s\n",
            "Epoch: 1407 loss_train: 0.3309 acc_train: 0.8911 loss_val: 1.4477 acc_val: 0.7349 time: 757.8375s\n",
            "Epoch: 1408 loss_train: 0.3258 acc_train: 0.8972 loss_val: 1.4445 acc_val: 0.7367 time: 758.3728s\n",
            "Epoch: 1409 loss_train: 0.3290 acc_train: 0.8965 loss_val: 1.4456 acc_val: 0.7484 time: 758.9078s\n",
            "Epoch: 1410 loss_train: 0.3298 acc_train: 0.8969 loss_val: 1.4499 acc_val: 0.7582 time: 759.4692s\n",
            "Epoch: 1411 loss_train: 0.3401 acc_train: 0.8941 loss_val: 1.4438 acc_val: 0.7433 time: 760.0282s\n",
            "Epoch: 1412 loss_train: 0.3423 acc_train: 0.8932 loss_val: 1.4467 acc_val: 0.7538 time: 760.5559s\n",
            "Epoch: 1413 loss_train: 0.3276 acc_train: 0.8974 loss_val: 1.4455 acc_val: 0.7415 time: 761.0822s\n",
            "Epoch: 1414 loss_train: 0.3363 acc_train: 0.8924 loss_val: 1.4429 acc_val: 0.7411 time: 761.6034s\n",
            "Epoch: 1415 loss_train: 0.3297 acc_train: 0.8950 loss_val: 1.4482 acc_val: 0.7513 time: 762.1399s\n",
            "Epoch: 1416 loss_train: 0.3309 acc_train: 0.8974 loss_val: 1.4516 acc_val: 0.7611 time: 762.6749s\n",
            "Epoch: 1417 loss_train: 0.3355 acc_train: 0.8974 loss_val: 1.4439 acc_val: 0.7455 time: 763.2062s\n",
            "Epoch: 1418 loss_train: 0.3231 acc_train: 0.8973 loss_val: 1.4404 acc_val: 0.7291 time: 763.7590s\n",
            "Epoch: 1419 loss_train: 0.3328 acc_train: 0.8897 loss_val: 1.4475 acc_val: 0.7505 time: 764.2869s\n",
            "Epoch: 1420 loss_train: 0.3236 acc_train: 0.8988 loss_val: 1.4524 acc_val: 0.7578 time: 764.8279s\n",
            "Epoch: 1421 loss_train: 0.3277 acc_train: 0.8972 loss_val: 1.4412 acc_val: 0.7371 time: 765.3563s\n",
            "Epoch: 1422 loss_train: 0.3375 acc_train: 0.8904 loss_val: 1.4394 acc_val: 0.7425 time: 765.8866s\n",
            "Epoch: 1423 loss_train: 0.3376 acc_train: 0.8948 loss_val: 1.4384 acc_val: 0.7396 time: 766.4070s\n",
            "Epoch: 1424 loss_train: 0.3146 acc_train: 0.8991 loss_val: 1.4454 acc_val: 0.7353 time: 766.9408s\n",
            "Epoch: 1425 loss_train: 0.3315 acc_train: 0.8930 loss_val: 1.4527 acc_val: 0.7582 time: 767.4726s\n",
            "Epoch: 1426 loss_train: 0.3343 acc_train: 0.8950 loss_val: 1.4421 acc_val: 0.7436 time: 768.0012s\n",
            "Epoch: 1427 loss_train: 0.3227 acc_train: 0.8987 loss_val: 1.4393 acc_val: 0.7367 time: 768.5333s\n",
            "Epoch: 1428 loss_train: 0.3373 acc_train: 0.8940 loss_val: 1.4397 acc_val: 0.7385 time: 769.0770s\n",
            "Epoch: 1429 loss_train: 0.3300 acc_train: 0.8968 loss_val: 1.4448 acc_val: 0.7516 time: 769.6156s\n",
            "Epoch: 1430 loss_train: 0.3241 acc_train: 0.8976 loss_val: 1.4456 acc_val: 0.7487 time: 770.1570s\n",
            "Epoch: 1431 loss_train: 0.3299 acc_train: 0.8953 loss_val: 1.4421 acc_val: 0.7316 time: 770.7033s\n",
            "Epoch: 1432 loss_train: 0.3380 acc_train: 0.8834 loss_val: 1.4477 acc_val: 0.7502 time: 771.2339s\n",
            "Epoch: 1433 loss_train: 0.3421 acc_train: 0.8931 loss_val: 1.4480 acc_val: 0.7465 time: 771.7723s\n",
            "Epoch: 1434 loss_train: 0.3407 acc_train: 0.8955 loss_val: 1.4435 acc_val: 0.7371 time: 772.3045s\n",
            "Epoch: 1435 loss_train: 0.3260 acc_train: 0.8959 loss_val: 1.4446 acc_val: 0.7418 time: 772.8860s\n",
            "Epoch: 1436 loss_train: 0.3250 acc_train: 0.8955 loss_val: 1.4447 acc_val: 0.7484 time: 773.4387s\n",
            "Epoch: 1437 loss_train: 0.3387 acc_train: 0.8941 loss_val: 1.4419 acc_val: 0.7302 time: 773.9929s\n",
            "Epoch: 1438 loss_train: 0.3348 acc_train: 0.8911 loss_val: 1.4441 acc_val: 0.7349 time: 774.5293s\n",
            "Epoch: 1439 loss_train: 0.3278 acc_train: 0.8942 loss_val: 1.4519 acc_val: 0.7629 time: 775.0770s\n",
            "Epoch: 1440 loss_train: 0.3370 acc_train: 0.8983 loss_val: 1.4379 acc_val: 0.7324 time: 775.6074s\n",
            "Epoch: 1441 loss_train: 0.3318 acc_train: 0.8914 loss_val: 1.4329 acc_val: 0.7262 time: 776.1416s\n",
            "Epoch: 1442 loss_train: 0.3307 acc_train: 0.8912 loss_val: 1.4416 acc_val: 0.7556 time: 776.6617s\n",
            "Epoch: 1443 loss_train: 0.3294 acc_train: 0.8969 loss_val: 1.4447 acc_val: 0.7476 time: 777.1881s\n",
            "Epoch: 1444 loss_train: 0.3314 acc_train: 0.8960 loss_val: 1.4441 acc_val: 0.7233 time: 777.7254s\n",
            "Epoch: 1445 loss_train: 0.3357 acc_train: 0.8931 loss_val: 1.4495 acc_val: 0.7455 time: 778.2530s\n",
            "Epoch: 1446 loss_train: 0.3359 acc_train: 0.8963 loss_val: 1.4419 acc_val: 0.7422 time: 778.7931s\n",
            "Epoch: 1447 loss_train: 0.3341 acc_train: 0.8966 loss_val: 1.4314 acc_val: 0.7280 time: 779.3271s\n",
            "Epoch: 1448 loss_train: 0.3299 acc_train: 0.8943 loss_val: 1.4414 acc_val: 0.7516 time: 779.8745s\n",
            "Epoch: 1449 loss_train: 0.3366 acc_train: 0.8968 loss_val: 1.4428 acc_val: 0.7498 time: 780.4043s\n",
            "Epoch: 1450 loss_train: 0.3300 acc_train: 0.8983 loss_val: 1.4421 acc_val: 0.7287 time: 780.9484s\n",
            "Epoch: 1451 loss_train: 0.3352 acc_train: 0.8897 loss_val: 1.4478 acc_val: 0.7480 time: 781.4751s\n",
            "Epoch: 1452 loss_train: 0.3359 acc_train: 0.8964 loss_val: 1.4384 acc_val: 0.7396 time: 782.0172s\n",
            "Epoch: 1453 loss_train: 0.3311 acc_train: 0.8964 loss_val: 1.4366 acc_val: 0.7371 time: 782.5484s\n",
            "Epoch: 1454 loss_train: 0.3327 acc_train: 0.8921 loss_val: 1.4403 acc_val: 0.7480 time: 783.0943s\n",
            "Epoch: 1455 loss_train: 0.3362 acc_train: 0.8945 loss_val: 1.4426 acc_val: 0.7375 time: 783.6141s\n",
            "Epoch: 1456 loss_train: 0.3273 acc_train: 0.8957 loss_val: 1.4420 acc_val: 0.7400 time: 784.1607s\n",
            "Epoch: 1457 loss_train: 0.3305 acc_train: 0.8969 loss_val: 1.4405 acc_val: 0.7484 time: 784.6816s\n",
            "Epoch: 1458 loss_train: 0.3286 acc_train: 0.8975 loss_val: 1.4347 acc_val: 0.7356 time: 785.2083s\n",
            "Epoch: 1459 loss_train: 0.3333 acc_train: 0.8903 loss_val: 1.4385 acc_val: 0.7465 time: 785.7337s\n",
            "Epoch: 1460 loss_train: 0.3340 acc_train: 0.8958 loss_val: 1.4437 acc_val: 0.7462 time: 786.2689s\n",
            "Epoch: 1461 loss_train: 0.3305 acc_train: 0.8958 loss_val: 1.4476 acc_val: 0.7458 time: 786.7991s\n",
            "Epoch: 1462 loss_train: 0.3357 acc_train: 0.8937 loss_val: 1.4419 acc_val: 0.7371 time: 787.3325s\n",
            "Epoch: 1463 loss_train: 0.3246 acc_train: 0.8942 loss_val: 1.4364 acc_val: 0.7389 time: 787.8748s\n",
            "Epoch: 1464 loss_train: 0.3208 acc_train: 0.8986 loss_val: 1.4347 acc_val: 0.7465 time: 788.4022s\n",
            "Epoch: 1465 loss_train: 0.3188 acc_train: 0.9001 loss_val: 1.4355 acc_val: 0.7400 time: 788.9355s\n",
            "Epoch: 1466 loss_train: 0.3316 acc_train: 0.8961 loss_val: 1.4394 acc_val: 0.7425 time: 789.4736s\n",
            "Epoch: 1467 loss_train: 0.3306 acc_train: 0.8981 loss_val: 1.4425 acc_val: 0.7338 time: 790.0153s\n",
            "Epoch: 1468 loss_train: 0.3280 acc_train: 0.8961 loss_val: 1.4434 acc_val: 0.7382 time: 790.5325s\n",
            "Epoch: 1469 loss_train: 0.3251 acc_train: 0.8973 loss_val: 1.4425 acc_val: 0.7415 time: 791.0726s\n",
            "Epoch: 1470 loss_train: 0.3245 acc_train: 0.8987 loss_val: 1.4408 acc_val: 0.7480 time: 791.5963s\n",
            "Epoch: 1471 loss_train: 0.3289 acc_train: 0.8955 loss_val: 1.4362 acc_val: 0.7385 time: 792.1278s\n",
            "Epoch: 1472 loss_train: 0.3260 acc_train: 0.8959 loss_val: 1.4392 acc_val: 0.7342 time: 792.6427s\n",
            "Epoch: 1473 loss_train: 0.3297 acc_train: 0.8926 loss_val: 1.4437 acc_val: 0.7378 time: 793.1858s\n",
            "Epoch: 1474 loss_train: 0.3179 acc_train: 0.9008 loss_val: 1.4402 acc_val: 0.7345 time: 793.7220s\n",
            "Epoch: 1475 loss_train: 0.3290 acc_train: 0.8960 loss_val: 1.4381 acc_val: 0.7396 time: 794.2697s\n",
            "Epoch: 1476 loss_train: 0.3272 acc_train: 0.8969 loss_val: 1.4388 acc_val: 0.7407 time: 794.7959s\n",
            "Epoch: 1477 loss_train: 0.3337 acc_train: 0.8961 loss_val: 1.4390 acc_val: 0.7349 time: 795.3253s\n",
            "Epoch: 1478 loss_train: 0.3298 acc_train: 0.8955 loss_val: 1.4450 acc_val: 0.7411 time: 795.8451s\n",
            "Epoch: 1479 loss_train: 0.3232 acc_train: 0.8976 loss_val: 1.4434 acc_val: 0.7298 time: 796.3632s\n",
            "Epoch: 1480 loss_train: 0.3246 acc_train: 0.8973 loss_val: 1.4381 acc_val: 0.7331 time: 796.8909s\n",
            "Epoch: 1481 loss_train: 0.3207 acc_train: 0.8945 loss_val: 1.4420 acc_val: 0.7418 time: 797.4233s\n",
            "Epoch: 1482 loss_train: 0.3333 acc_train: 0.8957 loss_val: 1.4399 acc_val: 0.7156 time: 797.9549s\n",
            "Epoch: 1483 loss_train: 0.3251 acc_train: 0.8940 loss_val: 1.4472 acc_val: 0.7342 time: 798.4694s\n",
            "Epoch: 1484 loss_train: 0.3307 acc_train: 0.8958 loss_val: 1.4480 acc_val: 0.7429 time: 798.9922s\n",
            "Epoch: 1485 loss_train: 0.3318 acc_train: 0.8984 loss_val: 1.4360 acc_val: 0.7225 time: 799.5120s\n",
            "Epoch: 1486 loss_train: 0.3309 acc_train: 0.8925 loss_val: 1.4370 acc_val: 0.7378 time: 800.0379s\n",
            "Epoch: 1487 loss_train: 0.3212 acc_train: 0.8955 loss_val: 1.4416 acc_val: 0.7480 time: 800.5565s\n",
            "Epoch: 1488 loss_train: 0.3254 acc_train: 0.8988 loss_val: 1.4382 acc_val: 0.7335 time: 801.0812s\n",
            "Epoch: 1489 loss_train: 0.3316 acc_train: 0.8942 loss_val: 1.4400 acc_val: 0.7225 time: 801.6170s\n",
            "Epoch: 1490 loss_train: 0.3249 acc_train: 0.8963 loss_val: 1.4434 acc_val: 0.7364 time: 802.1484s\n",
            "Epoch: 1491 loss_train: 0.3200 acc_train: 0.9003 loss_val: 1.4438 acc_val: 0.7462 time: 802.6660s\n",
            "Epoch: 1492 loss_train: 0.3284 acc_train: 0.8969 loss_val: 1.4338 acc_val: 0.7298 time: 803.1987s\n",
            "Epoch: 1493 loss_train: 0.3205 acc_train: 0.8999 loss_val: 1.4334 acc_val: 0.7316 time: 803.7228s\n",
            "Epoch: 1494 loss_train: 0.3212 acc_train: 0.8964 loss_val: 1.4349 acc_val: 0.7360 time: 804.2686s\n",
            "Epoch: 1495 loss_train: 0.3184 acc_train: 0.8984 loss_val: 1.4359 acc_val: 0.7284 time: 804.8040s\n",
            "Epoch: 1496 loss_train: 0.3223 acc_train: 0.8953 loss_val: 1.4390 acc_val: 0.7364 time: 805.3276s\n",
            "Epoch: 1497 loss_train: 0.3256 acc_train: 0.8968 loss_val: 1.4413 acc_val: 0.7436 time: 805.8542s\n",
            "Epoch: 1498 loss_train: 0.3325 acc_train: 0.8967 loss_val: 1.4383 acc_val: 0.7218 time: 806.3828s\n",
            "Epoch: 1499 loss_train: 0.3304 acc_train: 0.8925 loss_val: 1.4381 acc_val: 0.7342 time: 806.9176s\n",
            "Epoch: 1500 loss_train: 0.3221 acc_train: 0.8991 loss_val: 1.4385 acc_val: 0.7353 time: 807.4528s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Optimization Finished!\n",
            "Total time elapsed: 807.8685s\n",
            "GCN(\n",
            "  (gc1): GraphConvolution()\n",
            "  (gc2): GraphConvolution()\n",
            ")\n",
            "Test set results: loss= 1.7078 accuracy= 0.6378\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}