{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FewShot_NoActiveL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "https://github.com/vahidseydi/CGN/blob/main/GCN.ipynb",
      "authorship_tag": "ABX9TyOBkkkS8OA5DNJszCDQhEux",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vahidseydi/CGN/blob/main/FewShot_NoActiveL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQrGTMcwGbkt"
      },
      "source": [
        "#Downloading dataset file from Github"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnHBK6RWuN9j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b0b79e0-7877-4bce-831e-d7f2e1323f77"
      },
      "source": [
        "#ابتدا دیتاست موجود در فولدر دیتا در آدرس گیت هاب پروژه، را در این نوت بوک دخیره می کنیم\n",
        "! wget 'https://github.com/vahidseydi/CGN/blob/main/Data/amazon_electronics_computers%20(1).npz?raw=true'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-24 11:15:30--  https://github.com/vahidseydi/CGN/blob/main/Data/amazon_electronics_computers%20(1).npz?raw=true\n",
            "Resolving github.com (github.com)... 192.30.255.112\n",
            "Connecting to github.com (github.com)|192.30.255.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/vahidseydi/CGN/raw/main/Data/amazon_electronics_computers%20(1).npz [following]\n",
            "--2021-05-24 11:15:31--  https://github.com/vahidseydi/CGN/raw/main/Data/amazon_electronics_computers%20(1).npz\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/vahidseydi/CGN/main/Data/amazon_electronics_computers%20(1).npz [following]\n",
            "--2021-05-24 11:15:31--  https://raw.githubusercontent.com/vahidseydi/CGN/main/Data/amazon_electronics_computers%20(1).npz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 31921488 (30M) [application/octet-stream]\n",
            "Saving to: ‘amazon_electronics_computers (1).npz?raw=true.1’\n",
            "\n",
            "amazon_electronics_ 100%[===================>]  30.44M   128MB/s    in 0.2s    \n",
            "\n",
            "2021-05-24 11:15:31 (128 MB/s) - ‘amazon_electronics_computers (1).npz?raw=true.1’ saved [31921488/31921488]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INK67X5IGkkc"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MYbUcRVdtDe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff15d5a7-5ed5-4b86-b265-1906b39df198"
      },
      "source": [
        "import numpy as np\n",
        "#فایل های موجود در دیتاست را می خوانیم\n",
        "npz_data=np.load('/content/amazon_electronics_computers (1).npz?raw=true')\n",
        "npz_data.files"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['adj_data',\n",
              " 'adj_indices',\n",
              " 'adj_indptr',\n",
              " 'adj_shape',\n",
              " 'attr_data',\n",
              " 'attr_indices',\n",
              " 'attr_indptr',\n",
              " 'attr_shape',\n",
              " 'labels',\n",
              " 'class_names']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmNiWW8bd-Xx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66680bfa-7ef3-42c1-d571-551bf6580e69"
      },
      "source": [
        "#کلاس های موجود در دیتاست شامل 10 کلاس می باشد\n",
        "class_names =npz_data['class_names']\n",
        "class_names"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Desktops', 'Data Storage', 'Laptops', 'Monitors',\n",
              "       'Computer Components', 'Video Projectors', 'Routers', 'Tablets',\n",
              "       'Networking Products', 'Webcams'], dtype='<U19')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFpxP_MNMmCX"
      },
      "source": [
        "#تابع زیر برای جداسازی داده های آموزش، تست و اعتبارسنجی استفاده می شود\n",
        "#برابر با تمام داده ها می باشد y\n",
        "labels =npz_data['labels']\n",
        "y=labels.size\n",
        "#p for calculating percentage for idx_train,val,test\n",
        "p = lambda x: x*y/100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9NewBDvWAdE"
      },
      "source": [
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import torch\n",
        "from scipy.sparse import  csr_matrix\n",
        "\n",
        "\n",
        "def load_data():\n",
        " \n",
        " features = sp.csr_matrix((npz_data['attr_data'], npz_data['attr_indices'], npz_data['attr_indptr']),shape=npz_data['attr_shape'])\n",
        "\n",
        " # build graph\n",
        " \n",
        " adj= sp.csr_matrix(sp.csr_matrix((npz_data['adj_data'], npz_data['adj_indices'], npz_data['adj_indptr']),shape=npz_data['adj_shape']))\n",
        " \n",
        " # build symmetric adjacency matrix\n",
        "\n",
        " adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
        " adj = normalize(adj + sp.eye(adj.shape[0]))\n",
        " #اول اعداد 0 و 1 هست بعد از نرمالایز تغییر میکند \n",
        "\n",
        " labels=npz_data['labels']\n",
        " #لیبل ها را یکی اضافه می کنیم تا لیبل 0 را به داده های بدون لیبل دهیم\n",
        " labels=labels+1\n",
        "\n",
        " #جداسازی داده ها \n",
        " idx_train =range(round(p(70)))\n",
        " idx_val = range(idx_train[-1],idx_train[-1]+round(p(15)))\n",
        " idx_test =range(idx_val[-1],idx_val[-1]+round(p(15)))\n",
        "\n",
        " idx_train = torch.LongTensor(idx_train)\n",
        " idx_val = torch.LongTensor(idx_val)\n",
        " idx_test = torch.LongTensor(idx_test)\n",
        "\n",
        " features = torch.FloatTensor(np.array(features.todense()))\n",
        " adj = torch.FloatTensor(np.array(adj.todense()))\n",
        " labels = torch.LongTensor(labels)\n",
        "\n",
        "\n",
        " return adj, features, labels, idx_train, idx_val, idx_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9YheZ8yNTSL"
      },
      "source": [
        "def normalize(mx):\n",
        "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
        "    rowsum = np.array(mx.sum(1))\n",
        "    #sum in every row \n",
        "    r_inv = np.power(rowsum, -1).flatten()\n",
        "    # every sum to the power of -1 \n",
        "    r_inv[np.isinf(r_inv)] = 0.\n",
        "    r_mat_inv = sp.diags(r_inv)\n",
        "    #diagonal matrice \n",
        "    mx = r_mat_inv.dot(mx)\n",
        "    return mx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iha5MQaYDJpG"
      },
      "source": [
        "def accuracy(output, labels):\n",
        "    preds = output.max(1)[1].type_as(labels)\n",
        "    correct = preds.eq(labels).double()\n",
        "    correct = correct.sum()\n",
        "    return correct / len(labels)    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQLBOry4WJvV"
      },
      "source": [
        "adj, features, labels, idx_train, idx_val, idx_test = load_data()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsHyFUvEUMFQ"
      },
      "source": [
        "\n",
        "\n",
        "#Constructing Train data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0-99ajritPS"
      },
      "source": [
        "#20% of train data for labeled\n",
        "#80% of train data for unlabeled\n",
        "#جداسازی داده های با لیبل و بدون لیبل برای آموزش \n",
        "\n",
        "p_train = lambda x: x*len(idx_train)/100\n",
        "\n",
        "idx_labeled_train =range(round(p_train(20)))\n",
        "idx_unlabeled_train = range(idx_labeled_train[-1],idx_labeled_train[-1]+round(p_train(80)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUovgcgTOAmt",
        "outputId": "7037d0e3-f2d3-4a4f-f37e-f91af3fbbbc0"
      },
      "source": [
        "print(idx_labeled_train,idx_unlabeled_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "range(0, 1925) range(1924, 9625)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6003R_bp4Xkz"
      },
      "source": [
        "#اندیس مربوط به هر کلاسی را جدا می کنیم\n",
        "class_1 =[]\n",
        "class_2 =[]\n",
        "class_3 =[]\n",
        "class_4 =[]\n",
        "class_5 =[]\n",
        "class_6 =[]\n",
        "class_7 =[]\n",
        "class_8 =[]\n",
        "class_9 =[]\n",
        "class_10=[]\n",
        "class_1=[i for i,x in enumerate(labels[idx_labeled_train]) if x==1] \n",
        "class_2=[i for i,x in enumerate(labels[idx_labeled_train]) if x==2] \n",
        "class_3=[i for i,x in enumerate(labels[idx_labeled_train]) if x==3]\n",
        "class_4=[i for i,x in enumerate(labels[idx_labeled_train]) if x==4] \n",
        "class_5=[i for i,x in enumerate(labels[idx_labeled_train]) if x==5] \n",
        "class_6=[i for i,x in enumerate(labels[idx_labeled_train]) if x==6] \n",
        "class_7=[i for i,x in enumerate(labels[idx_labeled_train]) if x==7] \n",
        "class_8=[i for i,x in enumerate(labels[idx_labeled_train]) if x==8] \n",
        "class_9=[i for i,x in enumerate(labels[idx_labeled_train]) if x==9] \n",
        "class_10=[i for i,x in enumerate(labels[idx_labeled_train]) if x==10] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unaiIobt6ExB"
      },
      "source": [
        "#class_10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1Sok0uo6Igw",
        "outputId": "ee3a9844-ee76-4f83-922e-5ee29ff80a17"
      },
      "source": [
        "labels[583]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DEj19oZ9yVr"
      },
      "source": [
        "#می خواهیم از هر کلاسی به صورت رندوم سمپل انتخاب کنیم\n",
        "import random\n",
        "Random_idx_train=[]\n",
        "#با تعداد شات تعیین می کنیم که از هر کلاس چند سمپل نیاز داریم\n",
        "N_shot=10\n",
        "for i in range(N_shot):\n",
        "  Random_idx_train.append(random.choice(class_1))\n",
        "  Random_idx_train.append(random.choice(class_2))\n",
        "  Random_idx_train.append(random.choice(class_3))\n",
        "  Random_idx_train.append(random.choice(class_4))\n",
        "  Random_idx_train.append(random.choice(class_5))\n",
        "  Random_idx_train.append(random.choice(class_6))\n",
        "  Random_idx_train.append(random.choice(class_7))\n",
        "  Random_idx_train.append(random.choice(class_8))\n",
        "  Random_idx_train.append(random.choice(class_9))\n",
        "  Random_idx_train.append(random.choice(class_10))\n",
        "\n",
        "#Random_idx_train#اندیس "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1_KllTImV3U"
      },
      "source": [
        "#اگر بخواهیم از هر کلاس یک سمپل انتخاب کنیم مجموعا 10 سمپل با لیبل داریم \n",
        "#اگر بخواهیم نسبت 20 به 80 را رعایت کنیم پس لازم است 40 داده بدون لیبل هم انتخاب کنیم\n",
        "\n",
        "hiddenIndex_train=[]\n",
        "\n",
        "for i in range(40*N_shot):\n",
        "  hiddenIndex_train.append(random.choice(idx_unlabeled_train))\n",
        "#hiddenIndex_train  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1C808icQZ2u_"
      },
      "source": [
        "#مجموع اندیس های آموزش شامل اندیس های زیر می باشد:\n",
        "#traindata_idx=Random_idx_train+hiddenIndex_train\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRhvyy9HgXT-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f93af9a8-4b69-43d8-b9ed-66de56c79f1b"
      },
      "source": [
        "#labels_train_hidden:\n",
        "#تمام لیبل ها حتی هیدن ها،برای استفاده در اکتیولرنینگ\n",
        "labels_train_hidden=torch.hstack((labels[Random_idx_train],labels[hiddenIndex_train]))\n",
        "labels_train_hidden.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([500])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SkfjGpTMeVw"
      },
      "source": [
        "#labels_train_hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8OE8yFZhkFvI",
        "outputId": "ea7d8f3d-9cb2-46d0-dc74-31fa657c7f22"
      },
      "source": [
        "labels[Random_idx_train]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10,  1,  2,  3,  4,  5,  6,  7,  8,\n",
              "         9, 10,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10,  1,  2,  3,  4,  5,  6,\n",
              "         7,  8,  9, 10,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10,  1,  2,  3,  4,\n",
              "         5,  6,  7,  8,  9, 10,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10,  1,  2,\n",
              "         3,  4,  5,  6,  7,  8,  9, 10,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10,\n",
              "         1,  2,  3,  4,  5,  6,  7,  8,  9, 10])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOr1K7YJm7td"
      },
      "source": [
        "#labels[Random_idx_train]==labels_train_hidden[0:10*N_shot]\n",
        "#labels[hiddenIndex_train]==labels_train_hidden[10*N_shot:10*N_shot*4+10*N_shot]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xo58EqipMk_m"
      },
      "source": [
        "#labels_train=labels_train_hidden\n",
        "#با توجه به اینکه از هر کلاس چند تا سمپل انتخاب کردیم از شماره آخری بقیه مربوط به هیدن ها میشود که صفر میذاریم\n",
        "#labels_train[10*N_shot+1:]=0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btpV4dc2Mnyc"
      },
      "source": [
        "#labels_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yt6_cLWoMsTH"
      },
      "source": [
        "#labels_train_hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7Lwde1zkTe9"
      },
      "source": [
        "labels_train=labels_train_hidden.detach().clone()\n",
        "#با توجه به اینکه از هر کلاس چند تا سمپل انتخاب کردیم از شماره آخری بقیه مربوط به هیدن ها میشود که صفر میذاریم\n",
        "labels_train[10*N_shot:]=0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EU2yK_3RM6Cl"
      },
      "source": [
        "#labels_train_hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EteMi10CkrbR"
      },
      "source": [
        "#labels_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FiNVOiOZf9OM",
        "outputId": "41146366-76a8-4e2e-e03a-ff69080dda85"
      },
      "source": [
        "#ایجاد ماتریس ویژگی ها از داده های با لیبل و بدون لیبل\n",
        "features_train=torch.vstack((features[Random_idx_train],features[hiddenIndex_train]))\n",
        "features_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([500, 767])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R6bLiV3Ynb0l",
        "outputId": "6c63b895-1481-4706-c90f-40d9f5ac613d"
      },
      "source": [
        "features_train[0:10*N_shot]==features[Random_idx_train]\n",
        "features_train[10*N_shot:10*N_shot*4+10*N_shot]==features[hiddenIndex_train]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[True, True, True,  ..., True, True, True],\n",
              "        [True, True, True,  ..., True, True, True],\n",
              "        [True, True, True,  ..., True, True, True],\n",
              "        ...,\n",
              "        [True, True, True,  ..., True, True, True],\n",
              "        [True, True, True,  ..., True, True, True],\n",
              "        [True, True, True,  ..., True, True, True]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28I52FrOiii1"
      },
      "source": [
        "#adj[1:10*N_shot,1:10*N_shot].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4x-xGrv0Sw0"
      },
      "source": [
        "#ایجاد ماتریس مجاورت از داده های با لیبل و بدون لیبل\n",
        "adj_train=torch.vstack((adj[Random_idx_train],adj[hiddenIndex_train]))\n",
        "\n",
        "for i in hiddenIndex_train :\n",
        "    Random_idx_train.append(i)\n",
        "    \n",
        "#ایجاد تقارن در ماتریس مجاورت\n",
        "adj_train=adj_train[0:10*N_shot*4+10*N_shot,Random_idx_train]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7_IwfbRKIt_"
      },
      "source": [
        "#adj_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kerWgiRDBcDP"
      },
      "source": [
        "#Random_idx_train[2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCpOOU5-83cf"
      },
      "source": [
        "#adj[717].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4yLSWzV83Z6"
      },
      "source": [
        "#adj_train[0].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08hYvuat7Z5w"
      },
      "source": [
        "#adj[782,782]\n",
        "#adj[30,30]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elrp9c9FTi2w"
      },
      "source": [
        "#Constructing Test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jorUo0cyTv7o"
      },
      "source": [
        "##داده های تست و اعتبارسنجی نیز دقیقا مشابه داده های آموزش ایجاد می شوند##\n",
        "\n",
        "#20% of test data for labeled\n",
        "#80% of test data for unlabeled\n",
        "\n",
        "p_test = lambda x: x*len(idx_test)/100\n",
        "\n",
        "idx_labeled_test =range(round(p_test(20)))\n",
        "idx_unlabeled_test = range(idx_labeled_test[-1],idx_labeled_test[-1]+round(p_test(80)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZR91mjVTv7q",
        "outputId": "22b6b9c6-47a9-4754-f188-d326868d20f2"
      },
      "source": [
        "print(idx_labeled_test,idx_unlabeled_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "range(0, 413) range(412, 2062)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRdkBXRUTv7s"
      },
      "source": [
        "#اندیس مربوط به هر کلاسی را جدا می کنیم\n",
        "class_1 =[]\n",
        "class_2 =[]\n",
        "class_3 =[]\n",
        "class_4 =[]\n",
        "class_5 =[]\n",
        "class_6 =[]\n",
        "class_7 =[]\n",
        "class_8 =[]\n",
        "class_9 =[]\n",
        "class_10=[]\n",
        "\n",
        "class_1 =[i for i,x in enumerate(labels[idx_labeled_test]) if x==1] \n",
        "class_2 =[i for i,x in enumerate(labels[idx_labeled_test]) if x==2] \n",
        "class_3 =[i for i,x in enumerate(labels[idx_labeled_test]) if x==3]\n",
        "class_4 =[i for i,x in enumerate(labels[idx_labeled_test]) if x==4] \n",
        "class_5 =[i for i,x in enumerate(labels[idx_labeled_test]) if x==5] \n",
        "class_6 =[i for i,x in enumerate(labels[idx_labeled_test]) if x==6] \n",
        "class_7 =[i for i,x in enumerate(labels[idx_labeled_test]) if x==7] \n",
        "class_8 =[i for i,x in enumerate(labels[idx_labeled_test]) if x==8] \n",
        "class_9 =[i for i,x in enumerate(labels[idx_labeled_test]) if x==9] \n",
        "class_10=[i for i,x in enumerate(labels[idx_labeled_test]) if x==10] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sx8QozEI6WyU",
        "outputId": "7584611c-4d10-459c-bf8f-73f8cbe177b7"
      },
      "source": [
        "print(class_3 , labels[48])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[22, 29, 37, 48, 50, 57, 68, 72, 73, 87, 92, 106, 113, 125, 129, 131, 135, 149, 157, 190, 199, 207, 223, 226, 231, 241, 252, 254, 262, 277, 302, 309, 311, 318, 321, 322, 324, 325, 338, 359, 364, 367, 376, 393, 403] tensor(3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61JWi16l6zM3",
        "outputId": "129065c6-8bb6-4ffa-8137-f82cdfaf5fcc"
      },
      "source": [
        "type(class_1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AE5Sw2ZTv7t"
      },
      "source": [
        "# از هر کلاسی بتوانیم به صورت رندوم سمپل انتخاب کنیم\n",
        "import random\n",
        "Random_idx_test=[]\n",
        "\n",
        "for i in range(N_shot):\n",
        "  Random_idx_test.append(random.choice(class_1))\n",
        "  Random_idx_test.append(random.choice(class_2))\n",
        "  Random_idx_test.append(random.choice(class_3))\n",
        "  Random_idx_test.append(random.choice(class_4))\n",
        "  Random_idx_test.append(random.choice(class_5))\n",
        "  Random_idx_test.append(random.choice(class_6))\n",
        "  Random_idx_test.append(random.choice(class_7))\n",
        "  Random_idx_test.append(random.choice(class_8))\n",
        "  Random_idx_test.append(random.choice(class_9))\n",
        "  Random_idx_test.append(random.choice(class_10))\n",
        "\n",
        "#Random_idx_test#اندیس "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rv7SWldYTv7u"
      },
      "source": [
        "#اگر بخواهیم از هر کلاس یک سمپل انتخاب کنیم مجموعا 10 سمپل با لیبل داریم \n",
        "#اگر بخواهیم نسبت 20 به 80 را ارعایت کنیم پس لازم است 40 داده بدون لیبل هم انتخاب کنیم\n",
        "\n",
        "hiddenIndex_test=[]\n",
        "\n",
        "for i in range(40*N_shot):\n",
        "  hiddenIndex_test.append(random.choice(idx_unlabeled_test))\n",
        "#hiddenIndex_test "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDcysoakTv7v",
        "outputId": "90054ce4-dd1e-4ddc-9fb7-3707f8552067"
      },
      "source": [
        "labels_test_hidden=torch.hstack((labels[Random_idx_test],labels[hiddenIndex_test]))\n",
        "labels_test_hidden.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([500])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRnuyYHiTv7w"
      },
      "source": [
        "labels_test=labels_test_hidden.detach().clone()\n",
        "#با توجه به اینکه از هر کلاس چند تا سمپل انتخاب کردیم از شماره آخری بقیه مربوط به هیدن ها میشود که صفر میذاریم\n",
        "labels_test[10*N_shot:]=0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7ICA7fEEztv"
      },
      "source": [
        "#labels_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWejgHLyE3qT"
      },
      "source": [
        "#labels_test_hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xtqi7LwXTv7x",
        "outputId": "333c18bf-eab2-41fe-fe10-8ccdf11aa59d"
      },
      "source": [
        "features_test=torch.vstack((features[Random_idx_test],features[hiddenIndex_test]))\n",
        "features_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([500, 767])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnW1mVA5Tv7y",
        "outputId": "32703cf5-5129-43eb-fca9-2010b6edfa5e"
      },
      "source": [
        "features_test[0:10*N_shot]==features[Random_idx_test]\n",
        "features_test[10*N_shot:10*N_shot*4+10*N_shot]==features[hiddenIndex_test]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[True, True, True,  ..., True, True, True],\n",
              "        [True, True, True,  ..., True, True, True],\n",
              "        [True, True, True,  ..., True, True, True],\n",
              "        ...,\n",
              "        [True, True, True,  ..., True, True, True],\n",
              "        [True, True, True,  ..., True, True, True],\n",
              "        [True, True, True,  ..., True, True, True]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3CLd-KVTv7z"
      },
      "source": [
        "adj_test=torch.vstack((adj[Random_idx_test],adj[hiddenIndex_test]))\n",
        "\n",
        "for i in hiddenIndex_test :\n",
        "    Random_idx_test.append(i)\n",
        "\n",
        "adj_test=adj_test[0:10*N_shot*4+10*N_shot,Random_idx_test]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dyz6oDB6Yyuf"
      },
      "source": [
        "#Constructing Validation Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRkhidE6YWTu"
      },
      "source": [
        "#20% of val data for labeled\n",
        "#80% of val data for unlabeled\n",
        "\n",
        "p_val = lambda x: x*len(idx_val)/100\n",
        "\n",
        "idx_labeled_val =range(round(p_val(20)))\n",
        "idx_unlabeled_val = range(idx_labeled_val[-1],idx_labeled_val[-1]+round(p_val(80)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqHcKhKYYWTw",
        "outputId": "c8ce9f47-3326-476d-adc6-7b1cff442a71"
      },
      "source": [
        "print(idx_labeled_val,idx_unlabeled_val)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "range(0, 413) range(412, 2062)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hR67HnajYWUE"
      },
      "source": [
        "#اندیس مربوط به هر کلاسی را جدا می کنیم\n",
        "class_1 =[]\n",
        "class_2 =[]\n",
        "class_3 =[]\n",
        "class_4 =[]\n",
        "class_5 =[]\n",
        "class_6 =[]\n",
        "class_7 =[]\n",
        "class_8 =[]\n",
        "class_9 =[]\n",
        "class_10=[]\n",
        "\n",
        "class_1=[i for i,x in enumerate(labels[idx_labeled_val]) if x==1] \n",
        "class_2=[i for i,x in enumerate(labels[idx_labeled_val]) if x==2] \n",
        "class_3=[i for i,x in enumerate(labels[idx_labeled_val]) if x==3]\n",
        "class_4=[i for i,x in enumerate(labels[idx_labeled_val]) if x==4] \n",
        "class_5=[i for i,x in enumerate(labels[idx_labeled_val]) if x==5] \n",
        "class_6=[i for i,x in enumerate(labels[idx_labeled_val]) if x==6] \n",
        "class_7=[i for i,x in enumerate(labels[idx_labeled_val]) if x==7] \n",
        "class_8=[i for i,x in enumerate(labels[idx_labeled_val]) if x==8] \n",
        "class_9=[i for i,x in enumerate(labels[idx_labeled_val]) if x==9] \n",
        "class_10=[i for i,x in enumerate(labels[idx_labeled_val]) if x==10] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9H3cAsQYWUF"
      },
      "source": [
        "# از هر کلاسی بتوانیم به صورت رندوم سمپل انتخاب کنیم\n",
        "import random\n",
        "Random_idx_val=[]\n",
        "\n",
        "for i in range(N_shot):\n",
        "  Random_idx_val.append(random.choice(class_1))\n",
        "  Random_idx_val.append(random.choice(class_2))\n",
        "  Random_idx_val.append(random.choice(class_3))\n",
        "  Random_idx_val.append(random.choice(class_4))\n",
        "  Random_idx_val.append(random.choice(class_5))\n",
        "  Random_idx_val.append(random.choice(class_6))\n",
        "  Random_idx_val.append(random.choice(class_7))\n",
        "  Random_idx_val.append(random.choice(class_8))\n",
        "  Random_idx_val.append(random.choice(class_9))\n",
        "  Random_idx_val.append(random.choice(class_10))\n",
        "\n",
        "#Random_idx_val#اندیس "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0eRUuhJqYWUH"
      },
      "source": [
        "#اگر بخواهیم از هر کلاس یک سمپل انتخاب کنیم مجموعا 10 سمپل با لیبل داریم \n",
        "#اگر بخواهیم نسبت 20 به 80 را ارعایت کنیم پس لازم است 40 داده بدون لیبل هم انتخاب کنیم\n",
        "\n",
        "hiddenIndex_val=[]\n",
        "\n",
        "for i in range(40*N_shot):\n",
        "  hiddenIndex_val.append(random.choice(idx_unlabeled_val))\n",
        "#hiddenIndex_val "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GGJZ-_e4YWUI",
        "outputId": "ad38fbeb-f235-4300-ae5a-20a5dab68088"
      },
      "source": [
        "labels_val_hidden=torch.hstack((labels[Random_idx_val],labels[hiddenIndex_val]))\n",
        "labels_val_hidden.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([500])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEhEarXCYWUI"
      },
      "source": [
        "labels_val=labels_val_hidden.detach().clone()\n",
        "#با توجه به اینکه از هر کلاس چند تا سمپل انتخاب کردیم از شماره آخری بقیه مربوط به هیدن ها میشود که صفر میذاریم\n",
        "labels_val[10*N_shot:]=0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3F00cnT2YWUJ",
        "outputId": "ea2e6615-a9e4-44b2-df91-c07298458b51"
      },
      "source": [
        "features_val=torch.vstack((features[Random_idx_val],features[hiddenIndex_val]))\n",
        "features_val.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([500, 767])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_oHe-7abYWUK",
        "outputId": "c1640ac9-2de9-4cf3-bbf3-7f9243d19c4c"
      },
      "source": [
        "features_val[0:10*N_shot]==features[Random_idx_val]\n",
        "features_val[10*N_shot:10*N_shot*4+10*N_shot]==features[hiddenIndex_val]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[True, True, True,  ..., True, True, True],\n",
              "        [True, True, True,  ..., True, True, True],\n",
              "        [True, True, True,  ..., True, True, True],\n",
              "        ...,\n",
              "        [True, True, True,  ..., True, True, True],\n",
              "        [True, True, True,  ..., True, True, True],\n",
              "        [True, True, True,  ..., True, True, True]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJhiH9xIYWUL"
      },
      "source": [
        "adj_val=torch.vstack((adj[Random_idx_val],adj[hiddenIndex_val]))\n",
        "\n",
        "for i in hiddenIndex_val :\n",
        "    Random_idx_val.append(i)\n",
        "\n",
        "adj_val=adj_val[0:10*N_shot*4+10*N_shot,Random_idx_val]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rXMvNIZF6BU"
      },
      "source": [
        "# Constructing hidden_labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GL4nZ9-pMmSM"
      },
      "source": [
        "#hidden_labels:\n",
        "#برای استفاده در قسمت اکتیولرنینگ که فقط بر روی هیدن لیبل ها اعمال شود\n",
        "#سایز آن به اندازه تعداد کل سمپل ها می باشد\n",
        "hidden_labels_train=np.zeros((10*N_shot*4+10*N_shot), dtype='int')\n",
        "hidden_labels_train[10*N_shot:10*N_shot*4+10*N_shot]=1\n",
        "#RuntimeError: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead. site:stackoverflow.com\n",
        "#برای همین به تنسور تبدیلش کردم\n",
        "hidden_labels_train=torch.tensor(hidden_labels_train)\n",
        "#hidden_labels_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSwwuzUJFR1n"
      },
      "source": [
        "hidden_labels_test=np.zeros((10*N_shot*4+10*N_shot), dtype='int')\n",
        "hidden_labels_test[10*N_shot:10*N_shot*4+10*N_shot]=1\n",
        "hidden_labels_test=torch.tensor(hidden_labels_test)\n",
        "#hidden_labels_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46SwJE2eFRj4"
      },
      "source": [
        "hidden_labels_val=np.zeros((10*N_shot*4+10*N_shot), dtype='int')\n",
        "hidden_labels_val[10*N_shot:10*N_shot*4+10*N_shot]=1\n",
        "hidden_labels_val=torch.tensor(hidden_labels_val)\n",
        "#hidden_labels_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNJQMW6-a2-h",
        "outputId": "96846f5a-08e5-4551-d1bb-ef68b4012180"
      },
      "source": [
        "features.shape[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13752"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRzfVoOLakwY"
      },
      "source": [
        "hidden_labels=np.zeros((features.shape[0]), dtype='int')\n",
        "hidden_labels=torch.tensor(hidden_labels)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7zL2lQQRV9V",
        "outputId": "59aeed35-dfa1-407c-8846-d9e14b124db7"
      },
      "source": [
        "hidden_labels_val.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([500])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScJWIK7yGDcY"
      },
      "source": [
        "#Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "us3XSHLHDWoP"
      },
      "source": [
        "\n",
        "import math\n",
        "\n",
        "import torch\n",
        "\n",
        "from torch.nn.parameter import Parameter\n",
        "\n",
        "#class parameter:\n",
        "#پارامترها را کش می کند\n",
        "\n",
        "from torch.nn.modules.module import Module\n",
        "\n",
        "\n",
        "class GraphConvolution(Module):\n",
        "    \"\"\"\n",
        "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features ,out_features, bias=True):\n",
        "        super(GraphConvolution, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        \n",
        "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
        "        if bias:\n",
        "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
        "        self.weight.data.uniform_(-stdv, stdv)\n",
        "        if self.bias is not None:\n",
        "            self.bias.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self, input, input_adj):\n",
        "        support = torch.mm(input, self.weight)\n",
        "        #torch.mm=matrix multiplication\n",
        "        #torch.spmm=Sparse matrix multiplication\n",
        "        #ضرب ویژگی ها در ماتریس مجاورت\n",
        "        output = torch.spmm(input_adj, support)\n",
        "        if self.bias is not None:\n",
        "            return output + self.bias\n",
        "        else:\n",
        "            return output\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vTcR4OmGJIR"
      },
      "source": [
        "#Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXDaJHg7DeHl"
      },
      "source": [
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self, nfeat, nhid,nclass, dropout):\n",
        "        super(GCN, self).__init__()\n",
        "\n",
        "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
        "        self.gc2 = GraphConvolution(nhid, nclass)\n",
        "        self.dropout = dropout\n",
        "       \n",
        "        \n",
        "    def forward(self, x, input_adj,hidden_labels,mode):\n",
        "        #if N-shot=10 then samples=500\n",
        "        #features_num=767\n",
        "        #x.shape=500*767\n",
        "        #samples=500,features=767\n",
        "        x = F.relu(self.gc1(x, input_adj))\n",
        "        #x.shape=500*16\n",
        "        #hidden_unit=16\n",
        "        if mode==0:\n",
        "          x = F.dropout(x, self.dropout)\n",
        "        #training=self.training parameter of drop out func\n",
        "        #x.shape=500*16\n",
        "        #hidden_unit=16\n",
        "        #طبق مقاله مربوط به اکتیولرنینگ بعد از لایه اول اکتیو لرنینگ را اجرا می کنیم تا تصمیم گیری شود که لیبل کدام یک از داده های هیدن را اضافه کنیم\n",
        "        if mode not in (-1,0):\n",
        "          x,decision=self.active(x,hidden_labels)\n",
        "        else:\n",
        "          decision=-1  \n",
        "        x = self.gc2(x, input_adj)\n",
        "        #x.shape=500*11\n",
        "        #class_num=11\n",
        "        return F.log_softmax(x,dim=1),x,decision\n",
        "\n",
        "    def active(self, x,hidden):\n",
        "        #x.shape=500*16\n",
        "        x_active = torch.transpose(x, 0, 1) \n",
        "        x_active=x_active.unsqueeze_(0)\n",
        "        #unsqueeze_داده 2 بعدی را به 3 بعدی تبدیل می کند و بعد اول را 1 می کند\n",
        "        #این کار را برای آنکه لایه کانولوشن درست کار کند انجام دادم\n",
        "        #x_active.shape=1*16*500\n",
        "        #x_active.shape[1]=16\n",
        "\n",
        "        conv_active_1 = nn.Conv1d(x_active.shape[1],x_active.shape[1],1)\n",
        "        bn_active = nn.BatchNorm1d(x_active.shape[1])\n",
        "        conv_active_2 = nn.Conv1d(x_active.shape[1],1,1)\n",
        "\n",
        "        x_active = conv_active_1(x_active)\n",
        "        #x_active.shape=1*16*500\n",
        "        x_active = F.leaky_relu(bn_active(x_active))\n",
        "        #x_active.shape=1*16*500\n",
        "        x_active = conv_active_2(x_active)\n",
        "        #x_active.shape=1*1*500 \n",
        "        x_active = torch.transpose(x_active, 0, 1) \n",
        "        x_active = x_active.squeeze(0) \n",
        "        #x_active.shape=1*500 \n",
        "        #squeeze_ داده 3 بعدی را به 2 بعدی تبدیل می کند و مجدداورودی به حالت اول بر می گردد\n",
        "\n",
        "        #x_active = x_active - (1-hidden)*1e8\n",
        "        x_active = F.softmax(x_active)\n",
        "        #x_active.shape=1*500 \n",
        "        \n",
        "        #برای آنکه فقط روی هیدن لیبل ها تصمیم گیری شود، با ضرب زیر مقدار مربوط به سمپل های با لیبل صفر می شود\n",
        "        x_active = x_active*hidden \n",
        "        decision = torch.argmax(x_active)\n",
        "        decision = decision.detach()\n",
        "        return x,int(decision)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYdsEgftGRS8"
      },
      "source": [
        "#Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kAvN1C2QDq89",
        "outputId": "d10f2233-0cca-4b06-d194-8650a8318274"
      },
      "source": [
        "\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "import time\n",
        "import argparse\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "# Training settings\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
        "                    help='Disables CUDA training.')\n",
        "parser.add_argument('--fastmode', action='store_true', default=False,\n",
        "                    help='Validate during training pass.')\n",
        "parser.add_argument('--seed', type=int, default=42, help='Random seed.')\n",
        "parser.add_argument('--epochs', type=int, default=800,\n",
        "                    help='Number of epochs to train.')\n",
        "parser.add_argument('--lr', type=float, default=0.01,\n",
        "                    help='Initial learning rate.')\n",
        "parser.add_argument('--weight_decay', type=float, default=5e-4,\n",
        "                    help='Weight decay (L2 loss on parameters).')\n",
        "parser.add_argument('--hidden', type=int, default=16,\n",
        "                    help='Number of hidden units.')\n",
        "parser.add_argument('--dropout', type=float, default=0.5,\n",
        "                    help='Dropout rate (1 - keep probability).')\n",
        "\n",
        "\n",
        "#args = parser.parse_args()\n",
        "#error midad khate bala,khate paein jaigozin shod\n",
        "\n",
        "args = parser.parse_known_args()[0]\n",
        "\n",
        "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
        "\n",
        "np.random.seed(args.seed)\n",
        "torch.manual_seed(args.seed)\n",
        "if args.cuda:\n",
        "    torch.cuda.manual_seed(args.seed)\n",
        "\n",
        "\n",
        "# Model and optimizer\n",
        "model = GCN(nfeat=features.shape[1],\n",
        "            nhid=args.hidden,\n",
        "            nclass=class_names.size+1,\n",
        "            #بدلیل صفر کردن لیبل هیدن ها تعداد کلاسها یکی بیشتر میشود\n",
        "            dropout=args.dropout\n",
        "            )\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(),\n",
        "                       lr=args.lr, weight_decay=args.weight_decay)\n",
        "\n",
        "# to set cuda as your device if possible\n",
        "#training on  GPU\n",
        "\n",
        "if args.cuda:\n",
        "    model.cuda()\n",
        "    features_test = features_test.cuda()\n",
        "    adj_test = adj_test.cuda()\n",
        "    labels_test = labels_test.cuda()\n",
        "    features_train = features_train.cuda()\n",
        "    adj_train = adj_train.cuda()\n",
        "    labels_train = labels_train.cuda()\n",
        "    features_val = features_val.cuda()\n",
        "    adj_val = adj_val.cuda()\n",
        "    labels_val = labels_val.cuda()\n",
        "    # train:adjust the weights on the neural network\n",
        "    # validation:used to minimize overfitting\n",
        "\n",
        "\n",
        "def train(epoch):\n",
        "  model.train()\n",
        "  optimizer.zero_grad()\n",
        "  output,predicted_output,act_dec = model(features_train,adj_train,hidden_labels_train,mode=0)\n",
        "\n",
        "  #مقدار تصمیم گیری شده در اکتیولرنینگ یک اندیس از هیدن لیبل هاست که بیشترین تاثیر در بهبود خطاهای شبکه را دارد\n",
        "  #(در واقع مقدارش را صفر میکنیم)لیبل آن را بدست آورده و به لیبل ها اضافه می کنیمو از هیدن ها هم آن را حذف میکنیم\n",
        "\n",
        "  Uncover_label=labels_train_hidden[act_dec]\n",
        "  labels_train[act_dec]=Uncover_label\n",
        "  hidden_labels_train[act_dec]=0\n",
        "\n",
        "  loss_train = F.nll_loss(output, labels_train)\n",
        "  acc_train = accuracy(output,labels_train)\n",
        "  # Computing the gradients necessary to adjust the weights\n",
        "  loss_train.backward()\n",
        "  # Updating the weights of the neural network\n",
        "  optimizer.step()\n",
        "  losses.append(loss_train.item())\n",
        "  acc.append(acc_train.item())\n",
        "\n",
        "  if not args.fastmode:\n",
        "    # Evaluate validation set performance separately,\n",
        "    # deactivates dropout during validation run.\n",
        "    model.eval()\n",
        "    output,predicted_output,act_dec_val = model(features_val, adj_val,hidden_labels_val,mode=-1)\n",
        "    \n",
        "  Uncover_label=labels_val_hidden[act_dec_val]\n",
        "  labels_val[act_dec_val]=Uncover_label\n",
        "  hidden_labels_val[act_dec_val]=0\n",
        "  \n",
        "  loss_val = F.nll_loss(output, labels_val)\n",
        "  acc_val = accuracy(output, labels_val)\n",
        "  losses_val.append(loss_val.item())\n",
        "  acc_valid.append(acc_val.item())\n",
        "\n",
        "  print('Epoch: {:04d}'.format(epoch+1),\n",
        "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
        "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
        "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
        "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
        "          'time: {:.4f}s'.format(time.time() - t),\n",
        "          act_dec,act_dec_val)\n",
        "\n",
        "# Train model\n",
        "t_total = time.time()\n",
        "losses = []\n",
        "acc=[]\n",
        "losses_val = []\n",
        "acc_valid=[]\n",
        "t = time.time()\n",
        "for epoch in range(args.epochs):\n",
        "    train(epoch)\n",
        "\n",
        "\n",
        "def test(): \n",
        "    model.eval()\n",
        "  \n",
        "    output,predicted_output,act_dec = model(features[idx_test]\n",
        "                   ,adj[idx_test[0]:int(idx_test[-1])+1,idx_test[0]:int(idx_test[-1])+1],hidden_labels[idx_test],mode=-1)\n",
        "    \n",
        "    #Uncover_label=labels_test_hidden[act_dec]\n",
        "    #labels_test[act_dec]=Uncover_label\n",
        "    #hidden_labels_test[act_dec]=0\n",
        "\n",
        "    loss_test = F.nll_loss(output, labels[idx_test])\n",
        "    acc_test = accuracy(output, labels[idx_test])\n",
        "    print(\"Test set results:\",\n",
        "          \"loss= {:.4f}\".format(loss_test.item()),\n",
        "          \"accuracy= {:.4f}\".format(acc_test.item()),\n",
        "          \"act_dec_test= {:.4f}\".format(act_dec))\n",
        "# Testing\n",
        "test()\n",
        "\n",
        "print(model)\n",
        "\n",
        "#plotting loss_train_val:\n",
        "\n",
        "plt.plot(np.array(losses),label ='loss_train Plot')\n",
        "plt.plot(np.array(losses_val),label ='loss_val Plot')\n",
        "plt.title('loss_train_validation Plot')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('value')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "#plotting acc_train_val:\n",
        "\n",
        "plt.plot(np.array(acc),label ='Accuracy_train Plot')\n",
        "plt.plot(np.array(acc_valid),label ='Accuracy_val Plot')\n",
        "plt.title('acc_train_validation Plot')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('value')\n",
        "plt.legend()\n",
        "plt.show()    \n",
        "\n",
        "print(\"Optimization Finished!\")\n",
        "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0001 loss_train: 2.3388 acc_train: 0.0220 loss_val: 2.2936 acc_val: 0.2080 time: 0.1184s -1 -1\n",
            "Epoch: 0002 loss_train: 2.2948 acc_train: 0.1360 loss_val: 2.2697 acc_val: 0.4540 time: 0.1240s -1 -1\n",
            "Epoch: 0003 loss_train: 2.2741 acc_train: 0.3980 loss_val: 2.2456 acc_val: 0.7960 time: 0.1289s -1 -1\n",
            "Epoch: 0004 loss_train: 2.2304 acc_train: 0.7460 loss_val: 2.2319 acc_val: 0.7980 time: 0.1385s -1 -1\n",
            "Epoch: 0005 loss_train: 2.1898 acc_train: 0.7880 loss_val: 2.2239 acc_val: 0.7980 time: 0.1433s -1 -1\n",
            "Epoch: 0006 loss_train: 2.1351 acc_train: 0.7900 loss_val: 2.2282 acc_val: 0.7980 time: 0.1479s -1 -1\n",
            "Epoch: 0007 loss_train: 2.1132 acc_train: 0.7920 loss_val: 2.2373 acc_val: 0.7980 time: 0.1525s -1 -1\n",
            "Epoch: 0008 loss_train: 2.1017 acc_train: 0.7940 loss_val: 2.2540 acc_val: 0.7980 time: 0.1571s -1 -1\n",
            "Epoch: 0009 loss_train: 2.0496 acc_train: 0.7940 loss_val: 2.2741 acc_val: 0.7980 time: 0.1616s -1 -1\n",
            "Epoch: 0010 loss_train: 1.9922 acc_train: 0.7960 loss_val: 2.2977 acc_val: 0.7980 time: 0.1661s -1 -1\n",
            "Epoch: 0011 loss_train: 2.0061 acc_train: 0.7960 loss_val: 2.3206 acc_val: 0.7980 time: 0.1706s -1 -1\n",
            "Epoch: 0012 loss_train: 1.9457 acc_train: 0.7960 loss_val: 2.3488 acc_val: 0.7980 time: 0.1755s -1 -1\n",
            "Epoch: 0013 loss_train: 1.9675 acc_train: 0.7980 loss_val: 2.3683 acc_val: 0.7980 time: 0.1819s -1 -1\n",
            "Epoch: 0014 loss_train: 1.9417 acc_train: 0.7940 loss_val: 2.3878 acc_val: 0.7980 time: 0.1885s -1 -1\n",
            "Epoch: 0015 loss_train: 1.8931 acc_train: 0.7960 loss_val: 2.4060 acc_val: 0.7980 time: 0.1948s -1 -1\n",
            "Epoch: 0016 loss_train: 1.8913 acc_train: 0.7980 loss_val: 2.4191 acc_val: 0.7980 time: 0.2012s -1 -1\n",
            "Epoch: 0017 loss_train: 1.8866 acc_train: 0.7980 loss_val: 2.4173 acc_val: 0.7980 time: 0.2074s -1 -1\n",
            "Epoch: 0018 loss_train: 1.8753 acc_train: 0.8000 loss_val: 2.4091 acc_val: 0.7980 time: 0.2143s -1 -1\n",
            "Epoch: 0019 loss_train: 1.7777 acc_train: 0.7960 loss_val: 2.4002 acc_val: 0.7980 time: 0.2204s -1 -1\n",
            "Epoch: 0020 loss_train: 1.7792 acc_train: 0.8000 loss_val: 2.3845 acc_val: 0.7980 time: 0.2273s -1 -1\n",
            "Epoch: 0021 loss_train: 1.7767 acc_train: 0.7940 loss_val: 2.3668 acc_val: 0.7980 time: 0.2339s -1 -1\n",
            "Epoch: 0022 loss_train: 1.7245 acc_train: 0.7980 loss_val: 2.3539 acc_val: 0.7980 time: 0.2403s -1 -1\n",
            "Epoch: 0023 loss_train: 1.7716 acc_train: 0.7960 loss_val: 2.3351 acc_val: 0.7980 time: 0.2466s -1 -1\n",
            "Epoch: 0024 loss_train: 1.7309 acc_train: 0.8020 loss_val: 2.3176 acc_val: 0.7980 time: 0.2531s -1 -1\n",
            "Epoch: 0025 loss_train: 1.6540 acc_train: 0.8040 loss_val: 2.3097 acc_val: 0.7980 time: 0.2600s -1 -1\n",
            "Epoch: 0026 loss_train: 1.7104 acc_train: 0.7980 loss_val: 2.2864 acc_val: 0.7980 time: 0.2643s -1 -1\n",
            "Epoch: 0027 loss_train: 1.6569 acc_train: 0.8020 loss_val: 2.2711 acc_val: 0.7980 time: 0.2748s -1 -1\n",
            "Epoch: 0028 loss_train: 1.6373 acc_train: 0.8000 loss_val: 2.2591 acc_val: 0.7980 time: 0.2809s -1 -1\n",
            "Epoch: 0029 loss_train: 1.6313 acc_train: 0.8020 loss_val: 2.2399 acc_val: 0.7980 time: 0.2872s -1 -1\n",
            "Epoch: 0030 loss_train: 1.6295 acc_train: 0.7980 loss_val: 2.2211 acc_val: 0.7980 time: 0.2934s -1 -1\n",
            "Epoch: 0031 loss_train: 1.6055 acc_train: 0.8020 loss_val: 2.2037 acc_val: 0.7980 time: 0.2995s -1 -1\n",
            "Epoch: 0032 loss_train: 1.5722 acc_train: 0.8020 loss_val: 2.1813 acc_val: 0.7980 time: 0.3056s -1 -1\n",
            "Epoch: 0033 loss_train: 1.5781 acc_train: 0.8000 loss_val: 2.1625 acc_val: 0.7980 time: 0.3118s -1 -1\n",
            "Epoch: 0034 loss_train: 1.5541 acc_train: 0.8040 loss_val: 2.1418 acc_val: 0.7980 time: 0.3181s -1 -1\n",
            "Epoch: 0035 loss_train: 1.5725 acc_train: 0.8040 loss_val: 2.1208 acc_val: 0.7960 time: 0.3363s -1 -1\n",
            "Epoch: 0036 loss_train: 1.5329 acc_train: 0.8000 loss_val: 2.1031 acc_val: 0.7960 time: 0.3441s -1 -1\n",
            "Epoch: 0037 loss_train: 1.4881 acc_train: 0.8040 loss_val: 2.0826 acc_val: 0.7960 time: 0.3511s -1 -1\n",
            "Epoch: 0038 loss_train: 1.5160 acc_train: 0.7980 loss_val: 2.0613 acc_val: 0.7960 time: 0.3578s -1 -1\n",
            "Epoch: 0039 loss_train: 1.5076 acc_train: 0.8040 loss_val: 2.0211 acc_val: 0.7960 time: 0.3643s -1 -1\n",
            "Epoch: 0040 loss_train: 1.4869 acc_train: 0.8000 loss_val: 1.9823 acc_val: 0.7960 time: 0.3716s -1 -1\n",
            "Epoch: 0041 loss_train: 1.4634 acc_train: 0.8020 loss_val: 1.9468 acc_val: 0.7960 time: 0.3782s -1 -1\n",
            "Epoch: 0042 loss_train: 1.4638 acc_train: 0.8040 loss_val: 1.9103 acc_val: 0.7960 time: 0.3850s -1 -1\n",
            "Epoch: 0043 loss_train: 1.4251 acc_train: 0.8060 loss_val: 1.8950 acc_val: 0.7960 time: 0.3930s -1 -1\n",
            "Epoch: 0044 loss_train: 1.4351 acc_train: 0.8060 loss_val: 1.8926 acc_val: 0.8000 time: 0.4004s -1 -1\n",
            "Epoch: 0045 loss_train: 1.3972 acc_train: 0.8040 loss_val: 1.9061 acc_val: 0.7960 time: 0.4071s -1 -1\n",
            "Epoch: 0046 loss_train: 1.3859 acc_train: 0.8060 loss_val: 1.9045 acc_val: 0.7960 time: 0.4136s -1 -1\n",
            "Epoch: 0047 loss_train: 1.3783 acc_train: 0.8040 loss_val: 1.8906 acc_val: 0.8000 time: 0.4202s -1 -1\n",
            "Epoch: 0048 loss_train: 1.3726 acc_train: 0.8040 loss_val: 1.8807 acc_val: 0.8000 time: 0.4268s -1 -1\n",
            "Epoch: 0049 loss_train: 1.3179 acc_train: 0.8060 loss_val: 1.8784 acc_val: 0.8000 time: 0.4335s -1 -1\n",
            "Epoch: 0050 loss_train: 1.2960 acc_train: 0.8080 loss_val: 1.8835 acc_val: 0.8000 time: 0.4401s -1 -1\n",
            "Epoch: 0051 loss_train: 1.3455 acc_train: 0.7980 loss_val: 1.8656 acc_val: 0.8000 time: 0.4468s -1 -1\n",
            "Epoch: 0052 loss_train: 1.3151 acc_train: 0.8040 loss_val: 1.8474 acc_val: 0.8000 time: 0.4535s -1 -1\n",
            "Epoch: 0053 loss_train: 1.2913 acc_train: 0.8040 loss_val: 1.8381 acc_val: 0.8000 time: 0.4622s -1 -1\n",
            "Epoch: 0054 loss_train: 1.2957 acc_train: 0.8060 loss_val: 1.8254 acc_val: 0.8000 time: 0.4794s -1 -1\n",
            "Epoch: 0055 loss_train: 1.2550 acc_train: 0.8040 loss_val: 1.8171 acc_val: 0.8000 time: 0.4868s -1 -1\n",
            "Epoch: 0056 loss_train: 1.2844 acc_train: 0.8020 loss_val: 1.8069 acc_val: 0.8000 time: 0.4938s -1 -1\n",
            "Epoch: 0057 loss_train: 1.2594 acc_train: 0.8040 loss_val: 1.7846 acc_val: 0.8000 time: 0.5026s -1 -1\n",
            "Epoch: 0058 loss_train: 1.2243 acc_train: 0.8100 loss_val: 1.7738 acc_val: 0.8000 time: 0.5089s -1 -1\n",
            "Epoch: 0059 loss_train: 1.2397 acc_train: 0.8060 loss_val: 1.7613 acc_val: 0.8000 time: 0.5151s -1 -1\n",
            "Epoch: 0060 loss_train: 1.2632 acc_train: 0.8040 loss_val: 1.7452 acc_val: 0.8000 time: 0.5212s -1 -1\n",
            "Epoch: 0061 loss_train: 1.2298 acc_train: 0.8080 loss_val: 1.7371 acc_val: 0.8000 time: 0.5277s -1 -1\n",
            "Epoch: 0062 loss_train: 1.2211 acc_train: 0.8000 loss_val: 1.7370 acc_val: 0.8000 time: 0.5341s -1 -1\n",
            "Epoch: 0063 loss_train: 1.1875 acc_train: 0.8080 loss_val: 1.7325 acc_val: 0.8000 time: 0.5443s -1 -1\n",
            "Epoch: 0064 loss_train: 1.1847 acc_train: 0.8020 loss_val: 1.7191 acc_val: 0.8000 time: 0.5507s -1 -1\n",
            "Epoch: 0065 loss_train: 1.1871 acc_train: 0.8040 loss_val: 1.6851 acc_val: 0.8000 time: 0.5570s -1 -1\n",
            "Epoch: 0066 loss_train: 1.1639 acc_train: 0.8080 loss_val: 1.6418 acc_val: 0.8000 time: 0.5632s -1 -1\n",
            "Epoch: 0067 loss_train: 1.1819 acc_train: 0.8040 loss_val: 1.6017 acc_val: 0.8000 time: 0.5692s -1 -1\n",
            "Epoch: 0068 loss_train: 1.1364 acc_train: 0.8080 loss_val: 1.5670 acc_val: 0.8000 time: 0.5754s -1 -1\n",
            "Epoch: 0069 loss_train: 1.1661 acc_train: 0.8000 loss_val: 1.5379 acc_val: 0.8000 time: 0.5819s -1 -1\n",
            "Epoch: 0070 loss_train: 1.1319 acc_train: 0.8080 loss_val: 1.5200 acc_val: 0.8000 time: 0.5886s -1 -1\n",
            "Epoch: 0071 loss_train: 1.1481 acc_train: 0.8080 loss_val: 1.4979 acc_val: 0.8000 time: 0.5953s -1 -1\n",
            "Epoch: 0072 loss_train: 1.1448 acc_train: 0.8060 loss_val: 1.4906 acc_val: 0.8000 time: 0.6020s -1 -1\n",
            "Epoch: 0073 loss_train: 1.1147 acc_train: 0.8060 loss_val: 1.4879 acc_val: 0.8000 time: 0.6083s -1 -1\n",
            "Epoch: 0074 loss_train: 1.1342 acc_train: 0.8040 loss_val: 1.4930 acc_val: 0.8000 time: 0.6148s -1 -1\n",
            "Epoch: 0075 loss_train: 1.1104 acc_train: 0.8020 loss_val: 1.5117 acc_val: 0.8000 time: 0.6213s -1 -1\n",
            "Epoch: 0076 loss_train: 1.1188 acc_train: 0.8000 loss_val: 1.5284 acc_val: 0.8000 time: 0.6278s -1 -1\n",
            "Epoch: 0077 loss_train: 1.1029 acc_train: 0.8020 loss_val: 1.5502 acc_val: 0.8000 time: 0.6345s -1 -1\n",
            "Epoch: 0078 loss_train: 1.0915 acc_train: 0.8040 loss_val: 1.5646 acc_val: 0.8000 time: 0.6409s -1 -1\n",
            "Epoch: 0079 loss_train: 1.0893 acc_train: 0.8080 loss_val: 1.5473 acc_val: 0.8000 time: 0.6475s -1 -1\n",
            "Epoch: 0080 loss_train: 1.0909 acc_train: 0.8040 loss_val: 1.5187 acc_val: 0.8000 time: 0.6546s -1 -1\n",
            "Epoch: 0081 loss_train: 1.0745 acc_train: 0.8040 loss_val: 1.4744 acc_val: 0.8000 time: 0.6612s -1 -1\n",
            "Epoch: 0082 loss_train: 1.0663 acc_train: 0.8060 loss_val: 1.4269 acc_val: 0.8000 time: 0.6678s -1 -1\n",
            "Epoch: 0083 loss_train: 1.0663 acc_train: 0.8040 loss_val: 1.3895 acc_val: 0.8000 time: 0.6744s -1 -1\n",
            "Epoch: 0084 loss_train: 1.0479 acc_train: 0.8040 loss_val: 1.3778 acc_val: 0.8000 time: 0.6811s -1 -1\n",
            "Epoch: 0085 loss_train: 1.0525 acc_train: 0.8020 loss_val: 1.3822 acc_val: 0.8000 time: 0.6888s -1 -1\n",
            "Epoch: 0086 loss_train: 1.0523 acc_train: 0.8060 loss_val: 1.3947 acc_val: 0.8000 time: 0.6966s -1 -1\n",
            "Epoch: 0087 loss_train: 1.0558 acc_train: 0.8060 loss_val: 1.4114 acc_val: 0.8000 time: 0.7029s -1 -1\n",
            "Epoch: 0088 loss_train: 1.0413 acc_train: 0.8040 loss_val: 1.4285 acc_val: 0.8000 time: 0.7089s -1 -1\n",
            "Epoch: 0089 loss_train: 1.0526 acc_train: 0.8000 loss_val: 1.4439 acc_val: 0.8000 time: 0.7153s -1 -1\n",
            "Epoch: 0090 loss_train: 1.0351 acc_train: 0.8020 loss_val: 1.4339 acc_val: 0.8000 time: 0.7214s -1 -1\n",
            "Epoch: 0091 loss_train: 1.0096 acc_train: 0.8040 loss_val: 1.4100 acc_val: 0.8000 time: 0.7276s -1 -1\n",
            "Epoch: 0092 loss_train: 1.0088 acc_train: 0.8020 loss_val: 1.3955 acc_val: 0.8000 time: 0.7353s -1 -1\n",
            "Epoch: 0093 loss_train: 1.0138 acc_train: 0.8040 loss_val: 1.3877 acc_val: 0.8120 time: 0.7412s -1 -1\n",
            "Epoch: 0094 loss_train: 1.0063 acc_train: 0.8040 loss_val: 1.3867 acc_val: 0.8120 time: 0.7515s -1 -1\n",
            "Epoch: 0095 loss_train: 0.9832 acc_train: 0.8060 loss_val: 1.3909 acc_val: 0.8120 time: 0.7579s -1 -1\n",
            "Epoch: 0096 loss_train: 0.9981 acc_train: 0.8080 loss_val: 1.3735 acc_val: 0.8120 time: 0.7640s -1 -1\n",
            "Epoch: 0097 loss_train: 0.9931 acc_train: 0.8080 loss_val: 1.3698 acc_val: 0.8000 time: 0.7701s -1 -1\n",
            "Epoch: 0098 loss_train: 0.9969 acc_train: 0.8040 loss_val: 1.3575 acc_val: 0.8120 time: 0.7778s -1 -1\n",
            "Epoch: 0099 loss_train: 1.0034 acc_train: 0.8020 loss_val: 1.3503 acc_val: 0.8100 time: 0.7849s -1 -1\n",
            "Epoch: 0100 loss_train: 0.9833 acc_train: 0.8040 loss_val: 1.3532 acc_val: 0.7980 time: 0.7912s -1 -1\n",
            "Epoch: 0101 loss_train: 0.9754 acc_train: 0.8060 loss_val: 1.3542 acc_val: 0.7980 time: 0.7972s -1 -1\n",
            "Epoch: 0102 loss_train: 0.9980 acc_train: 0.8000 loss_val: 1.3498 acc_val: 0.7980 time: 0.8034s -1 -1\n",
            "Epoch: 0103 loss_train: 0.9737 acc_train: 0.8080 loss_val: 1.3443 acc_val: 0.7980 time: 0.8097s -1 -1\n",
            "Epoch: 0104 loss_train: 0.9720 acc_train: 0.8060 loss_val: 1.3417 acc_val: 0.7980 time: 0.8161s -1 -1\n",
            "Epoch: 0105 loss_train: 0.9702 acc_train: 0.8040 loss_val: 1.3349 acc_val: 0.8100 time: 0.8223s -1 -1\n",
            "Epoch: 0106 loss_train: 0.9825 acc_train: 0.8020 loss_val: 1.3268 acc_val: 0.8100 time: 0.8284s -1 -1\n",
            "Epoch: 0107 loss_train: 0.9662 acc_train: 0.8060 loss_val: 1.3180 acc_val: 0.8080 time: 0.8368s -1 -1\n",
            "Epoch: 0108 loss_train: 0.9706 acc_train: 0.8020 loss_val: 1.3149 acc_val: 0.8080 time: 0.8591s -1 -1\n",
            "Epoch: 0109 loss_train: 0.9461 acc_train: 0.8080 loss_val: 1.3190 acc_val: 0.8080 time: 0.8681s -1 -1\n",
            "Epoch: 0110 loss_train: 0.9695 acc_train: 0.8060 loss_val: 1.3224 acc_val: 0.8100 time: 0.8754s -1 -1\n",
            "Epoch: 0111 loss_train: 0.9578 acc_train: 0.8040 loss_val: 1.3091 acc_val: 0.8080 time: 0.8817s -1 -1\n",
            "Epoch: 0112 loss_train: 0.9538 acc_train: 0.8080 loss_val: 1.2965 acc_val: 0.8080 time: 0.8879s -1 -1\n",
            "Epoch: 0113 loss_train: 0.9497 acc_train: 0.8060 loss_val: 1.2768 acc_val: 0.8080 time: 0.8943s -1 -1\n",
            "Epoch: 0114 loss_train: 0.9613 acc_train: 0.8040 loss_val: 1.2631 acc_val: 0.8080 time: 0.9005s -1 -1\n",
            "Epoch: 0115 loss_train: 0.9578 acc_train: 0.8060 loss_val: 1.2561 acc_val: 0.8080 time: 0.9072s -1 -1\n",
            "Epoch: 0116 loss_train: 0.9398 acc_train: 0.8060 loss_val: 1.2575 acc_val: 0.8080 time: 0.9132s -1 -1\n",
            "Epoch: 0117 loss_train: 0.9299 acc_train: 0.8100 loss_val: 1.2688 acc_val: 0.8080 time: 0.9195s -1 -1\n",
            "Epoch: 0118 loss_train: 0.9467 acc_train: 0.8060 loss_val: 1.2769 acc_val: 0.8080 time: 0.9255s -1 -1\n",
            "Epoch: 0119 loss_train: 0.9388 acc_train: 0.8060 loss_val: 1.2884 acc_val: 0.8080 time: 0.9320s -1 -1\n",
            "Epoch: 0120 loss_train: 0.9568 acc_train: 0.8020 loss_val: 1.2981 acc_val: 0.8080 time: 0.9381s -1 -1\n",
            "Epoch: 0121 loss_train: 0.9257 acc_train: 0.8080 loss_val: 1.3092 acc_val: 0.8080 time: 0.9443s -1 -1\n",
            "Epoch: 0122 loss_train: 0.9246 acc_train: 0.8040 loss_val: 1.3160 acc_val: 0.8080 time: 0.9505s -1 -1\n",
            "Epoch: 0123 loss_train: 0.9225 acc_train: 0.8060 loss_val: 1.3129 acc_val: 0.7960 time: 0.9595s -1 -1\n",
            "Epoch: 0124 loss_train: 0.9497 acc_train: 0.8020 loss_val: 1.2988 acc_val: 0.7960 time: 0.9660s -1 -1\n",
            "Epoch: 0125 loss_train: 0.9172 acc_train: 0.8080 loss_val: 1.2813 acc_val: 0.7960 time: 0.9725s -1 -1\n",
            "Epoch: 0126 loss_train: 0.9142 acc_train: 0.8080 loss_val: 1.2800 acc_val: 0.7940 time: 0.9787s -1 -1\n",
            "Epoch: 0127 loss_train: 0.9139 acc_train: 0.8040 loss_val: 1.2792 acc_val: 0.7940 time: 0.9849s -1 -1\n",
            "Epoch: 0128 loss_train: 0.9173 acc_train: 0.8060 loss_val: 1.2740 acc_val: 0.7940 time: 0.9913s -1 -1\n",
            "Epoch: 0129 loss_train: 0.9226 acc_train: 0.8040 loss_val: 1.2696 acc_val: 0.7940 time: 0.9976s -1 -1\n",
            "Epoch: 0130 loss_train: 0.9200 acc_train: 0.8040 loss_val: 1.2810 acc_val: 0.8080 time: 1.0038s -1 -1\n",
            "Epoch: 0131 loss_train: 0.9095 acc_train: 0.8060 loss_val: 1.2896 acc_val: 0.8080 time: 1.0100s -1 -1\n",
            "Epoch: 0132 loss_train: 0.9270 acc_train: 0.8060 loss_val: 1.2890 acc_val: 0.8080 time: 1.0162s -1 -1\n",
            "Epoch: 0133 loss_train: 0.9077 acc_train: 0.8080 loss_val: 1.2851 acc_val: 0.8080 time: 1.0226s -1 -1\n",
            "Epoch: 0134 loss_train: 0.9059 acc_train: 0.8080 loss_val: 1.2696 acc_val: 0.8080 time: 1.0287s -1 -1\n",
            "Epoch: 0135 loss_train: 0.9095 acc_train: 0.8040 loss_val: 1.2561 acc_val: 0.8080 time: 1.0353s -1 -1\n",
            "Epoch: 0136 loss_train: 0.9195 acc_train: 0.8040 loss_val: 1.2515 acc_val: 0.8060 time: 1.0417s -1 -1\n",
            "Epoch: 0137 loss_train: 0.8949 acc_train: 0.8040 loss_val: 1.2489 acc_val: 0.8060 time: 1.0480s -1 -1\n",
            "Epoch: 0138 loss_train: 0.8978 acc_train: 0.8060 loss_val: 1.2541 acc_val: 0.8080 time: 1.0543s -1 -1\n",
            "Epoch: 0139 loss_train: 0.8867 acc_train: 0.8100 loss_val: 1.2651 acc_val: 0.8080 time: 1.0604s -1 -1\n",
            "Epoch: 0140 loss_train: 0.9053 acc_train: 0.8100 loss_val: 1.2644 acc_val: 0.7960 time: 1.0669s -1 -1\n",
            "Epoch: 0141 loss_train: 0.9070 acc_train: 0.8060 loss_val: 1.2643 acc_val: 0.7940 time: 1.0731s -1 -1\n",
            "Epoch: 0142 loss_train: 0.8922 acc_train: 0.8060 loss_val: 1.2721 acc_val: 0.7940 time: 1.0793s -1 -1\n",
            "Epoch: 0143 loss_train: 0.9007 acc_train: 0.8080 loss_val: 1.2756 acc_val: 0.7940 time: 1.0854s -1 -1\n",
            "Epoch: 0144 loss_train: 0.9019 acc_train: 0.8060 loss_val: 1.2565 acc_val: 0.8080 time: 1.0916s -1 -1\n",
            "Epoch: 0145 loss_train: 0.8962 acc_train: 0.8040 loss_val: 1.2555 acc_val: 0.8080 time: 1.0977s -1 -1\n",
            "Epoch: 0146 loss_train: 0.8874 acc_train: 0.8080 loss_val: 1.2541 acc_val: 0.8080 time: 1.1069s -1 -1\n",
            "Epoch: 0147 loss_train: 0.8868 acc_train: 0.8100 loss_val: 1.2427 acc_val: 0.8080 time: 1.1139s -1 -1\n",
            "Epoch: 0148 loss_train: 0.8885 acc_train: 0.8060 loss_val: 1.2253 acc_val: 0.8080 time: 1.1221s -1 -1\n",
            "Epoch: 0149 loss_train: 0.8838 acc_train: 0.8060 loss_val: 1.2166 acc_val: 0.8060 time: 1.1305s -1 -1\n",
            "Epoch: 0150 loss_train: 0.8854 acc_train: 0.8100 loss_val: 1.2198 acc_val: 0.8060 time: 1.1356s -1 -1\n",
            "Epoch: 0151 loss_train: 0.8712 acc_train: 0.8100 loss_val: 1.2257 acc_val: 0.8040 time: 1.1462s -1 -1\n",
            "Epoch: 0152 loss_train: 0.8806 acc_train: 0.8080 loss_val: 1.2344 acc_val: 0.8040 time: 1.1525s -1 -1\n",
            "Epoch: 0153 loss_train: 0.9003 acc_train: 0.8040 loss_val: 1.2467 acc_val: 0.8040 time: 1.1589s -1 -1\n",
            "Epoch: 0154 loss_train: 0.8881 acc_train: 0.8040 loss_val: 1.2591 acc_val: 0.8060 time: 1.1688s -1 -1\n",
            "Epoch: 0155 loss_train: 0.8841 acc_train: 0.8020 loss_val: 1.2712 acc_val: 0.8060 time: 1.1758s -1 -1\n",
            "Epoch: 0156 loss_train: 0.8724 acc_train: 0.8100 loss_val: 1.2816 acc_val: 0.8060 time: 1.1827s -1 -1\n",
            "Epoch: 0157 loss_train: 0.8876 acc_train: 0.8080 loss_val: 1.2826 acc_val: 0.8060 time: 1.1898s -1 -1\n",
            "Epoch: 0158 loss_train: 0.8870 acc_train: 0.8020 loss_val: 1.2788 acc_val: 0.8040 time: 1.1965s -1 -1\n",
            "Epoch: 0159 loss_train: 0.8849 acc_train: 0.8080 loss_val: 1.2622 acc_val: 0.8020 time: 1.2030s -1 -1\n",
            "Epoch: 0160 loss_train: 0.8707 acc_train: 0.8080 loss_val: 1.2490 acc_val: 0.8020 time: 1.2097s -1 -1\n",
            "Epoch: 0161 loss_train: 0.8668 acc_train: 0.8020 loss_val: 1.2565 acc_val: 0.7960 time: 1.2161s -1 -1\n",
            "Epoch: 0162 loss_train: 0.8675 acc_train: 0.8100 loss_val: 1.2548 acc_val: 0.7960 time: 1.2238s -1 -1\n",
            "Epoch: 0163 loss_train: 0.8591 acc_train: 0.8100 loss_val: 1.2329 acc_val: 0.8060 time: 1.2312s -1 -1\n",
            "Epoch: 0164 loss_train: 0.8892 acc_train: 0.8000 loss_val: 1.2093 acc_val: 0.8080 time: 1.2378s -1 -1\n",
            "Epoch: 0165 loss_train: 0.8817 acc_train: 0.8080 loss_val: 1.2027 acc_val: 0.8080 time: 1.2452s -1 -1\n",
            "Epoch: 0166 loss_train: 0.8900 acc_train: 0.8000 loss_val: 1.1933 acc_val: 0.8060 time: 1.2518s -1 -1\n",
            "Epoch: 0167 loss_train: 0.8739 acc_train: 0.8060 loss_val: 1.1993 acc_val: 0.8060 time: 1.2583s -1 -1\n",
            "Epoch: 0168 loss_train: 0.8650 acc_train: 0.8080 loss_val: 1.2201 acc_val: 0.7980 time: 1.2651s -1 -1\n",
            "Epoch: 0169 loss_train: 0.8624 acc_train: 0.8060 loss_val: 1.2535 acc_val: 0.7980 time: 1.2717s -1 -1\n",
            "Epoch: 0170 loss_train: 0.8736 acc_train: 0.8040 loss_val: 1.2765 acc_val: 0.7980 time: 1.2787s -1 -1\n",
            "Epoch: 0171 loss_train: 0.8661 acc_train: 0.8040 loss_val: 1.3203 acc_val: 0.7980 time: 1.2853s -1 -1\n",
            "Epoch: 0172 loss_train: 0.8724 acc_train: 0.8040 loss_val: 1.3704 acc_val: 0.7980 time: 1.2918s -1 -1\n",
            "Epoch: 0173 loss_train: 0.8718 acc_train: 0.8060 loss_val: 1.4777 acc_val: 0.7980 time: 1.2980s -1 -1\n",
            "Epoch: 0174 loss_train: 0.8610 acc_train: 0.8080 loss_val: 1.5473 acc_val: 0.7980 time: 1.3042s -1 -1\n",
            "Epoch: 0175 loss_train: 0.8655 acc_train: 0.8060 loss_val: 1.6274 acc_val: 0.7960 time: 1.3105s -1 -1\n",
            "Epoch: 0176 loss_train: 0.8709 acc_train: 0.8060 loss_val: 1.5794 acc_val: 0.7960 time: 1.3167s -1 -1\n",
            "Epoch: 0177 loss_train: 0.8408 acc_train: 0.8120 loss_val: 1.5044 acc_val: 0.7960 time: 1.3230s -1 -1\n",
            "Epoch: 0178 loss_train: 0.8600 acc_train: 0.8080 loss_val: 1.4512 acc_val: 0.7980 time: 1.3292s -1 -1\n",
            "Epoch: 0179 loss_train: 0.8609 acc_train: 0.8120 loss_val: 1.4199 acc_val: 0.7980 time: 1.3378s -1 -1\n",
            "Epoch: 0180 loss_train: 0.8636 acc_train: 0.8060 loss_val: 1.4530 acc_val: 0.7960 time: 1.3459s -1 -1\n",
            "Epoch: 0181 loss_train: 0.8647 acc_train: 0.8040 loss_val: 1.5435 acc_val: 0.7960 time: 1.3528s -1 -1\n",
            "Epoch: 0182 loss_train: 0.8726 acc_train: 0.8060 loss_val: 1.6042 acc_val: 0.7960 time: 1.3590s -1 -1\n",
            "Epoch: 0183 loss_train: 0.8639 acc_train: 0.8040 loss_val: 1.5735 acc_val: 0.7960 time: 1.3651s -1 -1\n",
            "Epoch: 0184 loss_train: 0.8585 acc_train: 0.8060 loss_val: 1.5692 acc_val: 0.7960 time: 1.3729s -1 -1\n",
            "Epoch: 0185 loss_train: 0.8534 acc_train: 0.8100 loss_val: 1.5048 acc_val: 0.7980 time: 1.3821s -1 -1\n",
            "Epoch: 0186 loss_train: 0.8389 acc_train: 0.8080 loss_val: 1.5093 acc_val: 0.7980 time: 1.3921s -1 -1\n",
            "Epoch: 0187 loss_train: 0.8596 acc_train: 0.8080 loss_val: 1.5150 acc_val: 0.7980 time: 1.3985s -1 -1\n",
            "Epoch: 0188 loss_train: 0.8402 acc_train: 0.8080 loss_val: 1.5438 acc_val: 0.7960 time: 1.4050s -1 -1\n",
            "Epoch: 0189 loss_train: 0.8471 acc_train: 0.8080 loss_val: 1.5969 acc_val: 0.7960 time: 1.4112s -1 -1\n",
            "Epoch: 0190 loss_train: 0.8582 acc_train: 0.8080 loss_val: 1.6477 acc_val: 0.7960 time: 1.4176s -1 -1\n",
            "Epoch: 0191 loss_train: 0.8533 acc_train: 0.8120 loss_val: 1.7412 acc_val: 0.7960 time: 1.4244s -1 -1\n",
            "Epoch: 0192 loss_train: 0.8476 acc_train: 0.8100 loss_val: 1.7119 acc_val: 0.7960 time: 1.4317s -1 -1\n",
            "Epoch: 0193 loss_train: 0.8382 acc_train: 0.8100 loss_val: 1.7007 acc_val: 0.7960 time: 1.4424s -1 -1\n",
            "Epoch: 0194 loss_train: 0.8498 acc_train: 0.8080 loss_val: 1.6376 acc_val: 0.7980 time: 1.4488s -1 -1\n",
            "Epoch: 0195 loss_train: 0.8356 acc_train: 0.8120 loss_val: 1.6041 acc_val: 0.7980 time: 1.4552s -1 -1\n",
            "Epoch: 0196 loss_train: 0.8567 acc_train: 0.8080 loss_val: 1.6379 acc_val: 0.7960 time: 1.4615s -1 -1\n",
            "Epoch: 0197 loss_train: 0.8392 acc_train: 0.8100 loss_val: 1.6744 acc_val: 0.7900 time: 1.4678s -1 -1\n",
            "Epoch: 0198 loss_train: 0.8472 acc_train: 0.8060 loss_val: 1.7267 acc_val: 0.7880 time: 1.4742s -1 -1\n",
            "Epoch: 0199 loss_train: 0.8609 acc_train: 0.8060 loss_val: 1.7743 acc_val: 0.7880 time: 1.4808s -1 -1\n",
            "Epoch: 0200 loss_train: 0.8614 acc_train: 0.8080 loss_val: 1.7614 acc_val: 0.7900 time: 1.4914s -1 -1\n",
            "Epoch: 0201 loss_train: 0.8632 acc_train: 0.8040 loss_val: 1.7478 acc_val: 0.7960 time: 1.4987s -1 -1\n",
            "Epoch: 0202 loss_train: 0.8426 acc_train: 0.8140 loss_val: 1.7322 acc_val: 0.7960 time: 1.5066s -1 -1\n",
            "Epoch: 0203 loss_train: 0.8394 acc_train: 0.8080 loss_val: 1.6273 acc_val: 0.7980 time: 1.5151s -1 -1\n",
            "Epoch: 0204 loss_train: 0.8568 acc_train: 0.8060 loss_val: 1.7132 acc_val: 0.7960 time: 1.5212s -1 -1\n",
            "Epoch: 0205 loss_train: 0.8469 acc_train: 0.8060 loss_val: 1.7879 acc_val: 0.7960 time: 1.5258s -1 -1\n",
            "Epoch: 0206 loss_train: 0.8401 acc_train: 0.8080 loss_val: 1.8388 acc_val: 0.7900 time: 1.5361s -1 -1\n",
            "Epoch: 0207 loss_train: 0.8453 acc_train: 0.8100 loss_val: 1.8582 acc_val: 0.7900 time: 1.5448s -1 -1\n",
            "Epoch: 0208 loss_train: 0.8511 acc_train: 0.8060 loss_val: 1.7856 acc_val: 0.7900 time: 1.5548s -1 -1\n",
            "Epoch: 0209 loss_train: 0.8328 acc_train: 0.8080 loss_val: 1.7250 acc_val: 0.7960 time: 1.5695s -1 -1\n",
            "Epoch: 0210 loss_train: 0.8471 acc_train: 0.8040 loss_val: 1.7249 acc_val: 0.7960 time: 1.5887s -1 -1\n",
            "Epoch: 0211 loss_train: 0.8258 acc_train: 0.8100 loss_val: 1.6842 acc_val: 0.7960 time: 1.6022s -1 -1\n",
            "Epoch: 0212 loss_train: 0.8433 acc_train: 0.8060 loss_val: 1.6723 acc_val: 0.7960 time: 1.6085s -1 -1\n",
            "Epoch: 0213 loss_train: 0.8497 acc_train: 0.8040 loss_val: 1.7413 acc_val: 0.7960 time: 1.6150s -1 -1\n",
            "Epoch: 0214 loss_train: 0.8455 acc_train: 0.8060 loss_val: 1.7957 acc_val: 0.7960 time: 1.6217s -1 -1\n",
            "Epoch: 0215 loss_train: 0.8446 acc_train: 0.8060 loss_val: 1.7859 acc_val: 0.7900 time: 1.6348s -1 -1\n",
            "Epoch: 0216 loss_train: 0.8504 acc_train: 0.8060 loss_val: 1.7835 acc_val: 0.7900 time: 1.6411s -1 -1\n",
            "Epoch: 0217 loss_train: 0.8321 acc_train: 0.8080 loss_val: 1.7450 acc_val: 0.7900 time: 1.6475s -1 -1\n",
            "Epoch: 0218 loss_train: 0.8416 acc_train: 0.8040 loss_val: 1.6962 acc_val: 0.7900 time: 1.6537s -1 -1\n",
            "Epoch: 0219 loss_train: 0.8539 acc_train: 0.8040 loss_val: 1.6730 acc_val: 0.7900 time: 1.6599s -1 -1\n",
            "Epoch: 0220 loss_train: 0.8438 acc_train: 0.8060 loss_val: 1.6159 acc_val: 0.7960 time: 1.6660s -1 -1\n",
            "Epoch: 0221 loss_train: 0.8208 acc_train: 0.8080 loss_val: 1.6058 acc_val: 0.7960 time: 1.6722s -1 -1\n",
            "Epoch: 0222 loss_train: 0.8358 acc_train: 0.8060 loss_val: 1.6558 acc_val: 0.7960 time: 1.6786s -1 -1\n",
            "Epoch: 0223 loss_train: 0.8397 acc_train: 0.8060 loss_val: 1.6957 acc_val: 0.7900 time: 1.6847s -1 -1\n",
            "Epoch: 0224 loss_train: 0.8302 acc_train: 0.8120 loss_val: 1.7467 acc_val: 0.7900 time: 1.6908s -1 -1\n",
            "Epoch: 0225 loss_train: 0.8351 acc_train: 0.8080 loss_val: 1.6933 acc_val: 0.7900 time: 1.6971s -1 -1\n",
            "Epoch: 0226 loss_train: 0.8404 acc_train: 0.8080 loss_val: 1.6341 acc_val: 0.7920 time: 1.7034s -1 -1\n",
            "Epoch: 0227 loss_train: 0.8433 acc_train: 0.8080 loss_val: 1.6337 acc_val: 0.7920 time: 1.7097s -1 -1\n",
            "Epoch: 0228 loss_train: 0.8390 acc_train: 0.8100 loss_val: 1.6711 acc_val: 0.7920 time: 1.7160s -1 -1\n",
            "Epoch: 0229 loss_train: 0.8264 acc_train: 0.8080 loss_val: 1.7138 acc_val: 0.7920 time: 1.7222s -1 -1\n",
            "Epoch: 0230 loss_train: 0.8257 acc_train: 0.8080 loss_val: 1.7652 acc_val: 0.7900 time: 1.7285s -1 -1\n",
            "Epoch: 0231 loss_train: 0.8360 acc_train: 0.8100 loss_val: 1.8178 acc_val: 0.7900 time: 1.7347s -1 -1\n",
            "Epoch: 0232 loss_train: 0.8302 acc_train: 0.8120 loss_val: 1.8560 acc_val: 0.7900 time: 1.7409s -1 -1\n",
            "Epoch: 0233 loss_train: 0.8419 acc_train: 0.8100 loss_val: 1.8378 acc_val: 0.7900 time: 1.7470s -1 -1\n",
            "Epoch: 0234 loss_train: 0.8263 acc_train: 0.8100 loss_val: 1.8323 acc_val: 0.7900 time: 1.7533s -1 -1\n",
            "Epoch: 0235 loss_train: 0.8209 acc_train: 0.8100 loss_val: 1.7762 acc_val: 0.7900 time: 1.7600s -1 -1\n",
            "Epoch: 0236 loss_train: 0.8328 acc_train: 0.8100 loss_val: 1.7438 acc_val: 0.7900 time: 1.7662s -1 -1\n",
            "Epoch: 0237 loss_train: 0.8141 acc_train: 0.8080 loss_val: 1.7124 acc_val: 0.7900 time: 1.7723s -1 -1\n",
            "Epoch: 0238 loss_train: 0.8317 acc_train: 0.8100 loss_val: 1.7276 acc_val: 0.7900 time: 1.7784s -1 -1\n",
            "Epoch: 0239 loss_train: 0.8323 acc_train: 0.8080 loss_val: 1.8561 acc_val: 0.7900 time: 1.7846s -1 -1\n",
            "Epoch: 0240 loss_train: 0.8328 acc_train: 0.8100 loss_val: 1.9262 acc_val: 0.7860 time: 1.7910s -1 -1\n",
            "Epoch: 0241 loss_train: 0.8364 acc_train: 0.8080 loss_val: 1.9394 acc_val: 0.7860 time: 1.8042s -1 -1\n",
            "Epoch: 0242 loss_train: 0.8326 acc_train: 0.8040 loss_val: 1.9295 acc_val: 0.7880 time: 1.8237s -1 -1\n",
            "Epoch: 0243 loss_train: 0.8254 acc_train: 0.8100 loss_val: 1.9010 acc_val: 0.7900 time: 1.8289s -1 -1\n",
            "Epoch: 0244 loss_train: 0.8336 acc_train: 0.8120 loss_val: 1.7395 acc_val: 0.7900 time: 1.8404s -1 -1\n",
            "Epoch: 0245 loss_train: 0.8434 acc_train: 0.8060 loss_val: 1.6155 acc_val: 0.7980 time: 1.8468s -1 -1\n",
            "Epoch: 0246 loss_train: 0.8160 acc_train: 0.8160 loss_val: 1.5648 acc_val: 0.7980 time: 1.8530s -1 -1\n",
            "Epoch: 0247 loss_train: 0.8303 acc_train: 0.8120 loss_val: 1.5596 acc_val: 0.7920 time: 1.8595s -1 -1\n",
            "Epoch: 0248 loss_train: 0.8184 acc_train: 0.8080 loss_val: 1.5795 acc_val: 0.7920 time: 1.8662s -1 -1\n",
            "Epoch: 0249 loss_train: 0.8442 acc_train: 0.8120 loss_val: 1.6514 acc_val: 0.7900 time: 1.8727s -1 -1\n",
            "Epoch: 0250 loss_train: 0.8229 acc_train: 0.8100 loss_val: 1.7481 acc_val: 0.7900 time: 1.8790s -1 -1\n",
            "Epoch: 0251 loss_train: 0.8354 acc_train: 0.8100 loss_val: 1.8455 acc_val: 0.7900 time: 1.8854s -1 -1\n",
            "Epoch: 0252 loss_train: 0.8288 acc_train: 0.8140 loss_val: 1.9006 acc_val: 0.7900 time: 1.8916s -1 -1\n",
            "Epoch: 0253 loss_train: 0.8146 acc_train: 0.8100 loss_val: 1.9445 acc_val: 0.7900 time: 1.8977s -1 -1\n",
            "Epoch: 0254 loss_train: 0.8154 acc_train: 0.8140 loss_val: 1.9863 acc_val: 0.7880 time: 1.9042s -1 -1\n",
            "Epoch: 0255 loss_train: 0.8368 acc_train: 0.8080 loss_val: 1.9146 acc_val: 0.7900 time: 1.9102s -1 -1\n",
            "Epoch: 0256 loss_train: 0.8246 acc_train: 0.8080 loss_val: 1.7837 acc_val: 0.7900 time: 1.9167s -1 -1\n",
            "Epoch: 0257 loss_train: 0.8143 acc_train: 0.8120 loss_val: 1.7144 acc_val: 0.7900 time: 1.9254s -1 -1\n",
            "Epoch: 0258 loss_train: 0.8309 acc_train: 0.8100 loss_val: 1.6408 acc_val: 0.7900 time: 1.9316s -1 -1\n",
            "Epoch: 0259 loss_train: 0.8327 acc_train: 0.8080 loss_val: 1.6354 acc_val: 0.7900 time: 1.9376s -1 -1\n",
            "Epoch: 0260 loss_train: 0.8175 acc_train: 0.8140 loss_val: 1.7056 acc_val: 0.7900 time: 1.9437s -1 -1\n",
            "Epoch: 0261 loss_train: 0.8179 acc_train: 0.8100 loss_val: 1.7864 acc_val: 0.7880 time: 1.9501s -1 -1\n",
            "Epoch: 0262 loss_train: 0.8264 acc_train: 0.8140 loss_val: 1.8600 acc_val: 0.7860 time: 1.9563s -1 -1\n",
            "Epoch: 0263 loss_train: 0.8257 acc_train: 0.8060 loss_val: 1.9097 acc_val: 0.7860 time: 1.9624s -1 -1\n",
            "Epoch: 0264 loss_train: 0.8117 acc_train: 0.8140 loss_val: 1.9637 acc_val: 0.7900 time: 1.9684s -1 -1\n",
            "Epoch: 0265 loss_train: 0.8148 acc_train: 0.8200 loss_val: 1.9569 acc_val: 0.7900 time: 1.9747s -1 -1\n",
            "Epoch: 0266 loss_train: 0.8214 acc_train: 0.8060 loss_val: 1.9568 acc_val: 0.7900 time: 1.9809s -1 -1\n",
            "Epoch: 0267 loss_train: 0.8324 acc_train: 0.8060 loss_val: 1.9783 acc_val: 0.7900 time: 1.9874s -1 -1\n",
            "Epoch: 0268 loss_train: 0.8069 acc_train: 0.8120 loss_val: 2.0327 acc_val: 0.7900 time: 1.9935s -1 -1\n",
            "Epoch: 0269 loss_train: 0.8263 acc_train: 0.8060 loss_val: 1.9367 acc_val: 0.7900 time: 2.0002s -1 -1\n",
            "Epoch: 0270 loss_train: 0.8314 acc_train: 0.8080 loss_val: 1.7882 acc_val: 0.7900 time: 2.0065s -1 -1\n",
            "Epoch: 0271 loss_train: 0.8220 acc_train: 0.8080 loss_val: 1.6662 acc_val: 0.7900 time: 2.0130s -1 -1\n",
            "Epoch: 0272 loss_train: 0.8284 acc_train: 0.8100 loss_val: 1.6053 acc_val: 0.7900 time: 2.0230s -1 -1\n",
            "Epoch: 0273 loss_train: 0.8260 acc_train: 0.8060 loss_val: 1.5683 acc_val: 0.7960 time: 2.0295s -1 -1\n",
            "Epoch: 0274 loss_train: 0.8195 acc_train: 0.8100 loss_val: 1.6377 acc_val: 0.7900 time: 2.0362s -1 -1\n",
            "Epoch: 0275 loss_train: 0.8262 acc_train: 0.8040 loss_val: 1.7023 acc_val: 0.7900 time: 2.0429s -1 -1\n",
            "Epoch: 0276 loss_train: 0.8315 acc_train: 0.8080 loss_val: 1.8209 acc_val: 0.7900 time: 2.0495s -1 -1\n",
            "Epoch: 0277 loss_train: 0.8237 acc_train: 0.8080 loss_val: 1.9356 acc_val: 0.7900 time: 2.0561s -1 -1\n",
            "Epoch: 0278 loss_train: 0.8184 acc_train: 0.8100 loss_val: 2.0546 acc_val: 0.7900 time: 2.0626s -1 -1\n",
            "Epoch: 0279 loss_train: 0.8091 acc_train: 0.8140 loss_val: 2.1148 acc_val: 0.7880 time: 2.0690s -1 -1\n",
            "Epoch: 0280 loss_train: 0.8184 acc_train: 0.8140 loss_val: 2.1296 acc_val: 0.7860 time: 2.0756s -1 -1\n",
            "Epoch: 0281 loss_train: 0.8352 acc_train: 0.8100 loss_val: 2.0956 acc_val: 0.7860 time: 2.0819s -1 -1\n",
            "Epoch: 0282 loss_train: 0.8130 acc_train: 0.8120 loss_val: 2.0220 acc_val: 0.7900 time: 2.0881s -1 -1\n",
            "Epoch: 0283 loss_train: 0.8259 acc_train: 0.8080 loss_val: 1.9523 acc_val: 0.7900 time: 2.0948s -1 -1\n",
            "Epoch: 0284 loss_train: 0.8133 acc_train: 0.8060 loss_val: 1.8767 acc_val: 0.7900 time: 2.1012s -1 -1\n",
            "Epoch: 0285 loss_train: 0.8162 acc_train: 0.8120 loss_val: 1.8389 acc_val: 0.7900 time: 2.1078s -1 -1\n",
            "Epoch: 0286 loss_train: 0.8280 acc_train: 0.8040 loss_val: 1.8063 acc_val: 0.7900 time: 2.1172s -1 -1\n",
            "Epoch: 0287 loss_train: 0.8181 acc_train: 0.8060 loss_val: 1.8316 acc_val: 0.7900 time: 2.1258s -1 -1\n",
            "Epoch: 0288 loss_train: 0.7989 acc_train: 0.8100 loss_val: 1.9000 acc_val: 0.7900 time: 2.1335s -1 -1\n",
            "Epoch: 0289 loss_train: 0.8185 acc_train: 0.8100 loss_val: 1.9223 acc_val: 0.7900 time: 2.1397s -1 -1\n",
            "Epoch: 0290 loss_train: 0.8239 acc_train: 0.8080 loss_val: 1.9368 acc_val: 0.7880 time: 2.1457s -1 -1\n",
            "Epoch: 0291 loss_train: 0.8166 acc_train: 0.8100 loss_val: 1.9965 acc_val: 0.7880 time: 2.1507s -1 -1\n",
            "Epoch: 0292 loss_train: 0.8297 acc_train: 0.8060 loss_val: 2.0676 acc_val: 0.7860 time: 2.1567s -1 -1\n",
            "Epoch: 0293 loss_train: 0.8241 acc_train: 0.8100 loss_val: 2.0676 acc_val: 0.7880 time: 2.1630s -1 -1\n",
            "Epoch: 0294 loss_train: 0.8344 acc_train: 0.8060 loss_val: 2.0528 acc_val: 0.7900 time: 2.1691s -1 -1\n",
            "Epoch: 0295 loss_train: 0.8168 acc_train: 0.8120 loss_val: 2.0407 acc_val: 0.7880 time: 2.1753s -1 -1\n",
            "Epoch: 0296 loss_train: 0.8075 acc_train: 0.8100 loss_val: 1.9957 acc_val: 0.7880 time: 2.1816s -1 -1\n",
            "Epoch: 0297 loss_train: 0.8228 acc_train: 0.8100 loss_val: 1.9157 acc_val: 0.7880 time: 2.1879s -1 -1\n",
            "Epoch: 0298 loss_train: 0.8149 acc_train: 0.8100 loss_val: 1.8578 acc_val: 0.7880 time: 2.1938s -1 -1\n",
            "Epoch: 0299 loss_train: 0.8060 acc_train: 0.8140 loss_val: 1.8511 acc_val: 0.7880 time: 2.2001s -1 -1\n",
            "Epoch: 0300 loss_train: 0.8103 acc_train: 0.8100 loss_val: 1.8492 acc_val: 0.7880 time: 2.2064s -1 -1\n",
            "Epoch: 0301 loss_train: 0.8118 acc_train: 0.8120 loss_val: 1.9548 acc_val: 0.7880 time: 2.2126s -1 -1\n",
            "Epoch: 0302 loss_train: 0.8049 acc_train: 0.8120 loss_val: 2.0889 acc_val: 0.7860 time: 2.2188s -1 -1\n",
            "Epoch: 0303 loss_train: 0.8024 acc_train: 0.8100 loss_val: 2.2495 acc_val: 0.7840 time: 2.2316s -1 -1\n",
            "Epoch: 0304 loss_train: 0.8060 acc_train: 0.8080 loss_val: 2.3299 acc_val: 0.7820 time: 2.2380s -1 -1\n",
            "Epoch: 0305 loss_train: 0.8235 acc_train: 0.8080 loss_val: 2.2398 acc_val: 0.7880 time: 2.2471s -1 -1\n",
            "Epoch: 0306 loss_train: 0.7964 acc_train: 0.8140 loss_val: 2.1742 acc_val: 0.7900 time: 2.2540s -1 -1\n",
            "Epoch: 0307 loss_train: 0.8051 acc_train: 0.8100 loss_val: 2.2056 acc_val: 0.7900 time: 2.2603s -1 -1\n",
            "Epoch: 0308 loss_train: 0.8002 acc_train: 0.8100 loss_val: 2.2765 acc_val: 0.7880 time: 2.2666s -1 -1\n",
            "Epoch: 0309 loss_train: 0.8153 acc_train: 0.8120 loss_val: 2.2510 acc_val: 0.7840 time: 2.2732s -1 -1\n",
            "Epoch: 0310 loss_train: 0.8065 acc_train: 0.8120 loss_val: 2.1796 acc_val: 0.7840 time: 2.2794s -1 -1\n",
            "Epoch: 0311 loss_train: 0.8235 acc_train: 0.8080 loss_val: 2.0891 acc_val: 0.7800 time: 2.2857s -1 -1\n",
            "Epoch: 0312 loss_train: 0.8134 acc_train: 0.8100 loss_val: 1.9379 acc_val: 0.7880 time: 2.2944s -1 -1\n",
            "Epoch: 0313 loss_train: 0.8115 acc_train: 0.8120 loss_val: 1.8225 acc_val: 0.7880 time: 2.3006s -1 -1\n",
            "Epoch: 0314 loss_train: 0.8198 acc_train: 0.8100 loss_val: 1.8090 acc_val: 0.7880 time: 2.3114s -1 -1\n",
            "Epoch: 0315 loss_train: 0.8002 acc_train: 0.8080 loss_val: 1.8486 acc_val: 0.7880 time: 2.3178s -1 -1\n",
            "Epoch: 0316 loss_train: 0.8191 acc_train: 0.8080 loss_val: 1.9794 acc_val: 0.7880 time: 2.3240s -1 -1\n",
            "Epoch: 0317 loss_train: 0.8179 acc_train: 0.8100 loss_val: 2.0990 acc_val: 0.7880 time: 2.3305s -1 -1\n",
            "Epoch: 0318 loss_train: 0.8129 acc_train: 0.8120 loss_val: 2.2539 acc_val: 0.7820 time: 2.3371s -1 -1\n",
            "Epoch: 0319 loss_train: 0.8068 acc_train: 0.8120 loss_val: 2.2879 acc_val: 0.7820 time: 2.3443s -1 -1\n",
            "Epoch: 0320 loss_train: 0.8106 acc_train: 0.8120 loss_val: 2.2411 acc_val: 0.7860 time: 2.3523s -1 -1\n",
            "Epoch: 0321 loss_train: 0.8045 acc_train: 0.8100 loss_val: 2.2068 acc_val: 0.7880 time: 2.3569s -1 -1\n",
            "Epoch: 0322 loss_train: 0.8037 acc_train: 0.8160 loss_val: 2.2423 acc_val: 0.7860 time: 2.3642s -1 -1\n",
            "Epoch: 0323 loss_train: 0.8096 acc_train: 0.8120 loss_val: 2.2526 acc_val: 0.7840 time: 2.3704s -1 -1\n",
            "Epoch: 0324 loss_train: 0.8017 acc_train: 0.8180 loss_val: 2.3076 acc_val: 0.7800 time: 2.3769s -1 -1\n",
            "Epoch: 0325 loss_train: 0.8179 acc_train: 0.8020 loss_val: 2.2756 acc_val: 0.7800 time: 2.3833s -1 -1\n",
            "Epoch: 0326 loss_train: 0.8032 acc_train: 0.8160 loss_val: 2.2222 acc_val: 0.7820 time: 2.3918s -1 -1\n",
            "Epoch: 0327 loss_train: 0.8234 acc_train: 0.8080 loss_val: 2.1286 acc_val: 0.7880 time: 2.3992s -1 -1\n",
            "Epoch: 0328 loss_train: 0.8209 acc_train: 0.8100 loss_val: 2.0724 acc_val: 0.7900 time: 2.4055s -1 -1\n",
            "Epoch: 0329 loss_train: 0.8075 acc_train: 0.8080 loss_val: 2.0521 acc_val: 0.7880 time: 2.4118s -1 -1\n",
            "Epoch: 0330 loss_train: 0.8084 acc_train: 0.8160 loss_val: 2.0524 acc_val: 0.7840 time: 2.4182s -1 -1\n",
            "Epoch: 0331 loss_train: 0.8059 acc_train: 0.8120 loss_val: 2.1353 acc_val: 0.7820 time: 2.4244s -1 -1\n",
            "Epoch: 0332 loss_train: 0.8066 acc_train: 0.8120 loss_val: 2.1347 acc_val: 0.7820 time: 2.4307s -1 -1\n",
            "Epoch: 0333 loss_train: 0.7954 acc_train: 0.8160 loss_val: 2.0720 acc_val: 0.7840 time: 2.4454s -1 -1\n",
            "Epoch: 0334 loss_train: 0.8174 acc_train: 0.8080 loss_val: 2.0537 acc_val: 0.7840 time: 2.4517s -1 -1\n",
            "Epoch: 0335 loss_train: 0.8109 acc_train: 0.8080 loss_val: 2.0487 acc_val: 0.7860 time: 2.4578s -1 -1\n",
            "Epoch: 0336 loss_train: 0.8237 acc_train: 0.8080 loss_val: 2.0639 acc_val: 0.7860 time: 2.4640s -1 -1\n",
            "Epoch: 0337 loss_train: 0.8133 acc_train: 0.8140 loss_val: 2.1427 acc_val: 0.7880 time: 2.4703s -1 -1\n",
            "Epoch: 0338 loss_train: 0.7961 acc_train: 0.8100 loss_val: 2.2915 acc_val: 0.7880 time: 2.4766s -1 -1\n",
            "Epoch: 0339 loss_train: 0.8009 acc_train: 0.8080 loss_val: 2.4120 acc_val: 0.7840 time: 2.4830s -1 -1\n",
            "Epoch: 0340 loss_train: 0.8063 acc_train: 0.8140 loss_val: 2.5181 acc_val: 0.7800 time: 2.4892s -1 -1\n",
            "Epoch: 0341 loss_train: 0.8029 acc_train: 0.8140 loss_val: 2.4599 acc_val: 0.7820 time: 2.4970s -1 -1\n",
            "Epoch: 0342 loss_train: 0.8052 acc_train: 0.8120 loss_val: 2.3155 acc_val: 0.7860 time: 2.5036s -1 -1\n",
            "Epoch: 0343 loss_train: 0.7897 acc_train: 0.8160 loss_val: 2.2072 acc_val: 0.7900 time: 2.5096s -1 -1\n",
            "Epoch: 0344 loss_train: 0.7948 acc_train: 0.8120 loss_val: 2.1623 acc_val: 0.7880 time: 2.5158s -1 -1\n",
            "Epoch: 0345 loss_train: 0.8124 acc_train: 0.8080 loss_val: 2.1698 acc_val: 0.7860 time: 2.5261s -1 -1\n",
            "Epoch: 0346 loss_train: 0.8144 acc_train: 0.8140 loss_val: 2.1982 acc_val: 0.7840 time: 2.5357s -1 -1\n",
            "Epoch: 0347 loss_train: 0.7893 acc_train: 0.8160 loss_val: 2.1104 acc_val: 0.7860 time: 2.5421s -1 -1\n",
            "Epoch: 0348 loss_train: 0.7913 acc_train: 0.8140 loss_val: 2.0582 acc_val: 0.7880 time: 2.5484s -1 -1\n",
            "Epoch: 0349 loss_train: 0.7973 acc_train: 0.8060 loss_val: 1.9974 acc_val: 0.7900 time: 2.5547s -1 -1\n",
            "Epoch: 0350 loss_train: 0.8013 acc_train: 0.8140 loss_val: 1.9843 acc_val: 0.7900 time: 2.5612s -1 -1\n",
            "Epoch: 0351 loss_train: 0.8147 acc_train: 0.8060 loss_val: 2.1003 acc_val: 0.7860 time: 2.5673s -1 -1\n",
            "Epoch: 0352 loss_train: 0.7983 acc_train: 0.8140 loss_val: 2.2544 acc_val: 0.7820 time: 2.5736s -1 -1\n",
            "Epoch: 0353 loss_train: 0.7933 acc_train: 0.8160 loss_val: 2.3301 acc_val: 0.7800 time: 2.5840s -1 -1\n",
            "Epoch: 0354 loss_train: 0.7900 acc_train: 0.8120 loss_val: 2.3833 acc_val: 0.7780 time: 2.5904s -1 -1\n",
            "Epoch: 0355 loss_train: 0.7833 acc_train: 0.8140 loss_val: 2.3240 acc_val: 0.7860 time: 2.5968s -1 -1\n",
            "Epoch: 0356 loss_train: 0.7845 acc_train: 0.8140 loss_val: 2.2439 acc_val: 0.7880 time: 2.6029s -1 -1\n",
            "Epoch: 0357 loss_train: 0.8083 acc_train: 0.8140 loss_val: 2.2023 acc_val: 0.7880 time: 2.6091s -1 -1\n",
            "Epoch: 0358 loss_train: 0.7973 acc_train: 0.8120 loss_val: 2.2385 acc_val: 0.7880 time: 2.6152s -1 -1\n",
            "Epoch: 0359 loss_train: 0.7899 acc_train: 0.8120 loss_val: 2.3219 acc_val: 0.7860 time: 2.6213s -1 -1\n",
            "Epoch: 0360 loss_train: 0.7804 acc_train: 0.8140 loss_val: 2.3534 acc_val: 0.7800 time: 2.6312s -1 -1\n",
            "Epoch: 0361 loss_train: 0.7988 acc_train: 0.8120 loss_val: 2.3489 acc_val: 0.7800 time: 2.6374s -1 -1\n",
            "Epoch: 0362 loss_train: 0.7866 acc_train: 0.8120 loss_val: 2.3000 acc_val: 0.7800 time: 2.6448s -1 -1\n",
            "Epoch: 0363 loss_train: 0.8020 acc_train: 0.8160 loss_val: 2.3128 acc_val: 0.7820 time: 2.6498s -1 -1\n",
            "Epoch: 0364 loss_train: 0.7941 acc_train: 0.8080 loss_val: 2.2941 acc_val: 0.7840 time: 2.6599s -1 -1\n",
            "Epoch: 0365 loss_train: 0.8077 acc_train: 0.8080 loss_val: 2.2532 acc_val: 0.7860 time: 2.6662s -1 -1\n",
            "Epoch: 0366 loss_train: 0.7977 acc_train: 0.8160 loss_val: 2.2571 acc_val: 0.7860 time: 2.6757s -1 -1\n",
            "Epoch: 0367 loss_train: 0.7938 acc_train: 0.8140 loss_val: 2.3150 acc_val: 0.7840 time: 2.6827s -1 -1\n",
            "Epoch: 0368 loss_train: 0.7973 acc_train: 0.8120 loss_val: 2.4311 acc_val: 0.7780 time: 2.6939s -1 -1\n",
            "Epoch: 0369 loss_train: 0.8052 acc_train: 0.8120 loss_val: 2.4270 acc_val: 0.7780 time: 2.7010s -1 -1\n",
            "Epoch: 0370 loss_train: 0.8001 acc_train: 0.8100 loss_val: 2.3690 acc_val: 0.7820 time: 2.7078s -1 -1\n",
            "Epoch: 0371 loss_train: 0.8182 acc_train: 0.8040 loss_val: 2.1450 acc_val: 0.7840 time: 2.7143s -1 -1\n",
            "Epoch: 0372 loss_train: 0.7771 acc_train: 0.8200 loss_val: 1.9974 acc_val: 0.7880 time: 2.7243s -1 -1\n",
            "Epoch: 0373 loss_train: 0.7985 acc_train: 0.8080 loss_val: 2.0854 acc_val: 0.7860 time: 2.7310s -1 -1\n",
            "Epoch: 0374 loss_train: 0.7893 acc_train: 0.8160 loss_val: 2.1967 acc_val: 0.7820 time: 2.7377s -1 -1\n",
            "Epoch: 0375 loss_train: 0.8019 acc_train: 0.8100 loss_val: 2.3082 acc_val: 0.7800 time: 2.7442s -1 -1\n",
            "Epoch: 0376 loss_train: 0.8005 acc_train: 0.8140 loss_val: 2.3708 acc_val: 0.7800 time: 2.7507s -1 -1\n",
            "Epoch: 0377 loss_train: 0.7859 acc_train: 0.8160 loss_val: 2.4516 acc_val: 0.7760 time: 2.7574s -1 -1\n",
            "Epoch: 0378 loss_train: 0.7797 acc_train: 0.8160 loss_val: 2.4091 acc_val: 0.7840 time: 2.7641s -1 -1\n",
            "Epoch: 0379 loss_train: 0.7882 acc_train: 0.8120 loss_val: 2.3529 acc_val: 0.7860 time: 2.7739s -1 -1\n",
            "Epoch: 0380 loss_train: 0.7868 acc_train: 0.8120 loss_val: 2.3246 acc_val: 0.7880 time: 2.7806s -1 -1\n",
            "Epoch: 0381 loss_train: 0.7970 acc_train: 0.8180 loss_val: 2.3208 acc_val: 0.7880 time: 2.7871s -1 -1\n",
            "Epoch: 0382 loss_train: 0.8057 acc_train: 0.8040 loss_val: 2.3434 acc_val: 0.7840 time: 2.7936s -1 -1\n",
            "Epoch: 0383 loss_train: 0.7932 acc_train: 0.8120 loss_val: 2.2770 acc_val: 0.7840 time: 2.8002s -1 -1\n",
            "Epoch: 0384 loss_train: 0.7996 acc_train: 0.8080 loss_val: 2.2093 acc_val: 0.7860 time: 2.8068s -1 -1\n",
            "Epoch: 0385 loss_train: 0.7904 acc_train: 0.8080 loss_val: 2.1616 acc_val: 0.7860 time: 2.8154s -1 -1\n",
            "Epoch: 0386 loss_train: 0.7860 acc_train: 0.8220 loss_val: 2.1169 acc_val: 0.7860 time: 2.8215s -1 -1\n",
            "Epoch: 0387 loss_train: 0.8064 acc_train: 0.8080 loss_val: 2.1413 acc_val: 0.7860 time: 2.8278s -1 -1\n",
            "Epoch: 0388 loss_train: 0.7833 acc_train: 0.8120 loss_val: 2.2554 acc_val: 0.7840 time: 2.8341s -1 -1\n",
            "Epoch: 0389 loss_train: 0.8060 acc_train: 0.8080 loss_val: 2.3915 acc_val: 0.7820 time: 2.8403s -1 -1\n",
            "Epoch: 0390 loss_train: 0.8020 acc_train: 0.8080 loss_val: 2.5247 acc_val: 0.7780 time: 2.8466s -1 -1\n",
            "Epoch: 0391 loss_train: 0.7996 acc_train: 0.8100 loss_val: 2.5618 acc_val: 0.7760 time: 2.8538s -1 -1\n",
            "Epoch: 0392 loss_train: 0.8002 acc_train: 0.8060 loss_val: 2.5041 acc_val: 0.7800 time: 2.8584s -1 -1\n",
            "Epoch: 0393 loss_train: 0.8064 acc_train: 0.8100 loss_val: 2.4631 acc_val: 0.7800 time: 2.8690s -1 -1\n",
            "Epoch: 0394 loss_train: 0.7972 acc_train: 0.8140 loss_val: 2.4144 acc_val: 0.7820 time: 2.8752s -1 -1\n",
            "Epoch: 0395 loss_train: 0.7818 acc_train: 0.8180 loss_val: 2.3969 acc_val: 0.7820 time: 2.8815s -1 -1\n",
            "Epoch: 0396 loss_train: 0.7820 acc_train: 0.8160 loss_val: 2.3928 acc_val: 0.7780 time: 2.8876s -1 -1\n",
            "Epoch: 0397 loss_train: 0.8145 acc_train: 0.8080 loss_val: 2.2375 acc_val: 0.7880 time: 2.8938s -1 -1\n",
            "Epoch: 0398 loss_train: 0.8107 acc_train: 0.8080 loss_val: 2.1650 acc_val: 0.7880 time: 2.9002s -1 -1\n",
            "Epoch: 0399 loss_train: 0.8066 acc_train: 0.8080 loss_val: 2.2403 acc_val: 0.7860 time: 2.9064s -1 -1\n",
            "Epoch: 0400 loss_train: 0.7880 acc_train: 0.8200 loss_val: 2.4719 acc_val: 0.7840 time: 2.9135s -1 -1\n",
            "Epoch: 0401 loss_train: 0.7856 acc_train: 0.8220 loss_val: 2.6429 acc_val: 0.7820 time: 2.9198s -1 -1\n",
            "Epoch: 0402 loss_train: 0.8049 acc_train: 0.8140 loss_val: 2.3985 acc_val: 0.7820 time: 2.9317s -1 -1\n",
            "Epoch: 0403 loss_train: 0.8042 acc_train: 0.8100 loss_val: 2.2363 acc_val: 0.7860 time: 2.9380s -1 -1\n",
            "Epoch: 0404 loss_train: 0.7890 acc_train: 0.8180 loss_val: 2.1426 acc_val: 0.7840 time: 2.9443s -1 -1\n",
            "Epoch: 0405 loss_train: 0.8092 acc_train: 0.8080 loss_val: 2.1634 acc_val: 0.7840 time: 2.9504s -1 -1\n",
            "Epoch: 0406 loss_train: 0.7966 acc_train: 0.8200 loss_val: 2.2234 acc_val: 0.7840 time: 2.9567s -1 -1\n",
            "Epoch: 0407 loss_train: 0.7875 acc_train: 0.8120 loss_val: 2.3300 acc_val: 0.7800 time: 2.9627s -1 -1\n",
            "Epoch: 0408 loss_train: 0.7800 acc_train: 0.8160 loss_val: 2.3696 acc_val: 0.7800 time: 2.9689s -1 -1\n",
            "Epoch: 0409 loss_train: 0.7894 acc_train: 0.8180 loss_val: 2.5203 acc_val: 0.7780 time: 2.9753s -1 -1\n",
            "Epoch: 0410 loss_train: 0.7912 acc_train: 0.8160 loss_val: 2.5961 acc_val: 0.7780 time: 2.9816s -1 -1\n",
            "Epoch: 0411 loss_train: 0.8141 acc_train: 0.8040 loss_val: 2.6543 acc_val: 0.7780 time: 2.9879s -1 -1\n",
            "Epoch: 0412 loss_train: 0.7955 acc_train: 0.8180 loss_val: 2.6384 acc_val: 0.7760 time: 2.9941s -1 -1\n",
            "Epoch: 0413 loss_train: 0.7759 acc_train: 0.8160 loss_val: 2.5933 acc_val: 0.7760 time: 3.0005s -1 -1\n",
            "Epoch: 0414 loss_train: 0.7935 acc_train: 0.8140 loss_val: 2.4199 acc_val: 0.7840 time: 3.0065s -1 -1\n",
            "Epoch: 0415 loss_train: 0.7710 acc_train: 0.8220 loss_val: 2.2804 acc_val: 0.7860 time: 3.0127s -1 -1\n",
            "Epoch: 0416 loss_train: 0.7826 acc_train: 0.8120 loss_val: 2.2260 acc_val: 0.7860 time: 3.0191s -1 -1\n",
            "Epoch: 0417 loss_train: 0.8037 acc_train: 0.8120 loss_val: 2.2485 acc_val: 0.7860 time: 3.0252s -1 -1\n",
            "Epoch: 0418 loss_train: 0.8018 acc_train: 0.8120 loss_val: 2.2933 acc_val: 0.7840 time: 3.0313s -1 -1\n",
            "Epoch: 0419 loss_train: 0.7860 acc_train: 0.8180 loss_val: 2.3808 acc_val: 0.7820 time: 3.0375s -1 -1\n",
            "Epoch: 0420 loss_train: 0.8027 acc_train: 0.8100 loss_val: 2.4175 acc_val: 0.7820 time: 3.0436s -1 -1\n",
            "Epoch: 0421 loss_train: 0.7872 acc_train: 0.8160 loss_val: 2.3732 acc_val: 0.7820 time: 3.0498s -1 -1\n",
            "Epoch: 0422 loss_train: 0.7883 acc_train: 0.8200 loss_val: 2.2971 acc_val: 0.7860 time: 3.0559s -1 -1\n",
            "Epoch: 0423 loss_train: 0.7845 acc_train: 0.8160 loss_val: 2.2014 acc_val: 0.7860 time: 3.0621s -1 -1\n",
            "Epoch: 0424 loss_train: 0.7939 acc_train: 0.8160 loss_val: 2.1756 acc_val: 0.7860 time: 3.0704s -1 -1\n",
            "Epoch: 0425 loss_train: 0.7816 acc_train: 0.8100 loss_val: 2.1962 acc_val: 0.7860 time: 3.0774s -1 -1\n",
            "Epoch: 0426 loss_train: 0.7829 acc_train: 0.8200 loss_val: 2.2916 acc_val: 0.7860 time: 3.0836s -1 -1\n",
            "Epoch: 0427 loss_train: 0.7870 acc_train: 0.8140 loss_val: 2.4215 acc_val: 0.7840 time: 3.0899s -1 -1\n",
            "Epoch: 0428 loss_train: 0.7899 acc_train: 0.8120 loss_val: 2.5605 acc_val: 0.7800 time: 3.0963s -1 -1\n",
            "Epoch: 0429 loss_train: 0.8032 acc_train: 0.8100 loss_val: 2.5902 acc_val: 0.7820 time: 3.1025s -1 -1\n",
            "Epoch: 0430 loss_train: 0.7997 acc_train: 0.8100 loss_val: 2.5453 acc_val: 0.7840 time: 3.1087s -1 -1\n",
            "Epoch: 0431 loss_train: 0.7900 acc_train: 0.8120 loss_val: 2.5257 acc_val: 0.7840 time: 3.1150s -1 -1\n",
            "Epoch: 0432 loss_train: 0.7870 acc_train: 0.8100 loss_val: 2.5009 acc_val: 0.7840 time: 3.1213s -1 -1\n",
            "Epoch: 0433 loss_train: 0.7755 acc_train: 0.8180 loss_val: 2.5538 acc_val: 0.7820 time: 3.1275s -1 -1\n",
            "Epoch: 0434 loss_train: 0.7759 acc_train: 0.8180 loss_val: 2.5653 acc_val: 0.7800 time: 3.1368s -1 -1\n",
            "Epoch: 0435 loss_train: 0.8050 acc_train: 0.8100 loss_val: 2.5977 acc_val: 0.7800 time: 3.1451s -1 -1\n",
            "Epoch: 0436 loss_train: 0.7909 acc_train: 0.8100 loss_val: 2.5409 acc_val: 0.7840 time: 3.1555s -1 -1\n",
            "Epoch: 0437 loss_train: 0.7693 acc_train: 0.8200 loss_val: 2.4388 acc_val: 0.7860 time: 3.1600s -1 -1\n",
            "Epoch: 0438 loss_train: 0.7688 acc_train: 0.8160 loss_val: 2.3729 acc_val: 0.7860 time: 3.1645s -1 -1\n",
            "Epoch: 0439 loss_train: 0.7758 acc_train: 0.8100 loss_val: 2.3520 acc_val: 0.7860 time: 3.1689s -1 -1\n",
            "Epoch: 0440 loss_train: 0.8009 acc_train: 0.8140 loss_val: 2.2893 acc_val: 0.7860 time: 3.1733s -1 -1\n",
            "Epoch: 0441 loss_train: 0.7940 acc_train: 0.8120 loss_val: 2.3670 acc_val: 0.7840 time: 3.1777s -1 -1\n",
            "Epoch: 0442 loss_train: 0.7948 acc_train: 0.8100 loss_val: 2.5876 acc_val: 0.7760 time: 3.1840s -1 -1\n",
            "Epoch: 0443 loss_train: 0.7956 acc_train: 0.8160 loss_val: 2.7674 acc_val: 0.7800 time: 3.1901s -1 -1\n",
            "Epoch: 0444 loss_train: 0.8187 acc_train: 0.8100 loss_val: 2.5659 acc_val: 0.7760 time: 3.1965s -1 -1\n",
            "Epoch: 0445 loss_train: 0.7861 acc_train: 0.8240 loss_val: 2.3999 acc_val: 0.7840 time: 3.2030s -1 -1\n",
            "Epoch: 0446 loss_train: 0.7835 acc_train: 0.8180 loss_val: 2.3297 acc_val: 0.7880 time: 3.2091s -1 -1\n",
            "Epoch: 0447 loss_train: 0.8032 acc_train: 0.8100 loss_val: 2.3660 acc_val: 0.7900 time: 3.2157s -1 -1\n",
            "Epoch: 0448 loss_train: 0.8006 acc_train: 0.8100 loss_val: 2.5572 acc_val: 0.7840 time: 3.2217s -1 -1\n",
            "Epoch: 0449 loss_train: 0.7857 acc_train: 0.8080 loss_val: 2.7726 acc_val: 0.7740 time: 3.2288s -1 -1\n",
            "Epoch: 0450 loss_train: 0.7819 acc_train: 0.8140 loss_val: 2.8366 acc_val: 0.7740 time: 3.2356s -1 -1\n",
            "Epoch: 0451 loss_train: 0.8083 acc_train: 0.8100 loss_val: 2.8162 acc_val: 0.7740 time: 3.2420s -1 -1\n",
            "Epoch: 0452 loss_train: 0.8026 acc_train: 0.8140 loss_val: 2.5742 acc_val: 0.7780 time: 3.2481s -1 -1\n",
            "Epoch: 0453 loss_train: 0.7824 acc_train: 0.8200 loss_val: 2.3143 acc_val: 0.7840 time: 3.2543s -1 -1\n",
            "Epoch: 0454 loss_train: 0.7832 acc_train: 0.8080 loss_val: 2.2405 acc_val: 0.7840 time: 3.2604s -1 -1\n",
            "Epoch: 0455 loss_train: 0.7902 acc_train: 0.8080 loss_val: 2.2167 acc_val: 0.7840 time: 3.2665s -1 -1\n",
            "Epoch: 0456 loss_train: 0.7871 acc_train: 0.8140 loss_val: 2.2246 acc_val: 0.7840 time: 3.2739s -1 -1\n",
            "Epoch: 0457 loss_train: 0.7928 acc_train: 0.8160 loss_val: 2.2680 acc_val: 0.7840 time: 3.2792s -1 -1\n",
            "Epoch: 0458 loss_train: 0.7791 acc_train: 0.8180 loss_val: 2.3179 acc_val: 0.7840 time: 3.2892s -1 -1\n",
            "Epoch: 0459 loss_train: 0.7886 acc_train: 0.8160 loss_val: 2.3979 acc_val: 0.7820 time: 3.2957s -1 -1\n",
            "Epoch: 0460 loss_train: 0.7765 acc_train: 0.8120 loss_val: 2.4095 acc_val: 0.7800 time: 3.3017s -1 -1\n",
            "Epoch: 0461 loss_train: 0.7895 acc_train: 0.8160 loss_val: 2.3527 acc_val: 0.7840 time: 3.3079s -1 -1\n",
            "Epoch: 0462 loss_train: 0.7590 acc_train: 0.8220 loss_val: 2.2888 acc_val: 0.7840 time: 3.3176s -1 -1\n",
            "Epoch: 0463 loss_train: 0.7935 acc_train: 0.8080 loss_val: 2.3269 acc_val: 0.7840 time: 3.3241s -1 -1\n",
            "Epoch: 0464 loss_train: 0.7737 acc_train: 0.8160 loss_val: 2.4351 acc_val: 0.7840 time: 3.3301s -1 -1\n",
            "Epoch: 0465 loss_train: 0.7737 acc_train: 0.8140 loss_val: 2.6340 acc_val: 0.7820 time: 3.3363s -1 -1\n",
            "Epoch: 0466 loss_train: 0.7784 acc_train: 0.8160 loss_val: 2.7619 acc_val: 0.7800 time: 3.3427s -1 -1\n",
            "Epoch: 0467 loss_train: 0.7750 acc_train: 0.8180 loss_val: 2.8931 acc_val: 0.7760 time: 3.3491s -1 -1\n",
            "Epoch: 0468 loss_train: 0.7869 acc_train: 0.8100 loss_val: 2.9035 acc_val: 0.7740 time: 3.3552s -1 -1\n",
            "Epoch: 0469 loss_train: 0.7918 acc_train: 0.8180 loss_val: 2.6530 acc_val: 0.7800 time: 3.3614s -1 -1\n",
            "Epoch: 0470 loss_train: 0.7870 acc_train: 0.8140 loss_val: 2.3372 acc_val: 0.7840 time: 3.3681s -1 -1\n",
            "Epoch: 0471 loss_train: 0.7816 acc_train: 0.8160 loss_val: 2.2331 acc_val: 0.7840 time: 3.3745s -1 -1\n",
            "Epoch: 0472 loss_train: 0.7765 acc_train: 0.8140 loss_val: 2.1806 acc_val: 0.7840 time: 3.3810s -1 -1\n",
            "Epoch: 0473 loss_train: 0.7913 acc_train: 0.8180 loss_val: 2.1093 acc_val: 0.7840 time: 3.3871s -1 -1\n",
            "Epoch: 0474 loss_train: 0.7858 acc_train: 0.8140 loss_val: 2.1951 acc_val: 0.7840 time: 3.3930s -1 -1\n",
            "Epoch: 0475 loss_train: 0.7856 acc_train: 0.8200 loss_val: 2.2915 acc_val: 0.7840 time: 3.3990s -1 -1\n",
            "Epoch: 0476 loss_train: 0.7716 acc_train: 0.8180 loss_val: 2.4315 acc_val: 0.7820 time: 3.4055s -1 -1\n",
            "Epoch: 0477 loss_train: 0.7966 acc_train: 0.8100 loss_val: 2.4936 acc_val: 0.7820 time: 3.4115s -1 -1\n",
            "Epoch: 0478 loss_train: 0.7817 acc_train: 0.8120 loss_val: 2.4886 acc_val: 0.7820 time: 3.4178s -1 -1\n",
            "Epoch: 0479 loss_train: 0.7834 acc_train: 0.8220 loss_val: 2.4804 acc_val: 0.7820 time: 3.4241s -1 -1\n",
            "Epoch: 0480 loss_train: 0.7570 acc_train: 0.8180 loss_val: 2.5094 acc_val: 0.7820 time: 3.4301s -1 -1\n",
            "Epoch: 0481 loss_train: 0.7795 acc_train: 0.8180 loss_val: 2.6149 acc_val: 0.7820 time: 3.4364s -1 -1\n",
            "Epoch: 0482 loss_train: 0.7840 acc_train: 0.8140 loss_val: 2.7810 acc_val: 0.7760 time: 3.4426s -1 -1\n",
            "Epoch: 0483 loss_train: 0.7863 acc_train: 0.8100 loss_val: 2.8616 acc_val: 0.7740 time: 3.4522s -1 -1\n",
            "Epoch: 0484 loss_train: 0.7973 acc_train: 0.8100 loss_val: 2.9227 acc_val: 0.7740 time: 3.4584s -1 -1\n",
            "Epoch: 0485 loss_train: 0.7812 acc_train: 0.8180 loss_val: 2.9625 acc_val: 0.7780 time: 3.4645s -1 -1\n",
            "Epoch: 0486 loss_train: 0.7892 acc_train: 0.8140 loss_val: 2.8851 acc_val: 0.7820 time: 3.4707s -1 -1\n",
            "Epoch: 0487 loss_train: 0.7954 acc_train: 0.8100 loss_val: 2.8223 acc_val: 0.7820 time: 3.4769s -1 -1\n",
            "Epoch: 0488 loss_train: 0.7773 acc_train: 0.8180 loss_val: 2.7402 acc_val: 0.7820 time: 3.4884s -1 -1\n",
            "Epoch: 0489 loss_train: 0.7881 acc_train: 0.8140 loss_val: 2.6413 acc_val: 0.7820 time: 3.4953s -1 -1\n",
            "Epoch: 0490 loss_train: 0.7712 acc_train: 0.8120 loss_val: 2.5704 acc_val: 0.7820 time: 3.5014s -1 -1\n",
            "Epoch: 0491 loss_train: 0.7647 acc_train: 0.8200 loss_val: 2.5266 acc_val: 0.7780 time: 3.5078s -1 -1\n",
            "Epoch: 0492 loss_train: 0.7891 acc_train: 0.8220 loss_val: 2.4856 acc_val: 0.7780 time: 3.5137s -1 -1\n",
            "Epoch: 0493 loss_train: 0.7758 acc_train: 0.8120 loss_val: 2.4452 acc_val: 0.7780 time: 3.5202s -1 -1\n",
            "Epoch: 0494 loss_train: 0.7669 acc_train: 0.8260 loss_val: 2.3099 acc_val: 0.7820 time: 3.5264s -1 -1\n",
            "Epoch: 0495 loss_train: 0.7902 acc_train: 0.8100 loss_val: 2.2356 acc_val: 0.7840 time: 3.5327s -1 -1\n",
            "Epoch: 0496 loss_train: 0.7662 acc_train: 0.8180 loss_val: 2.2038 acc_val: 0.7840 time: 3.5389s -1 -1\n",
            "Epoch: 0497 loss_train: 0.7789 acc_train: 0.8180 loss_val: 2.2768 acc_val: 0.7840 time: 3.5450s -1 -1\n",
            "Epoch: 0498 loss_train: 0.7776 acc_train: 0.8140 loss_val: 2.4134 acc_val: 0.7820 time: 3.5512s -1 -1\n",
            "Epoch: 0499 loss_train: 0.7608 acc_train: 0.8120 loss_val: 2.5482 acc_val: 0.7820 time: 3.5573s -1 -1\n",
            "Epoch: 0500 loss_train: 0.7836 acc_train: 0.8140 loss_val: 2.6541 acc_val: 0.7820 time: 3.5633s -1 -1\n",
            "Epoch: 0501 loss_train: 0.8048 acc_train: 0.8100 loss_val: 2.6274 acc_val: 0.7800 time: 3.5695s -1 -1\n",
            "Epoch: 0502 loss_train: 0.7799 acc_train: 0.8120 loss_val: 2.5068 acc_val: 0.7820 time: 3.5757s -1 -1\n",
            "Epoch: 0503 loss_train: 0.7786 acc_train: 0.8200 loss_val: 2.3949 acc_val: 0.7820 time: 3.5818s -1 -1\n",
            "Epoch: 0504 loss_train: 0.7689 acc_train: 0.8220 loss_val: 2.3272 acc_val: 0.7820 time: 3.5879s -1 -1\n",
            "Epoch: 0505 loss_train: 0.7707 acc_train: 0.8220 loss_val: 2.3248 acc_val: 0.7840 time: 3.5942s -1 -1\n",
            "Epoch: 0506 loss_train: 0.7637 acc_train: 0.8160 loss_val: 2.3879 acc_val: 0.7820 time: 3.6002s -1 -1\n",
            "Epoch: 0507 loss_train: 0.7823 acc_train: 0.8180 loss_val: 2.4479 acc_val: 0.7820 time: 3.6063s -1 -1\n",
            "Epoch: 0508 loss_train: 0.7731 acc_train: 0.8120 loss_val: 2.5653 acc_val: 0.7820 time: 3.6124s -1 -1\n",
            "Epoch: 0509 loss_train: 0.7674 acc_train: 0.8220 loss_val: 2.6435 acc_val: 0.7820 time: 3.6188s -1 -1\n",
            "Epoch: 0510 loss_train: 0.7706 acc_train: 0.8220 loss_val: 2.6345 acc_val: 0.7820 time: 3.6267s -1 -1\n",
            "Epoch: 0511 loss_train: 0.7609 acc_train: 0.8220 loss_val: 2.6730 acc_val: 0.7820 time: 3.6344s -1 -1\n",
            "Epoch: 0512 loss_train: 0.7789 acc_train: 0.8140 loss_val: 2.6943 acc_val: 0.7820 time: 3.6407s -1 -1\n",
            "Epoch: 0513 loss_train: 0.7640 acc_train: 0.8160 loss_val: 2.7029 acc_val: 0.7800 time: 3.6468s -1 -1\n",
            "Epoch: 0514 loss_train: 0.7640 acc_train: 0.8200 loss_val: 2.7226 acc_val: 0.7800 time: 3.6530s -1 -1\n",
            "Epoch: 0515 loss_train: 0.7604 acc_train: 0.8180 loss_val: 2.7339 acc_val: 0.7820 time: 3.6592s -1 -1\n",
            "Epoch: 0516 loss_train: 0.7577 acc_train: 0.8220 loss_val: 2.7971 acc_val: 0.7820 time: 3.6655s -1 -1\n",
            "Epoch: 0517 loss_train: 0.7851 acc_train: 0.8120 loss_val: 2.8485 acc_val: 0.7820 time: 3.6718s -1 -1\n",
            "Epoch: 0518 loss_train: 0.7712 acc_train: 0.8160 loss_val: 2.8293 acc_val: 0.7820 time: 3.6782s -1 -1\n",
            "Epoch: 0519 loss_train: 0.7618 acc_train: 0.8220 loss_val: 2.7341 acc_val: 0.7820 time: 3.6844s -1 -1\n",
            "Epoch: 0520 loss_train: 0.7634 acc_train: 0.8240 loss_val: 2.6388 acc_val: 0.7840 time: 3.6947s -1 -1\n",
            "Epoch: 0521 loss_train: 0.7766 acc_train: 0.8180 loss_val: 2.5968 acc_val: 0.7840 time: 3.7018s -1 -1\n",
            "Epoch: 0522 loss_train: 0.7681 acc_train: 0.8100 loss_val: 2.6275 acc_val: 0.7820 time: 3.7081s -1 -1\n",
            "Epoch: 0523 loss_train: 0.7571 acc_train: 0.8180 loss_val: 2.7157 acc_val: 0.7820 time: 3.7143s -1 -1\n",
            "Epoch: 0524 loss_train: 0.7806 acc_train: 0.8200 loss_val: 2.8428 acc_val: 0.7800 time: 3.7330s -1 -1\n",
            "Epoch: 0525 loss_train: 0.7599 acc_train: 0.8220 loss_val: 2.9556 acc_val: 0.7800 time: 3.7399s -1 -1\n",
            "Epoch: 0526 loss_train: 0.7744 acc_train: 0.8160 loss_val: 3.0267 acc_val: 0.7800 time: 3.7467s -1 -1\n",
            "Epoch: 0527 loss_train: 0.7627 acc_train: 0.8180 loss_val: 3.0729 acc_val: 0.7800 time: 3.7536s -1 -1\n",
            "Epoch: 0528 loss_train: 0.7849 acc_train: 0.8100 loss_val: 2.9181 acc_val: 0.7820 time: 3.7601s -1 -1\n",
            "Epoch: 0529 loss_train: 0.7783 acc_train: 0.8120 loss_val: 2.7659 acc_val: 0.7820 time: 3.7666s -1 -1\n",
            "Epoch: 0530 loss_train: 0.7779 acc_train: 0.8180 loss_val: 2.6330 acc_val: 0.7820 time: 3.7735s -1 -1\n",
            "Epoch: 0531 loss_train: 0.7727 acc_train: 0.8120 loss_val: 2.5341 acc_val: 0.7840 time: 3.7798s -1 -1\n",
            "Epoch: 0532 loss_train: 0.7617 acc_train: 0.8200 loss_val: 2.5222 acc_val: 0.7840 time: 3.7865s -1 -1\n",
            "Epoch: 0533 loss_train: 0.7624 acc_train: 0.8180 loss_val: 2.6041 acc_val: 0.7840 time: 3.7933s -1 -1\n",
            "Epoch: 0534 loss_train: 0.7453 acc_train: 0.8220 loss_val: 2.7363 acc_val: 0.7820 time: 3.7997s -1 -1\n",
            "Epoch: 0535 loss_train: 0.7681 acc_train: 0.8220 loss_val: 2.8575 acc_val: 0.7820 time: 3.8064s -1 -1\n",
            "Epoch: 0536 loss_train: 0.7492 acc_train: 0.8260 loss_val: 2.9679 acc_val: 0.7800 time: 3.8131s -1 -1\n",
            "Epoch: 0537 loss_train: 0.7793 acc_train: 0.8160 loss_val: 3.0074 acc_val: 0.7780 time: 3.8196s -1 -1\n",
            "Epoch: 0538 loss_train: 0.7831 acc_train: 0.8140 loss_val: 2.8709 acc_val: 0.7800 time: 3.8260s -1 -1\n",
            "Epoch: 0539 loss_train: 0.7494 acc_train: 0.8220 loss_val: 2.7968 acc_val: 0.7820 time: 3.8328s -1 -1\n",
            "Epoch: 0540 loss_train: 0.7648 acc_train: 0.8140 loss_val: 2.7636 acc_val: 0.7820 time: 3.8395s -1 -1\n",
            "Epoch: 0541 loss_train: 0.7582 acc_train: 0.8140 loss_val: 2.8029 acc_val: 0.7820 time: 3.8459s -1 -1\n",
            "Epoch: 0542 loss_train: 0.7695 acc_train: 0.8180 loss_val: 2.9060 acc_val: 0.7780 time: 3.8522s -1 -1\n",
            "Epoch: 0543 loss_train: 0.7658 acc_train: 0.8180 loss_val: 2.9858 acc_val: 0.7760 time: 3.8584s -1 -1\n",
            "Epoch: 0544 loss_train: 0.7849 acc_train: 0.8160 loss_val: 2.9594 acc_val: 0.7760 time: 3.8647s -1 -1\n",
            "Epoch: 0545 loss_train: 0.7584 acc_train: 0.8220 loss_val: 2.9055 acc_val: 0.7760 time: 3.8707s -1 -1\n",
            "Epoch: 0546 loss_train: 0.7626 acc_train: 0.8180 loss_val: 2.8412 acc_val: 0.7780 time: 3.8768s -1 -1\n",
            "Epoch: 0547 loss_train: 0.7739 acc_train: 0.8140 loss_val: 2.7976 acc_val: 0.7780 time: 3.8862s -1 -1\n",
            "Epoch: 0548 loss_train: 0.7665 acc_train: 0.8140 loss_val: 2.7544 acc_val: 0.7800 time: 3.8961s -1 -1\n",
            "Epoch: 0549 loss_train: 0.7483 acc_train: 0.8280 loss_val: 2.7219 acc_val: 0.7820 time: 3.9085s -1 -1\n",
            "Epoch: 0550 loss_train: 0.7483 acc_train: 0.8240 loss_val: 2.6963 acc_val: 0.7820 time: 3.9161s -1 -1\n",
            "Epoch: 0551 loss_train: 0.7796 acc_train: 0.8080 loss_val: 2.7323 acc_val: 0.7820 time: 3.9232s -1 -1\n",
            "Epoch: 0552 loss_train: 0.7783 acc_train: 0.8120 loss_val: 2.8083 acc_val: 0.7820 time: 3.9301s -1 -1\n",
            "Epoch: 0553 loss_train: 0.7546 acc_train: 0.8240 loss_val: 2.8366 acc_val: 0.7820 time: 3.9368s -1 -1\n",
            "Epoch: 0554 loss_train: 0.7712 acc_train: 0.8220 loss_val: 2.8422 acc_val: 0.7800 time: 3.9434s -1 -1\n",
            "Epoch: 0555 loss_train: 0.7658 acc_train: 0.8180 loss_val: 2.8529 acc_val: 0.7800 time: 3.9499s -1 -1\n",
            "Epoch: 0556 loss_train: 0.7572 acc_train: 0.8200 loss_val: 2.8597 acc_val: 0.7780 time: 3.9563s -1 -1\n",
            "Epoch: 0557 loss_train: 0.7510 acc_train: 0.8200 loss_val: 2.8200 acc_val: 0.7780 time: 3.9627s -1 -1\n",
            "Epoch: 0558 loss_train: 0.7495 acc_train: 0.8240 loss_val: 2.7509 acc_val: 0.7800 time: 3.9693s -1 -1\n",
            "Epoch: 0559 loss_train: 0.7594 acc_train: 0.8200 loss_val: 2.7111 acc_val: 0.7820 time: 3.9761s -1 -1\n",
            "Epoch: 0560 loss_train: 0.7581 acc_train: 0.8240 loss_val: 2.7896 acc_val: 0.7820 time: 3.9826s -1 -1\n",
            "Epoch: 0561 loss_train: 0.7467 acc_train: 0.8200 loss_val: 2.9347 acc_val: 0.7800 time: 3.9891s -1 -1\n",
            "Epoch: 0562 loss_train: 0.7678 acc_train: 0.8140 loss_val: 3.0843 acc_val: 0.7760 time: 3.9960s -1 -1\n",
            "Epoch: 0563 loss_train: 0.7760 acc_train: 0.8140 loss_val: 3.1840 acc_val: 0.7760 time: 4.0032s -1 -1\n",
            "Epoch: 0564 loss_train: 0.7432 acc_train: 0.8240 loss_val: 3.1560 acc_val: 0.7780 time: 4.0099s -1 -1\n",
            "Epoch: 0565 loss_train: 0.7765 acc_train: 0.8160 loss_val: 2.9090 acc_val: 0.7820 time: 4.0166s -1 -1\n",
            "Epoch: 0566 loss_train: 0.7384 acc_train: 0.8260 loss_val: 2.8539 acc_val: 0.7820 time: 4.0232s -1 -1\n",
            "Epoch: 0567 loss_train: 0.7537 acc_train: 0.8240 loss_val: 2.8392 acc_val: 0.7820 time: 4.0293s -1 -1\n",
            "Epoch: 0568 loss_train: 0.7591 acc_train: 0.8160 loss_val: 2.9024 acc_val: 0.7820 time: 4.0354s -1 -1\n",
            "Epoch: 0569 loss_train: 0.7788 acc_train: 0.8140 loss_val: 2.9655 acc_val: 0.7780 time: 4.0415s -1 -1\n",
            "Epoch: 0570 loss_train: 0.7752 acc_train: 0.8160 loss_val: 2.8544 acc_val: 0.7820 time: 4.0477s -1 -1\n",
            "Epoch: 0571 loss_train: 0.7682 acc_train: 0.8140 loss_val: 2.7402 acc_val: 0.7820 time: 4.0554s -1 -1\n",
            "Epoch: 0572 loss_train: 0.7383 acc_train: 0.8260 loss_val: 2.5881 acc_val: 0.7820 time: 4.0617s -1 -1\n",
            "Epoch: 0573 loss_train: 0.7847 acc_train: 0.8180 loss_val: 2.4951 acc_val: 0.7820 time: 4.0677s -1 -1\n",
            "Epoch: 0574 loss_train: 0.7663 acc_train: 0.8080 loss_val: 2.5044 acc_val: 0.7820 time: 4.0738s -1 -1\n",
            "Epoch: 0575 loss_train: 0.7647 acc_train: 0.8220 loss_val: 2.5790 acc_val: 0.7820 time: 4.0800s -1 -1\n",
            "Epoch: 0576 loss_train: 0.7643 acc_train: 0.8260 loss_val: 2.6577 acc_val: 0.7800 time: 4.0864s -1 -1\n",
            "Epoch: 0577 loss_train: 0.7715 acc_train: 0.8160 loss_val: 2.7278 acc_val: 0.7760 time: 4.0929s -1 -1\n",
            "Epoch: 0578 loss_train: 0.7596 acc_train: 0.8220 loss_val: 2.9608 acc_val: 0.7760 time: 4.1007s -1 -1\n",
            "Epoch: 0579 loss_train: 0.7640 acc_train: 0.8200 loss_val: 3.0457 acc_val: 0.7760 time: 4.1067s -1 -1\n",
            "Epoch: 0580 loss_train: 0.7805 acc_train: 0.8160 loss_val: 3.1773 acc_val: 0.7780 time: 4.1175s -1 -1\n",
            "Epoch: 0581 loss_train: 0.7464 acc_train: 0.8240 loss_val: 3.3228 acc_val: 0.7760 time: 4.1244s -1 -1\n",
            "Epoch: 0582 loss_train: 0.7798 acc_train: 0.8180 loss_val: 3.4516 acc_val: 0.7740 time: 4.1305s -1 -1\n",
            "Epoch: 0583 loss_train: 0.7800 acc_train: 0.8180 loss_val: 3.1860 acc_val: 0.7780 time: 4.1365s -1 -1\n",
            "Epoch: 0584 loss_train: 0.7680 acc_train: 0.8140 loss_val: 2.8289 acc_val: 0.7840 time: 4.1451s -1 -1\n",
            "Epoch: 0585 loss_train: 0.7591 acc_train: 0.8200 loss_val: 2.5669 acc_val: 0.7860 time: 4.1495s -1 -1\n",
            "Epoch: 0586 loss_train: 0.7426 acc_train: 0.8240 loss_val: 2.3520 acc_val: 0.7860 time: 4.1583s -1 -1\n",
            "Epoch: 0587 loss_train: 0.7642 acc_train: 0.8160 loss_val: 2.4187 acc_val: 0.7860 time: 4.1628s -1 -1\n",
            "Epoch: 0588 loss_train: 0.7658 acc_train: 0.8200 loss_val: 2.4548 acc_val: 0.7820 time: 4.1673s -1 -1\n",
            "Epoch: 0589 loss_train: 0.7927 acc_train: 0.8160 loss_val: 2.3949 acc_val: 0.7780 time: 4.1729s -1 -1\n",
            "Epoch: 0590 loss_train: 0.7751 acc_train: 0.8200 loss_val: 2.3686 acc_val: 0.7780 time: 4.1791s -1 -1\n",
            "Epoch: 0591 loss_train: 0.7633 acc_train: 0.8260 loss_val: 2.2580 acc_val: 0.7800 time: 4.1851s -1 -1\n",
            "Epoch: 0592 loss_train: 0.7763 acc_train: 0.8180 loss_val: 2.1755 acc_val: 0.7860 time: 4.1912s -1 -1\n",
            "Epoch: 0593 loss_train: 0.7818 acc_train: 0.8140 loss_val: 2.2893 acc_val: 0.7840 time: 4.1973s -1 -1\n",
            "Epoch: 0594 loss_train: 0.7580 acc_train: 0.8200 loss_val: 2.5624 acc_val: 0.7840 time: 4.2035s -1 -1\n",
            "Epoch: 0595 loss_train: 0.7751 acc_train: 0.8100 loss_val: 2.7891 acc_val: 0.7820 time: 4.2096s -1 -1\n",
            "Epoch: 0596 loss_train: 0.7650 acc_train: 0.8220 loss_val: 2.9642 acc_val: 0.7780 time: 4.2156s -1 -1\n",
            "Epoch: 0597 loss_train: 0.7825 acc_train: 0.8160 loss_val: 2.9986 acc_val: 0.7780 time: 4.2216s -1 -1\n",
            "Epoch: 0598 loss_train: 0.7760 acc_train: 0.8160 loss_val: 2.8425 acc_val: 0.7820 time: 4.2278s -1 -1\n",
            "Epoch: 0599 loss_train: 0.7659 acc_train: 0.8220 loss_val: 2.7321 acc_val: 0.7820 time: 4.2350s -1 -1\n",
            "Epoch: 0600 loss_train: 0.7541 acc_train: 0.8240 loss_val: 2.5673 acc_val: 0.7840 time: 4.2417s -1 -1\n",
            "Epoch: 0601 loss_train: 0.7813 acc_train: 0.8120 loss_val: 2.4524 acc_val: 0.7860 time: 4.2478s -1 -1\n",
            "Epoch: 0602 loss_train: 0.7514 acc_train: 0.8280 loss_val: 2.4078 acc_val: 0.7840 time: 4.2539s -1 -1\n",
            "Epoch: 0603 loss_train: 0.7803 acc_train: 0.8120 loss_val: 2.4983 acc_val: 0.7820 time: 4.2603s -1 -1\n",
            "Epoch: 0604 loss_train: 0.7677 acc_train: 0.8200 loss_val: 2.6678 acc_val: 0.7820 time: 4.2664s -1 -1\n",
            "Epoch: 0605 loss_train: 0.7738 acc_train: 0.8140 loss_val: 2.8682 acc_val: 0.7780 time: 4.2726s -1 -1\n",
            "Epoch: 0606 loss_train: 0.7575 acc_train: 0.8180 loss_val: 3.0030 acc_val: 0.7780 time: 4.2787s -1 -1\n",
            "Epoch: 0607 loss_train: 0.7627 acc_train: 0.8240 loss_val: 3.0500 acc_val: 0.7780 time: 4.2851s -1 -1\n",
            "Epoch: 0608 loss_train: 0.7728 acc_train: 0.8140 loss_val: 3.0094 acc_val: 0.7820 time: 4.2915s -1 -1\n",
            "Epoch: 0609 loss_train: 0.7503 acc_train: 0.8300 loss_val: 2.9231 acc_val: 0.7820 time: 4.2977s -1 -1\n",
            "Epoch: 0610 loss_train: 0.7898 acc_train: 0.8120 loss_val: 2.9462 acc_val: 0.7820 time: 4.3040s -1 -1\n",
            "Epoch: 0611 loss_train: 0.7311 acc_train: 0.8300 loss_val: 2.8893 acc_val: 0.7820 time: 4.3101s -1 -1\n",
            "Epoch: 0612 loss_train: 0.7749 acc_train: 0.8160 loss_val: 2.8139 acc_val: 0.7820 time: 4.3160s -1 -1\n",
            "Epoch: 0613 loss_train: 0.7552 acc_train: 0.8200 loss_val: 2.7795 acc_val: 0.7820 time: 4.3260s -1 -1\n",
            "Epoch: 0614 loss_train: 0.7659 acc_train: 0.8220 loss_val: 2.8257 acc_val: 0.7820 time: 4.3327s -1 -1\n",
            "Epoch: 0615 loss_train: 0.7555 acc_train: 0.8160 loss_val: 2.9074 acc_val: 0.7780 time: 4.3388s -1 -1\n",
            "Epoch: 0616 loss_train: 0.7519 acc_train: 0.8260 loss_val: 2.9552 acc_val: 0.7760 time: 4.3449s -1 -1\n",
            "Epoch: 0617 loss_train: 0.7549 acc_train: 0.8240 loss_val: 3.0438 acc_val: 0.7760 time: 4.3512s -1 -1\n",
            "Epoch: 0618 loss_train: 0.7477 acc_train: 0.8180 loss_val: 3.1458 acc_val: 0.7760 time: 4.3576s -1 -1\n",
            "Epoch: 0619 loss_train: 0.7673 acc_train: 0.8160 loss_val: 2.9752 acc_val: 0.7820 time: 4.3641s -1 -1\n",
            "Epoch: 0620 loss_train: 0.7706 acc_train: 0.8140 loss_val: 2.8658 acc_val: 0.7820 time: 4.3702s -1 -1\n",
            "Epoch: 0621 loss_train: 0.7633 acc_train: 0.8240 loss_val: 2.9002 acc_val: 0.7800 time: 4.3765s -1 -1\n",
            "Epoch: 0622 loss_train: 0.7394 acc_train: 0.8200 loss_val: 2.9931 acc_val: 0.7760 time: 4.3828s -1 -1\n",
            "Epoch: 0623 loss_train: 0.7469 acc_train: 0.8140 loss_val: 3.0663 acc_val: 0.7760 time: 4.3889s -1 -1\n",
            "Epoch: 0624 loss_train: 0.7473 acc_train: 0.8280 loss_val: 3.0841 acc_val: 0.7780 time: 4.3955s -1 -1\n",
            "Epoch: 0625 loss_train: 0.7454 acc_train: 0.8220 loss_val: 3.0468 acc_val: 0.7820 time: 4.4017s -1 -1\n",
            "Epoch: 0626 loss_train: 0.7532 acc_train: 0.8200 loss_val: 2.9778 acc_val: 0.7820 time: 4.4077s -1 -1\n",
            "Epoch: 0627 loss_train: 0.7549 acc_train: 0.8160 loss_val: 2.9520 acc_val: 0.7820 time: 4.4139s -1 -1\n",
            "Epoch: 0628 loss_train: 0.7597 acc_train: 0.8140 loss_val: 3.0174 acc_val: 0.7820 time: 4.4200s -1 -1\n",
            "Epoch: 0629 loss_train: 0.7490 acc_train: 0.8260 loss_val: 3.1314 acc_val: 0.7800 time: 4.4262s -1 -1\n",
            "Epoch: 0630 loss_train: 0.7468 acc_train: 0.8280 loss_val: 3.1726 acc_val: 0.7800 time: 4.4325s -1 -1\n",
            "Epoch: 0631 loss_train: 0.7671 acc_train: 0.8160 loss_val: 3.0831 acc_val: 0.7820 time: 4.4387s -1 -1\n",
            "Epoch: 0632 loss_train: 0.7609 acc_train: 0.8200 loss_val: 3.0436 acc_val: 0.7820 time: 4.4446s -1 -1\n",
            "Epoch: 0633 loss_train: 0.7680 acc_train: 0.8160 loss_val: 3.0088 acc_val: 0.7820 time: 4.4507s -1 -1\n",
            "Epoch: 0634 loss_train: 0.7612 acc_train: 0.8140 loss_val: 3.0059 acc_val: 0.7820 time: 4.4571s -1 -1\n",
            "Epoch: 0635 loss_train: 0.7468 acc_train: 0.8200 loss_val: 3.0378 acc_val: 0.7820 time: 4.4632s -1 -1\n",
            "Epoch: 0636 loss_train: 0.7734 acc_train: 0.8180 loss_val: 3.0504 acc_val: 0.7800 time: 4.4692s -1 -1\n",
            "Epoch: 0637 loss_train: 0.7393 acc_train: 0.8240 loss_val: 3.1253 acc_val: 0.7800 time: 4.4757s -1 -1\n",
            "Epoch: 0638 loss_train: 0.7886 acc_train: 0.8060 loss_val: 3.1882 acc_val: 0.7800 time: 4.4819s -1 -1\n",
            "Epoch: 0639 loss_train: 0.7596 acc_train: 0.8100 loss_val: 3.2735 acc_val: 0.7760 time: 4.4881s -1 -1\n",
            "Epoch: 0640 loss_train: 0.7300 acc_train: 0.8280 loss_val: 3.3791 acc_val: 0.7760 time: 4.4947s -1 -1\n",
            "Epoch: 0641 loss_train: 0.7575 acc_train: 0.8240 loss_val: 3.3392 acc_val: 0.7780 time: 4.5012s -1 -1\n",
            "Epoch: 0642 loss_train: 0.7737 acc_train: 0.8160 loss_val: 3.2042 acc_val: 0.7800 time: 4.5073s -1 -1\n",
            "Epoch: 0643 loss_train: 0.7530 acc_train: 0.8280 loss_val: 3.0707 acc_val: 0.7820 time: 4.5134s -1 -1\n",
            "Epoch: 0644 loss_train: 0.7550 acc_train: 0.8200 loss_val: 3.0374 acc_val: 0.7820 time: 4.5197s -1 -1\n",
            "Epoch: 0645 loss_train: 0.7492 acc_train: 0.8220 loss_val: 3.0590 acc_val: 0.7820 time: 4.5258s -1 -1\n",
            "Epoch: 0646 loss_train: 0.7678 acc_train: 0.8180 loss_val: 3.1370 acc_val: 0.7820 time: 4.5357s -1 -1\n",
            "Epoch: 0647 loss_train: 0.7602 acc_train: 0.8220 loss_val: 3.3058 acc_val: 0.7760 time: 4.5423s -1 -1\n",
            "Epoch: 0648 loss_train: 0.7639 acc_train: 0.8160 loss_val: 3.4715 acc_val: 0.7760 time: 4.5485s -1 -1\n",
            "Epoch: 0649 loss_train: 0.7629 acc_train: 0.8140 loss_val: 3.5191 acc_val: 0.7760 time: 4.5545s -1 -1\n",
            "Epoch: 0650 loss_train: 0.7373 acc_train: 0.8300 loss_val: 3.3791 acc_val: 0.7760 time: 4.5607s -1 -1\n",
            "Epoch: 0651 loss_train: 0.7524 acc_train: 0.8140 loss_val: 3.3312 acc_val: 0.7800 time: 4.5670s -1 -1\n",
            "Epoch: 0652 loss_train: 0.7507 acc_train: 0.8120 loss_val: 3.3357 acc_val: 0.7800 time: 4.5729s -1 -1\n",
            "Epoch: 0653 loss_train: 0.7337 acc_train: 0.8260 loss_val: 3.3779 acc_val: 0.7800 time: 4.5792s -1 -1\n",
            "Epoch: 0654 loss_train: 0.7463 acc_train: 0.8220 loss_val: 3.5010 acc_val: 0.7780 time: 4.5856s -1 -1\n",
            "Epoch: 0655 loss_train: 0.7722 acc_train: 0.8140 loss_val: 3.6015 acc_val: 0.7780 time: 4.5917s -1 -1\n",
            "Epoch: 0656 loss_train: 0.7481 acc_train: 0.8260 loss_val: 3.5606 acc_val: 0.7780 time: 4.5979s -1 -1\n",
            "Epoch: 0657 loss_train: 0.7473 acc_train: 0.8220 loss_val: 3.5638 acc_val: 0.7760 time: 4.6039s -1 -1\n",
            "Epoch: 0658 loss_train: 0.7585 acc_train: 0.8260 loss_val: 3.3592 acc_val: 0.7780 time: 4.6113s -1 -1\n",
            "Epoch: 0659 loss_train: 0.7556 acc_train: 0.8300 loss_val: 3.0794 acc_val: 0.7820 time: 4.6173s -1 -1\n",
            "Epoch: 0660 loss_train: 0.7436 acc_train: 0.8260 loss_val: 2.8889 acc_val: 0.7820 time: 4.6236s -1 -1\n",
            "Epoch: 0661 loss_train: 0.7707 acc_train: 0.8100 loss_val: 2.8189 acc_val: 0.7820 time: 4.6296s -1 -1\n",
            "Epoch: 0662 loss_train: 0.7520 acc_train: 0.8220 loss_val: 2.8317 acc_val: 0.7820 time: 4.6356s -1 -1\n",
            "Epoch: 0663 loss_train: 0.7438 acc_train: 0.8260 loss_val: 2.8926 acc_val: 0.7840 time: 4.6417s -1 -1\n",
            "Epoch: 0664 loss_train: 0.7887 acc_train: 0.8140 loss_val: 3.0141 acc_val: 0.7820 time: 4.6479s -1 -1\n",
            "Epoch: 0665 loss_train: 0.7377 acc_train: 0.8280 loss_val: 3.1661 acc_val: 0.7820 time: 4.6553s -1 -1\n",
            "Epoch: 0666 loss_train: 0.7528 acc_train: 0.8180 loss_val: 3.2668 acc_val: 0.7820 time: 4.6651s -1 -1\n",
            "Epoch: 0667 loss_train: 0.7396 acc_train: 0.8320 loss_val: 3.2206 acc_val: 0.7820 time: 4.6713s -1 -1\n",
            "Epoch: 0668 loss_train: 0.7522 acc_train: 0.8180 loss_val: 3.2594 acc_val: 0.7820 time: 4.6776s -1 -1\n",
            "Epoch: 0669 loss_train: 0.7499 acc_train: 0.8240 loss_val: 3.3298 acc_val: 0.7800 time: 4.6835s -1 -1\n",
            "Epoch: 0670 loss_train: 0.7515 acc_train: 0.8280 loss_val: 3.3498 acc_val: 0.7800 time: 4.6895s -1 -1\n",
            "Epoch: 0671 loss_train: 0.7520 acc_train: 0.8160 loss_val: 3.3919 acc_val: 0.7760 time: 4.6957s -1 -1\n",
            "Epoch: 0672 loss_train: 0.7673 acc_train: 0.8200 loss_val: 3.2867 acc_val: 0.7780 time: 4.7020s -1 -1\n",
            "Epoch: 0673 loss_train: 0.7543 acc_train: 0.8220 loss_val: 3.0793 acc_val: 0.7820 time: 4.7081s -1 -1\n",
            "Epoch: 0674 loss_train: 0.7599 acc_train: 0.8140 loss_val: 2.8856 acc_val: 0.7820 time: 4.7150s -1 -1\n",
            "Epoch: 0675 loss_train: 0.7500 acc_train: 0.8240 loss_val: 2.7920 acc_val: 0.7840 time: 4.7236s -1 -1\n",
            "Epoch: 0676 loss_train: 0.7418 acc_train: 0.8280 loss_val: 2.8123 acc_val: 0.7840 time: 4.7305s -1 -1\n",
            "Epoch: 0677 loss_train: 0.7593 acc_train: 0.8160 loss_val: 2.9423 acc_val: 0.7820 time: 4.7378s -1 -1\n",
            "Epoch: 0678 loss_train: 0.7396 acc_train: 0.8240 loss_val: 3.0586 acc_val: 0.7800 time: 4.7429s -1 -1\n",
            "Epoch: 0679 loss_train: 0.7384 acc_train: 0.8280 loss_val: 3.2744 acc_val: 0.7740 time: 4.7533s -1 -1\n",
            "Epoch: 0680 loss_train: 0.7456 acc_train: 0.8240 loss_val: 3.4184 acc_val: 0.7740 time: 4.7598s -1 -1\n",
            "Epoch: 0681 loss_train: 0.7450 acc_train: 0.8280 loss_val: 3.3632 acc_val: 0.7780 time: 4.7660s -1 -1\n",
            "Epoch: 0682 loss_train: 0.7417 acc_train: 0.8300 loss_val: 3.2743 acc_val: 0.7800 time: 4.7722s -1 -1\n",
            "Epoch: 0683 loss_train: 0.7369 acc_train: 0.8240 loss_val: 3.2569 acc_val: 0.7820 time: 4.7786s -1 -1\n",
            "Epoch: 0684 loss_train: 0.7487 acc_train: 0.8180 loss_val: 3.3121 acc_val: 0.7800 time: 4.7848s -1 -1\n",
            "Epoch: 0685 loss_train: 0.7756 acc_train: 0.8140 loss_val: 3.3747 acc_val: 0.7780 time: 4.7912s -1 -1\n",
            "Epoch: 0686 loss_train: 0.7596 acc_train: 0.8140 loss_val: 3.4554 acc_val: 0.7760 time: 4.7974s -1 -1\n",
            "Epoch: 0687 loss_train: 0.7698 acc_train: 0.8220 loss_val: 3.4177 acc_val: 0.7780 time: 4.8036s -1 -1\n",
            "Epoch: 0688 loss_train: 0.7407 acc_train: 0.8280 loss_val: 3.3190 acc_val: 0.7780 time: 4.8098s -1 -1\n",
            "Epoch: 0689 loss_train: 0.7498 acc_train: 0.8200 loss_val: 3.2498 acc_val: 0.7800 time: 4.8160s -1 -1\n",
            "Epoch: 0690 loss_train: 0.7228 acc_train: 0.8320 loss_val: 3.2209 acc_val: 0.7800 time: 4.8223s -1 -1\n",
            "Epoch: 0691 loss_train: 0.7558 acc_train: 0.8120 loss_val: 3.2404 acc_val: 0.7800 time: 4.8282s -1 -1\n",
            "Epoch: 0692 loss_train: 0.7232 acc_train: 0.8340 loss_val: 3.3273 acc_val: 0.7800 time: 4.8342s -1 -1\n",
            "Epoch: 0693 loss_train: 0.7530 acc_train: 0.8180 loss_val: 3.3881 acc_val: 0.7800 time: 4.8404s -1 -1\n",
            "Epoch: 0694 loss_train: 0.7344 acc_train: 0.8300 loss_val: 3.4773 acc_val: 0.7800 time: 4.8465s -1 -1\n",
            "Epoch: 0695 loss_train: 0.7447 acc_train: 0.8220 loss_val: 3.4035 acc_val: 0.7780 time: 4.8525s -1 -1\n",
            "Epoch: 0696 loss_train: 0.7490 acc_train: 0.8220 loss_val: 3.3251 acc_val: 0.7800 time: 4.8586s -1 -1\n",
            "Epoch: 0697 loss_train: 0.7506 acc_train: 0.8200 loss_val: 3.2598 acc_val: 0.7820 time: 4.8647s -1 -1\n",
            "Epoch: 0698 loss_train: 0.7630 acc_train: 0.8120 loss_val: 3.1576 acc_val: 0.7820 time: 4.8710s -1 -1\n",
            "Epoch: 0699 loss_train: 0.7378 acc_train: 0.8220 loss_val: 3.1084 acc_val: 0.7820 time: 4.8774s -1 -1\n",
            "Epoch: 0700 loss_train: 0.7285 acc_train: 0.8260 loss_val: 3.1674 acc_val: 0.7800 time: 4.8835s -1 -1\n",
            "Epoch: 0701 loss_train: 0.7330 acc_train: 0.8280 loss_val: 3.2516 acc_val: 0.7800 time: 4.8898s -1 -1\n",
            "Epoch: 0702 loss_train: 0.7474 acc_train: 0.8200 loss_val: 3.4184 acc_val: 0.7780 time: 4.8959s -1 -1\n",
            "Epoch: 0703 loss_train: 0.7532 acc_train: 0.8180 loss_val: 3.5596 acc_val: 0.7800 time: 4.9018s -1 -1\n",
            "Epoch: 0704 loss_train: 0.7309 acc_train: 0.8340 loss_val: 3.7148 acc_val: 0.7780 time: 4.9079s -1 -1\n",
            "Epoch: 0705 loss_train: 0.7618 acc_train: 0.8200 loss_val: 3.8189 acc_val: 0.7760 time: 4.9141s -1 -1\n",
            "Epoch: 0706 loss_train: 0.7563 acc_train: 0.8200 loss_val: 3.7669 acc_val: 0.7780 time: 4.9203s -1 -1\n",
            "Epoch: 0707 loss_train: 0.7318 acc_train: 0.8280 loss_val: 3.6119 acc_val: 0.7780 time: 4.9263s -1 -1\n",
            "Epoch: 0708 loss_train: 0.7525 acc_train: 0.8180 loss_val: 3.5286 acc_val: 0.7780 time: 4.9326s -1 -1\n",
            "Epoch: 0709 loss_train: 0.7216 acc_train: 0.8340 loss_val: 3.4121 acc_val: 0.7780 time: 4.9389s -1 -1\n",
            "Epoch: 0710 loss_train: 0.7600 acc_train: 0.8200 loss_val: 3.3349 acc_val: 0.7800 time: 4.9451s -1 -1\n",
            "Epoch: 0711 loss_train: 0.7703 acc_train: 0.8180 loss_val: 3.2429 acc_val: 0.7800 time: 4.9512s -1 -1\n",
            "Epoch: 0712 loss_train: 0.7425 acc_train: 0.8200 loss_val: 3.1881 acc_val: 0.7800 time: 4.9617s -1 -1\n",
            "Epoch: 0713 loss_train: 0.7457 acc_train: 0.8240 loss_val: 3.1360 acc_val: 0.7780 time: 4.9679s -1 -1\n",
            "Epoch: 0714 loss_train: 0.7428 acc_train: 0.8200 loss_val: 3.1326 acc_val: 0.7780 time: 4.9741s -1 -1\n",
            "Epoch: 0715 loss_train: 0.7341 acc_train: 0.8200 loss_val: 3.0695 acc_val: 0.7780 time: 4.9803s -1 -1\n",
            "Epoch: 0716 loss_train: 0.7430 acc_train: 0.8300 loss_val: 3.0458 acc_val: 0.7820 time: 4.9864s -1 -1\n",
            "Epoch: 0717 loss_train: 0.7426 acc_train: 0.8240 loss_val: 3.0766 acc_val: 0.7820 time: 4.9927s -1 -1\n",
            "Epoch: 0718 loss_train: 0.7590 acc_train: 0.8120 loss_val: 3.1736 acc_val: 0.7820 time: 4.9990s -1 -1\n",
            "Epoch: 0719 loss_train: 0.7399 acc_train: 0.8280 loss_val: 3.2895 acc_val: 0.7800 time: 5.0053s -1 -1\n",
            "Epoch: 0720 loss_train: 0.7658 acc_train: 0.8240 loss_val: 3.3651 acc_val: 0.7760 time: 5.0116s -1 -1\n",
            "Epoch: 0721 loss_train: 0.7342 acc_train: 0.8240 loss_val: 3.4864 acc_val: 0.7740 time: 5.0180s -1 -1\n",
            "Epoch: 0722 loss_train: 0.7276 acc_train: 0.8320 loss_val: 3.4847 acc_val: 0.7760 time: 5.0242s -1 -1\n",
            "Epoch: 0723 loss_train: 0.7456 acc_train: 0.8160 loss_val: 3.4422 acc_val: 0.7780 time: 5.0304s -1 -1\n",
            "Epoch: 0724 loss_train: 0.7511 acc_train: 0.8280 loss_val: 3.4153 acc_val: 0.7820 time: 5.0365s -1 -1\n",
            "Epoch: 0725 loss_train: 0.7271 acc_train: 0.8240 loss_val: 3.4430 acc_val: 0.7820 time: 5.0425s -1 -1\n",
            "Epoch: 0726 loss_train: 0.7436 acc_train: 0.8160 loss_val: 3.5053 acc_val: 0.7800 time: 5.0487s -1 -1\n",
            "Epoch: 0727 loss_train: 0.7415 acc_train: 0.8260 loss_val: 3.5046 acc_val: 0.7800 time: 5.0548s -1 -1\n",
            "Epoch: 0728 loss_train: 0.7521 acc_train: 0.8180 loss_val: 3.3603 acc_val: 0.7800 time: 5.0610s -1 -1\n",
            "Epoch: 0729 loss_train: 0.7572 acc_train: 0.8300 loss_val: 3.1659 acc_val: 0.7800 time: 5.0670s -1 -1\n",
            "Epoch: 0730 loss_train: 0.7526 acc_train: 0.8200 loss_val: 2.9566 acc_val: 0.7820 time: 5.0730s -1 -1\n",
            "Epoch: 0731 loss_train: 0.7475 acc_train: 0.8200 loss_val: 2.8693 acc_val: 0.7820 time: 5.0792s -1 -1\n",
            "Epoch: 0732 loss_train: 0.7636 acc_train: 0.8220 loss_val: 2.8896 acc_val: 0.7820 time: 5.0853s -1 -1\n",
            "Epoch: 0733 loss_train: 0.7410 acc_train: 0.8200 loss_val: 3.0635 acc_val: 0.7820 time: 5.0915s -1 -1\n",
            "Epoch: 0734 loss_train: 0.7687 acc_train: 0.8180 loss_val: 3.2876 acc_val: 0.7800 time: 5.0979s -1 -1\n",
            "Epoch: 0735 loss_train: 0.7498 acc_train: 0.8160 loss_val: 3.3523 acc_val: 0.7800 time: 5.1059s -1 -1\n",
            "Epoch: 0736 loss_train: 0.7532 acc_train: 0.8240 loss_val: 3.3292 acc_val: 0.7820 time: 5.1125s -1 -1\n",
            "Epoch: 0737 loss_train: 0.7533 acc_train: 0.8280 loss_val: 3.3084 acc_val: 0.7820 time: 5.1185s -1 -1\n",
            "Epoch: 0738 loss_train: 0.7307 acc_train: 0.8260 loss_val: 3.3583 acc_val: 0.7820 time: 5.1248s -1 -1\n",
            "Epoch: 0739 loss_train: 0.7225 acc_train: 0.8300 loss_val: 3.4664 acc_val: 0.7820 time: 5.1326s -1 -1\n",
            "Epoch: 0740 loss_train: 0.7527 acc_train: 0.8140 loss_val: 3.5888 acc_val: 0.7780 time: 5.1388s -1 -1\n",
            "Epoch: 0741 loss_train: 0.7702 acc_train: 0.8140 loss_val: 3.4908 acc_val: 0.7800 time: 5.1450s -1 -1\n",
            "Epoch: 0742 loss_train: 0.7457 acc_train: 0.8200 loss_val: 3.3905 acc_val: 0.7800 time: 5.1512s -1 -1\n",
            "Epoch: 0743 loss_train: 0.7369 acc_train: 0.8240 loss_val: 3.3799 acc_val: 0.7800 time: 5.1598s -1 -1\n",
            "Epoch: 0744 loss_train: 0.7587 acc_train: 0.8200 loss_val: 3.4386 acc_val: 0.7740 time: 5.1656s -1 -1\n",
            "Epoch: 0745 loss_train: 0.7612 acc_train: 0.8200 loss_val: 3.4160 acc_val: 0.7740 time: 5.1777s -1 -1\n",
            "Epoch: 0746 loss_train: 0.7398 acc_train: 0.8200 loss_val: 3.3599 acc_val: 0.7740 time: 5.1843s -1 -1\n",
            "Epoch: 0747 loss_train: 0.7876 acc_train: 0.8100 loss_val: 3.2347 acc_val: 0.7760 time: 5.1908s -1 -1\n",
            "Epoch: 0748 loss_train: 0.7325 acc_train: 0.8260 loss_val: 3.1689 acc_val: 0.7820 time: 5.1976s -1 -1\n",
            "Epoch: 0749 loss_train: 0.7352 acc_train: 0.8300 loss_val: 3.1294 acc_val: 0.7840 time: 5.2037s -1 -1\n",
            "Epoch: 0750 loss_train: 0.7358 acc_train: 0.8260 loss_val: 3.1431 acc_val: 0.7840 time: 5.2096s -1 -1\n",
            "Epoch: 0751 loss_train: 0.7642 acc_train: 0.8140 loss_val: 3.2317 acc_val: 0.7820 time: 5.2158s -1 -1\n",
            "Epoch: 0752 loss_train: 0.7799 acc_train: 0.8060 loss_val: 3.3902 acc_val: 0.7760 time: 5.2218s -1 -1\n",
            "Epoch: 0753 loss_train: 0.7332 acc_train: 0.8260 loss_val: 3.4015 acc_val: 0.7740 time: 5.2281s -1 -1\n",
            "Epoch: 0754 loss_train: 0.7520 acc_train: 0.8200 loss_val: 3.2799 acc_val: 0.7760 time: 5.2353s -1 -1\n",
            "Epoch: 0755 loss_train: 0.7484 acc_train: 0.8240 loss_val: 3.1259 acc_val: 0.7840 time: 5.2416s -1 -1\n",
            "Epoch: 0756 loss_train: 0.7468 acc_train: 0.8200 loss_val: 3.0683 acc_val: 0.7880 time: 5.2477s -1 -1\n",
            "Epoch: 0757 loss_train: 0.7486 acc_train: 0.8240 loss_val: 3.2777 acc_val: 0.7840 time: 5.2539s -1 -1\n",
            "Epoch: 0758 loss_train: 0.7473 acc_train: 0.8140 loss_val: 3.5033 acc_val: 0.7840 time: 5.2612s -1 -1\n",
            "Epoch: 0759 loss_train: 0.7403 acc_train: 0.8280 loss_val: 3.5908 acc_val: 0.7780 time: 5.2662s -1 -1\n",
            "Epoch: 0760 loss_train: 0.7553 acc_train: 0.8260 loss_val: 3.4196 acc_val: 0.7820 time: 5.2814s -1 -1\n",
            "Epoch: 0761 loss_train: 0.7313 acc_train: 0.8340 loss_val: 3.3141 acc_val: 0.7820 time: 5.2882s -1 -1\n",
            "Epoch: 0762 loss_train: 0.7458 acc_train: 0.8220 loss_val: 3.3431 acc_val: 0.7820 time: 5.2945s -1 -1\n",
            "Epoch: 0763 loss_train: 0.7391 acc_train: 0.8200 loss_val: 3.4608 acc_val: 0.7820 time: 5.3007s -1 -1\n",
            "Epoch: 0764 loss_train: 0.7602 acc_train: 0.8300 loss_val: 3.5430 acc_val: 0.7780 time: 5.3068s -1 -1\n",
            "Epoch: 0765 loss_train: 0.7486 acc_train: 0.8100 loss_val: 3.6978 acc_val: 0.7740 time: 5.3131s -1 -1\n",
            "Epoch: 0766 loss_train: 0.7389 acc_train: 0.8260 loss_val: 3.7251 acc_val: 0.7740 time: 5.3194s -1 -1\n",
            "Epoch: 0767 loss_train: 0.7674 acc_train: 0.8180 loss_val: 3.5898 acc_val: 0.7760 time: 5.3262s -1 -1\n",
            "Epoch: 0768 loss_train: 0.7377 acc_train: 0.8260 loss_val: 3.4979 acc_val: 0.7820 time: 5.3323s -1 -1\n",
            "Epoch: 0769 loss_train: 0.7216 acc_train: 0.8320 loss_val: 3.4350 acc_val: 0.7820 time: 5.3387s -1 -1\n",
            "Epoch: 0770 loss_train: 0.7301 acc_train: 0.8260 loss_val: 3.2758 acc_val: 0.7820 time: 5.3448s -1 -1\n",
            "Epoch: 0771 loss_train: 0.7558 acc_train: 0.8160 loss_val: 3.2445 acc_val: 0.7820 time: 5.3508s -1 -1\n",
            "Epoch: 0772 loss_train: 0.7341 acc_train: 0.8160 loss_val: 3.3290 acc_val: 0.7820 time: 5.3571s -1 -1\n",
            "Epoch: 0773 loss_train: 0.7302 acc_train: 0.8220 loss_val: 3.4621 acc_val: 0.7800 time: 5.3631s -1 -1\n",
            "Epoch: 0774 loss_train: 0.7497 acc_train: 0.8180 loss_val: 3.5762 acc_val: 0.7760 time: 5.3694s -1 -1\n",
            "Epoch: 0775 loss_train: 0.7472 acc_train: 0.8240 loss_val: 3.5790 acc_val: 0.7760 time: 5.3796s -1 -1\n",
            "Epoch: 0776 loss_train: 0.7427 acc_train: 0.8240 loss_val: 3.5479 acc_val: 0.7760 time: 5.3891s -1 -1\n",
            "Epoch: 0777 loss_train: 0.7268 acc_train: 0.8340 loss_val: 3.4704 acc_val: 0.7800 time: 5.3956s -1 -1\n",
            "Epoch: 0778 loss_train: 0.7430 acc_train: 0.8260 loss_val: 3.4370 acc_val: 0.7820 time: 5.4020s -1 -1\n",
            "Epoch: 0779 loss_train: 0.7534 acc_train: 0.8140 loss_val: 3.5034 acc_val: 0.7800 time: 5.4084s -1 -1\n",
            "Epoch: 0780 loss_train: 0.7457 acc_train: 0.8240 loss_val: 3.6202 acc_val: 0.7740 time: 5.4150s -1 -1\n",
            "Epoch: 0781 loss_train: 0.7153 acc_train: 0.8360 loss_val: 3.7330 acc_val: 0.7740 time: 5.4213s -1 -1\n",
            "Epoch: 0782 loss_train: 0.7267 acc_train: 0.8260 loss_val: 3.8207 acc_val: 0.7740 time: 5.4280s -1 -1\n",
            "Epoch: 0783 loss_train: 0.7399 acc_train: 0.8300 loss_val: 3.7849 acc_val: 0.7740 time: 5.4346s -1 -1\n",
            "Epoch: 0784 loss_train: 0.7392 acc_train: 0.8220 loss_val: 3.7227 acc_val: 0.7740 time: 5.4413s -1 -1\n",
            "Epoch: 0785 loss_train: 0.7440 acc_train: 0.8200 loss_val: 3.7016 acc_val: 0.7760 time: 5.4480s -1 -1\n",
            "Epoch: 0786 loss_train: 0.7483 acc_train: 0.8240 loss_val: 3.6666 acc_val: 0.7800 time: 5.4545s -1 -1\n",
            "Epoch: 0787 loss_train: 0.7412 acc_train: 0.8300 loss_val: 3.7061 acc_val: 0.7780 time: 5.4611s -1 -1\n",
            "Epoch: 0788 loss_train: 0.7501 acc_train: 0.8220 loss_val: 3.7101 acc_val: 0.7780 time: 5.4677s -1 -1\n",
            "Epoch: 0789 loss_train: 0.7491 acc_train: 0.8160 loss_val: 3.7004 acc_val: 0.7800 time: 5.4742s -1 -1\n",
            "Epoch: 0790 loss_train: 0.7543 acc_train: 0.8140 loss_val: 3.6924 acc_val: 0.7800 time: 5.4812s -1 -1\n",
            "Epoch: 0791 loss_train: 0.7482 acc_train: 0.8300 loss_val: 3.7909 acc_val: 0.7780 time: 5.4878s -1 -1\n",
            "Epoch: 0792 loss_train: 0.7199 acc_train: 0.8280 loss_val: 3.8375 acc_val: 0.7720 time: 5.4944s -1 -1\n",
            "Epoch: 0793 loss_train: 0.7427 acc_train: 0.8280 loss_val: 3.6862 acc_val: 0.7720 time: 5.5010s -1 -1\n",
            "Epoch: 0794 loss_train: 0.7577 acc_train: 0.8140 loss_val: 3.4576 acc_val: 0.7780 time: 5.5076s -1 -1\n",
            "Epoch: 0795 loss_train: 0.7539 acc_train: 0.8200 loss_val: 3.3480 acc_val: 0.7820 time: 5.5143s -1 -1\n",
            "Epoch: 0796 loss_train: 0.7244 acc_train: 0.8280 loss_val: 3.3616 acc_val: 0.7820 time: 5.5207s -1 -1\n",
            "Epoch: 0797 loss_train: 0.7348 acc_train: 0.8220 loss_val: 3.5144 acc_val: 0.7780 time: 5.5269s -1 -1\n",
            "Epoch: 0798 loss_train: 0.7604 acc_train: 0.8240 loss_val: 3.6674 acc_val: 0.7720 time: 5.5333s -1 -1\n",
            "Epoch: 0799 loss_train: 0.7520 acc_train: 0.8260 loss_val: 3.8128 acc_val: 0.7720 time: 5.5393s -1 -1\n",
            "Epoch: 0800 loss_train: 0.7529 acc_train: 0.8240 loss_val: 3.7750 acc_val: 0.7740 time: 5.5452s -1 -1\n",
            "Test set results: loss= 5.0970 accuracy= 0.0194 act_dec_test= -1.0000\n",
            "GCN(\n",
            "  (gc1): GraphConvolution()\n",
            "  (gc2): GraphConvolution()\n",
            ")\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Optimization Finished!\n",
            "Total time elapsed: 5.9417s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhEk4gRHSOny"
      },
      "source": [
        "#labels_test_hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wyQvWnzSKCDq"
      },
      "source": [
        "#labels_test_hidden[467]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmO9s0xfJ4OQ"
      },
      "source": [
        "#labels_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBmQ-VOGXADS"
      },
      "source": [
        "#labels_test[467]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09oMI4PcWZEf"
      },
      "source": [
        "#labels_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6IMUMXEUMCS"
      },
      "source": [
        "#labels_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r46Svk3DbIpp"
      },
      "source": [
        "#Classify whole graph\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zoA3OfE3agdf",
        "outputId": "c6ea9a8d-51de-4b5b-fe50-f2f820bd5af4"
      },
      "source": [
        "prediction,predicted_output,act_dec = model(features,adj,hidden_labels,-1)\n",
        "\n",
        "acc_all=accuracy(prediction,labels)\n",
        "print(\n",
        "          \"accuracy_Of_Classification= {:.4f}\".format(acc_all.item())\n",
        "        #  ,predicted_output\n",
        "          )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy_Of_Classification= 0.1469\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1f3YWI6MUew"
      },
      "source": [
        "predicted_class=[]\n",
        "for i in range(len(labels)):\n",
        "  predicted_class.append(torch.argmax(predicted_output[i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-XFQPZjNY9f"
      },
      "source": [
        " predicted_class=torch.LongTensor(predicted_class)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15pOWgBcx_rl"
      },
      "source": [
        "#prediction[20]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "yP690T9UbrHL",
        "outputId": "94e04234-ee5c-495e-90a5-c8a20085f7d5"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame({\"Predicted\": predicted_class, \"True\": labels})\n",
        "df=df.loc [df['Predicted']==df['True']]\n",
        "df\n",
        "#df.head(1000)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Predicted</th>\n",
              "      <th>True</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>10</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13688</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13703</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13704</th>\n",
              "      <td>10</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13743</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13746</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2020 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Predicted  True\n",
              "20            10    10\n",
              "22             3     3\n",
              "29             3     3\n",
              "37             3     3\n",
              "48             3     3\n",
              "...          ...   ...\n",
              "13688          3     3\n",
              "13703          3     3\n",
              "13704         10    10\n",
              "13743          3     3\n",
              "13746          3     3\n",
              "\n",
              "[2020 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6AKDwxIvcl_o",
        "outputId": "c7d60c46-5069-4982-c21f-6360a5ee2bca"
      },
      "source": [
        "df.count()# True prediction out of 13752\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Predicted    2020\n",
              "True         2020\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    }
  ]
}