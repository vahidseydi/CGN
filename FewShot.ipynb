{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FewShot.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "https://github.com/vahidseydi/CGN/blob/main/GCN.ipynb",
      "authorship_tag": "ABX9TyPdfNm+Fxivl5BvYSLpyV68",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vahidseydi/CGN/blob/main/FewShot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DucMDBmguhnh"
      },
      "source": [
        "Downloading dataset file from Github "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnHBK6RWuN9j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ef41062-0a86-4de7-b302-9a81538a5eea"
      },
      "source": [
        "! wget 'https://github.com/vahidseydi/CGN/blob/main/Data/amazon_electronics_computers%20(1).npz?raw=true'"
      ],
      "execution_count": 217,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-15 09:49:24--  https://github.com/vahidseydi/CGN/blob/main/Data/amazon_electronics_computers%20(1).npz?raw=true\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/vahidseydi/CGN/raw/main/Data/amazon_electronics_computers%20(1).npz [following]\n",
            "--2021-05-15 09:49:24--  https://github.com/vahidseydi/CGN/raw/main/Data/amazon_electronics_computers%20(1).npz\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/vahidseydi/CGN/main/Data/amazon_electronics_computers%20(1).npz [following]\n",
            "--2021-05-15 09:49:24--  https://raw.githubusercontent.com/vahidseydi/CGN/main/Data/amazon_electronics_computers%20(1).npz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 31921488 (30M) [application/octet-stream]\n",
            "Saving to: ‘amazon_electronics_computers (1).npz?raw=true.4’\n",
            "\n",
            "amazon_electronics_ 100%[===================>]  30.44M   164MB/s    in 0.2s    \n",
            "\n",
            "2021-05-15 09:49:25 (164 MB/s) - ‘amazon_electronics_computers (1).npz?raw=true.4’ saved [31921488/31921488]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MYbUcRVdtDe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7af9c34-1cfa-4fb2-ac08-835604b0cab5"
      },
      "source": [
        "import numpy as np\n",
        "npz_data=np.load('/content/amazon_electronics_computers (1).npz?raw=true')\n",
        "npz_data.files"
      ],
      "execution_count": 218,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['adj_data',\n",
              " 'adj_indices',\n",
              " 'adj_indptr',\n",
              " 'adj_shape',\n",
              " 'attr_data',\n",
              " 'attr_indices',\n",
              " 'attr_indptr',\n",
              " 'attr_shape',\n",
              " 'labels',\n",
              " 'class_names']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 218
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmNiWW8bd-Xx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cd9a5a7-88f6-4c4e-a049-43a1f725bbf9"
      },
      "source": [
        "class_names =npz_data['class_names']\n",
        "class_names"
      ],
      "execution_count": 219,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Desktops', 'Data Storage', 'Laptops', 'Monitors',\n",
              "       'Computer Components', 'Video Projectors', 'Routers', 'Tablets',\n",
              "       'Networking Products', 'Webcams'], dtype='<U19')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 219
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFpxP_MNMmCX"
      },
      "source": [
        "labels =npz_data['labels']\n",
        "y=labels.size\n",
        "#p for calculating percentage for idx_train,val,test\n",
        "p = lambda x: x*y/100"
      ],
      "execution_count": 220,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9NewBDvWAdE"
      },
      "source": [
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import torch\n",
        "from scipy.sparse import  csr_matrix\n",
        "\n",
        "\n",
        "def load_data():\n",
        "\n",
        " features = sp.csr_matrix((npz_data['attr_data'], npz_data['attr_indices'], npz_data['attr_indptr']),\n",
        "                                        shape=npz_data['attr_shape'])\n",
        "\n",
        " # build graph\n",
        " \n",
        " adj= sp.csr_matrix(sp.csr_matrix((npz_data['adj_data'], npz_data['adj_indices'], npz_data['adj_indptr']),\n",
        "                                   shape=npz_data['adj_shape']))\n",
        " \n",
        " # build symmetric adjacency matrix\n",
        "\n",
        " adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
        " adj = normalize(adj + sp.eye(adj.shape[0]))\n",
        " #اول اعداد 0 و 1 هست بعد از نرمالایز تغییر میکند \n",
        "\n",
        " labels=npz_data['labels']\n",
        " #لیبل ها را یکی اضافه می کنیم تا لیبل 0 را به داده های بدون لیبل دهیم\n",
        " labels=labels+1\n",
        "\n",
        " idx_train =range(round(p(40)))\n",
        " idx_val = range(idx_train[-1],idx_train[-1]+round(p(30)))\n",
        " idx_test =range(idx_val[-1],idx_val[-1]+round(p(30)))\n",
        "\n",
        " idx_train = torch.LongTensor(idx_train)\n",
        " idx_val = torch.LongTensor(idx_val)\n",
        " idx_test = torch.LongTensor(idx_test)\n",
        "\n",
        " features = torch.FloatTensor(np.array(features.todense()))\n",
        " adj = torch.FloatTensor(np.array(adj.todense()))\n",
        " labels = torch.LongTensor(labels)\n",
        "\n",
        "\n",
        " return adj, features, labels, idx_train, idx_val, idx_test"
      ],
      "execution_count": 221,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9YheZ8yNTSL"
      },
      "source": [
        "def normalize(mx):\n",
        "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
        "    rowsum = np.array(mx.sum(1))\n",
        "    #sum in every row \n",
        "    r_inv = np.power(rowsum, -1).flatten()\n",
        "    # every sum to the power of -1 \n",
        "    r_inv[np.isinf(r_inv)] = 0.\n",
        "    r_mat_inv = sp.diags(r_inv)\n",
        "    #diagonal matrice \n",
        "    mx = r_mat_inv.dot(mx)\n",
        "    return mx"
      ],
      "execution_count": 222,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQLBOry4WJvV"
      },
      "source": [
        "adj, features, labels, idx_train, idx_val, idx_test = load_data()\n"
      ],
      "execution_count": 223,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsHyFUvEUMFQ"
      },
      "source": [
        "Constructing Train data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0-99ajritPS"
      },
      "source": [
        "#20% of train data for labeled\n",
        "#80% of train data for unlabeled\n",
        "\n",
        "p_train = lambda x: x*len(idx_train)/100\n",
        "\n",
        "idx_labeled_train =range(round(p_train(20)))\n",
        "idx_unlabeled_train = range(idx_labeled_train[-1],idx_labeled_train[-1]+round(p_train(80)))\n"
      ],
      "execution_count": 224,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUovgcgTOAmt",
        "outputId": "121db927-244c-466b-8546-0616eb417737"
      },
      "source": [
        "print(idx_labeled_train,idx_unlabeled_train)"
      ],
      "execution_count": 225,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "range(0, 1100) range(1099, 5500)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6003R_bp4Xkz"
      },
      "source": [
        "#اندیس مربوط به هر کلاسی را جدا می کنیم\n",
        "class_1=[i for i,x in enumerate(labels[idx_labeled_train]) if x==1] \n",
        "class_2=[i for i,x in enumerate(labels[idx_labeled_train]) if x==2] \n",
        "class_3=[i for i,x in enumerate(labels[idx_labeled_train]) if x==3]\n",
        "class_4=[i for i,x in enumerate(labels[idx_labeled_train]) if x==4] \n",
        "class_5=[i for i,x in enumerate(labels[idx_labeled_train]) if x==5] \n",
        "class_6=[i for i,x in enumerate(labels[idx_labeled_train]) if x==6] \n",
        "class_7=[i for i,x in enumerate(labels[idx_labeled_train]) if x==7] \n",
        "class_8=[i for i,x in enumerate(labels[idx_labeled_train]) if x==8] \n",
        "class_9=[i for i,x in enumerate(labels[idx_labeled_train]) if x==9] \n",
        "class_10=[i for i,x in enumerate(labels[idx_labeled_train]) if x==10] "
      ],
      "execution_count": 226,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DEj19oZ9yVr",
        "outputId": "b9daad89-68ca-4459-e036-2912dddc6ee0"
      },
      "source": [
        "# از هر کلاسی بتوانیم به صورت رندوم سمپل انتخاب کنیم\n",
        "import random\n",
        "Random_idx_train=[]\n",
        "N_shot=1\n",
        "for i in range(N_shot):\n",
        "  Random_idx_train.append(random.choice(class_1))\n",
        "  Random_idx_train.append(random.choice(class_2))\n",
        "  Random_idx_train.append(random.choice(class_3))\n",
        "  Random_idx_train.append(random.choice(class_4))\n",
        "  Random_idx_train.append(random.choice(class_5))\n",
        "  Random_idx_train.append(random.choice(class_6))\n",
        "  Random_idx_train.append(random.choice(class_7))\n",
        "  Random_idx_train.append(random.choice(class_8))\n",
        "  Random_idx_train.append(random.choice(class_9))\n",
        "  Random_idx_train.append(random.choice(class_10))\n",
        "\n",
        "Random_idx_train#اندیس "
      ],
      "execution_count": 227,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[417, 599, 788, 1065, 708, 456, 912, 768, 390, 845]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 227
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1_KllTImV3U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "606143dd-89a6-4a61-8baa-ad90715728ad"
      },
      "source": [
        "#اگر بخواهیم از هر کلاس یک سمپل انتخاب کنیم مجموعا 10 سمپل با لیبل داریم \n",
        "#اگر بخواهیم نسبت 20 به 80 را ارعایت کنیم پس لازم است 40 داده بدون لیبل هم انتخاب کنیم\n",
        "\n",
        "hiddenIndex_train=[]\n",
        "\n",
        "for i in range(40*N_shot):\n",
        "  hiddenIndex_train.append(random.choice(idx_unlabeled_train))\n",
        "hiddenIndex_train  "
      ],
      "execution_count": 228,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1370,\n",
              " 2158,\n",
              " 4848,\n",
              " 3374,\n",
              " 1798,\n",
              " 4758,\n",
              " 3951,\n",
              " 1300,\n",
              " 2627,\n",
              " 5446,\n",
              " 3892,\n",
              " 3424,\n",
              " 2332,\n",
              " 4404,\n",
              " 1558,\n",
              " 1148,\n",
              " 3311,\n",
              " 3720,\n",
              " 3009,\n",
              " 1636,\n",
              " 4135,\n",
              " 4248,\n",
              " 1810,\n",
              " 3500,\n",
              " 2655,\n",
              " 2619,\n",
              " 3749,\n",
              " 1860,\n",
              " 4709,\n",
              " 2779,\n",
              " 1202,\n",
              " 5479,\n",
              " 5088,\n",
              " 5293,\n",
              " 5110,\n",
              " 1220,\n",
              " 3940,\n",
              " 2616,\n",
              " 1614,\n",
              " 4359]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 228
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJUGxxHm_0lU",
        "outputId": "15bd8301-58b4-41c2-9165-e332487fff03"
      },
      "source": [
        "labels[1642]"
      ],
      "execution_count": 229,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 229
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1C808icQZ2u_"
      },
      "source": [
        "#traindata=Random_labeled+hiddenlabels\n"
      ],
      "execution_count": 230,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ieH7hStLQlKV",
        "outputId": "53f5f4c6-6ee7-4bad-b268-307e518f4b48"
      },
      "source": [
        "Random_idx_train"
      ],
      "execution_count": 231,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[417, 599, 788, 1065, 708, 456, 912, 768, 390, 845]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 231
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRhvyy9HgXT-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b58ee279-37b8-4980-84d9-bd07c07ce753"
      },
      "source": [
        "labels_train_hidden=torch.hstack((labels[Random_idx_train],labels[hiddenIndex_train]))\n",
        "labels_train_hidden.shape"
      ],
      "execution_count": 232,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([50])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 232
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8OE8yFZhkFvI",
        "outputId": "56c29e56-a100-4503-ab07-053a31d9c2a0"
      },
      "source": [
        "labels[Random_idx_train]"
      ],
      "execution_count": 233,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 233
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7s5MNhNLgsxd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24dd1c18-73ef-4be1-f027-2dac8f2196a6"
      },
      "source": [
        "labels_train_hidden\n",
        "#تمام لیبل ها حتی هیدن ها،برای استفاده در اکتیولرنینگ"
      ],
      "execution_count": 234,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10,  9,  5,  2,  2,  2,  5,  5,  4,\n",
              "         7,  5,  7,  5,  5,  2,  5,  8,  6,  3,  5,  2,  5,  5,  7,  2,  5,  7,\n",
              "         8,  7,  5,  9,  5,  3,  5,  4,  6,  3,  4,  5,  5,  7])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 234
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOr1K7YJm7td",
        "outputId": "971eee4b-b912-4528-cbba-bfe012bb8de0"
      },
      "source": [
        "labels[Random_idx_train]==labels_train_hidden[0:10*N_shot]\n",
        "labels[hiddenIndex_train]==labels_train_hidden[10*N_shot:10*N_shot*4+10*N_shot]"
      ],
      "execution_count": 235,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 235
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7Lwde1zkTe9"
      },
      "source": [
        "labels_train=labels_train_hidden\n",
        "#با توجه به اینکه از هر کلاس چند تا سمپل انتخاب کردیم از شماره آخری بقیه مربوط به هیدن ها میشود که صفر میذاریم\n",
        "labels_train[10*N_shot+1:]=0"
      ],
      "execution_count": 236,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EteMi10CkrbR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f2003bd-14bb-4d03-eb87-54325c5a64ca"
      },
      "source": [
        "labels_train"
      ],
      "execution_count": 237,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10,  9,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 237
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FiNVOiOZf9OM",
        "outputId": "6d2c5479-8991-461c-b5a8-8ea82cf23095"
      },
      "source": [
        "features_train=torch.vstack((features[Random_idx_train],features[hiddenIndex_train]))\n",
        "features_train.shape"
      ],
      "execution_count": 238,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([50, 767])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 238
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R6bLiV3Ynb0l",
        "outputId": "cf91c225-c989-4fa3-e668-d7f7c9d25291"
      },
      "source": [
        "features_train[0:10*N_shot]==features[Random_idx_train]\n",
        "features_train[10*N_shot:10*N_shot*4+10*N_shot]==features[hiddenIndex_train]"
      ],
      "execution_count": 239,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[True, True, True,  ..., True, True, True],\n",
              "        [True, True, True,  ..., True, True, True],\n",
              "        [True, True, True,  ..., True, True, True],\n",
              "        ...,\n",
              "        [True, True, True,  ..., True, True, True],\n",
              "        [True, True, True,  ..., True, True, True],\n",
              "        [True, True, True,  ..., True, True, True]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 239
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28I52FrOiii1",
        "outputId": "a8913ab1-309b-40f3-84e0-45d43946834b"
      },
      "source": [
        "adj[1:10*N_shot,1:10*N_shot].shape"
      ],
      "execution_count": 240,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([9, 9])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 240
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4x-xGrv0Sw0"
      },
      "source": [
        "adj_train=torch.vstack((adj[Random_idx_train],adj[hiddenIndex_train]))\n",
        "\n",
        "for i in hiddenIndex_train :\n",
        "    Random_idx_train.append(i)\n",
        "\n",
        "adj_train=adj_train[0:10*N_shot*4+10*N_shot,Random_idx_train]"
      ],
      "execution_count": 241,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kerWgiRDBcDP",
        "outputId": "c8a4ead5-54a9-4489-e05d-7df3fa47c04a"
      },
      "source": [
        "Random_idx_train[2]"
      ],
      "execution_count": 242,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "788"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 242
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCpOOU5-83cf",
        "outputId": "39b26811-6355-45bc-e79b-cf29e05438e8"
      },
      "source": [
        "adj[717].shape"
      ],
      "execution_count": 243,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([13752])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 243
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4yLSWzV83Z6",
        "outputId": "0abcd9bd-3c55-453c-ade5-ae996824923e"
      },
      "source": [
        "adj_train[0].shape"
      ],
      "execution_count": 244,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([50])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 244
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08hYvuat7Z5w",
        "outputId": "4252235a-e7fe-4cb7-92bc-4bb35dd0da9f"
      },
      "source": [
        "adj[782,782]\n",
        "adj[30,30]"
      ],
      "execution_count": 245,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.1429)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 245
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elrp9c9FTi2w"
      },
      "source": [
        "Constructing Test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jorUo0cyTv7o"
      },
      "source": [
        "#20% of test data for labeled\n",
        "#80% of test data for unlabeled\n",
        "\n",
        "p_test = lambda x: x*len(idx_test)/100\n",
        "\n",
        "idx_labeled_test =range(round(p_test(20)))\n",
        "idx_unlabeled_test = range(idx_labeled_test[-1],idx_labeled_test[-1]+round(p_test(80)))\n"
      ],
      "execution_count": 246,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZR91mjVTv7q",
        "outputId": "55985c65-4b84-4a5a-e2d6-086d57521925"
      },
      "source": [
        "print(idx_labeled_test,idx_unlabeled_test)"
      ],
      "execution_count": 247,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "range(0, 825) range(824, 4125)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRdkBXRUTv7s"
      },
      "source": [
        "#اندیس مربوط به هر کلاسی را جدا می کنیم\n",
        "class_1=[i for i,x in enumerate(labels[idx_labeled_test]) if x==1] \n",
        "class_2=[i for i,x in enumerate(labels[idx_labeled_test]) if x==2] \n",
        "class_3=[i for i,x in enumerate(labels[idx_labeled_test]) if x==3]\n",
        "class_4=[i for i,x in enumerate(labels[idx_labeled_test]) if x==4] \n",
        "class_5=[i for i,x in enumerate(labels[idx_labeled_test]) if x==5] \n",
        "class_6=[i for i,x in enumerate(labels[idx_labeled_test]) if x==6] \n",
        "class_7=[i for i,x in enumerate(labels[idx_labeled_test]) if x==7] \n",
        "class_8=[i for i,x in enumerate(labels[idx_labeled_test]) if x==8] \n",
        "class_9=[i for i,x in enumerate(labels[idx_labeled_test]) if x==9] \n",
        "class_10=[i for i,x in enumerate(labels[idx_labeled_test]) if x==10] "
      ],
      "execution_count": 248,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3AE5Sw2ZTv7t",
        "outputId": "68ef8a1a-2ac4-4033-83b1-de30b1628523"
      },
      "source": [
        "# از هر کلاسی بتوانیم به صورت رندوم سمپل انتخاب کنیم\n",
        "import random\n",
        "Random_idx_test=[]\n",
        "\n",
        "for i in range(N_shot):\n",
        "  Random_idx_test.append(random.choice(class_1))\n",
        "  Random_idx_test.append(random.choice(class_2))\n",
        "  Random_idx_test.append(random.choice(class_3))\n",
        "  Random_idx_test.append(random.choice(class_4))\n",
        "  Random_idx_test.append(random.choice(class_5))\n",
        "  Random_idx_test.append(random.choice(class_6))\n",
        "  Random_idx_test.append(random.choice(class_7))\n",
        "  Random_idx_test.append(random.choice(class_8))\n",
        "  Random_idx_test.append(random.choice(class_9))\n",
        "  Random_idx_test.append(random.choice(class_10))\n",
        "\n",
        "Random_idx_test#اندیس "
      ],
      "execution_count": 249,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[371, 554, 434, 181, 727, 456, 236, 102, 174, 357]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 249
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rv7SWldYTv7u",
        "outputId": "71de9557-7dda-41a1-aa26-47d3edf54955"
      },
      "source": [
        "#اگر بخواهیم از هر کلاس یک سمپل انتخاب کنیم مجموعا 10 سمپل با لیبل داریم \n",
        "#اگر بخواهیم نسبت 20 به 80 را ارعایت کنیم پس لازم است 40 داده بدون لیبل هم انتخاب کنیم\n",
        "\n",
        "hiddenIndex_test=[]\n",
        "\n",
        "for i in range(40*N_shot):\n",
        "  hiddenIndex_test.append(random.choice(idx_unlabeled_test))\n",
        "hiddenIndex_test "
      ],
      "execution_count": 250,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2144,\n",
              " 2415,\n",
              " 2017,\n",
              " 2860,\n",
              " 3244,\n",
              " 2933,\n",
              " 1339,\n",
              " 2200,\n",
              " 3456,\n",
              " 1548,\n",
              " 1965,\n",
              " 1267,\n",
              " 2840,\n",
              " 3541,\n",
              " 2598,\n",
              " 3886,\n",
              " 1088,\n",
              " 3939,\n",
              " 1435,\n",
              " 891,\n",
              " 1247,\n",
              " 3623,\n",
              " 1988,\n",
              " 3486,\n",
              " 832,\n",
              " 947,\n",
              " 1571,\n",
              " 2975,\n",
              " 4113,\n",
              " 4033,\n",
              " 3580,\n",
              " 2609,\n",
              " 3333,\n",
              " 1399,\n",
              " 2252,\n",
              " 839,\n",
              " 3458,\n",
              " 3411,\n",
              " 3840,\n",
              " 3727]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 250
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDcysoakTv7v",
        "outputId": "884f10c6-dfe8-4353-b3d0-77a090959cb4"
      },
      "source": [
        "labels_test_hidden=torch.hstack((labels[Random_idx_test],labels[hiddenIndex_test]))\n",
        "labels_test_hidden.shape"
      ],
      "execution_count": 251,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([50])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 251
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRnuyYHiTv7w"
      },
      "source": [
        "labels_test=labels_test_hidden\n",
        "#با توجه به اینکه از هر کلاس چند تا سمپل انتخاب کردیم از شماره آخری بقیه مربوط به هیدن ها میشود که صفر میذاریم\n",
        "labels_test[10*N_shot+1:]=0"
      ],
      "execution_count": 252,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xtqi7LwXTv7x",
        "outputId": "5ac91731-58f7-4cd0-9dcd-b8e0afe017e3"
      },
      "source": [
        "features_test=torch.vstack((features[Random_idx_test],features[hiddenIndex_test]))\n",
        "features_test.shape"
      ],
      "execution_count": 253,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([50, 767])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 253
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnW1mVA5Tv7y",
        "outputId": "7bb736aa-7298-49f4-9bf2-704bcc1941fc"
      },
      "source": [
        "features_test[0:10*N_shot]==features[Random_idx_test]\n",
        "features_test[10*N_shot:10*N_shot*4+10*N_shot]==features[hiddenIndex_test]"
      ],
      "execution_count": 254,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[True, True, True,  ..., True, True, True],\n",
              "        [True, True, True,  ..., True, True, True],\n",
              "        [True, True, True,  ..., True, True, True],\n",
              "        ...,\n",
              "        [True, True, True,  ..., True, True, True],\n",
              "        [True, True, True,  ..., True, True, True],\n",
              "        [True, True, True,  ..., True, True, True]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 254
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3CLd-KVTv7z"
      },
      "source": [
        "adj_test=torch.vstack((adj[Random_idx_test],adj[hiddenIndex_test]))\n",
        "\n",
        "for i in hiddenIndex_test :\n",
        "    Random_idx_test.append(i)\n",
        "\n",
        "adj_test=adj_test[0:10*N_shot*4+10*N_shot,Random_idx_test]"
      ],
      "execution_count": 255,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dyz6oDB6Yyuf"
      },
      "source": [
        "Constructing Validation Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRkhidE6YWTu"
      },
      "source": [
        "#20% of val data for labeled\n",
        "#80% of val data for unlabeled\n",
        "\n",
        "p_val = lambda x: x*len(idx_val)/100\n",
        "\n",
        "idx_labeled_val =range(round(p_val(20)))\n",
        "idx_unlabeled_val = range(idx_labeled_val[-1],idx_labeled_val[-1]+round(p_val(80)))\n"
      ],
      "execution_count": 256,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqHcKhKYYWTw",
        "outputId": "ab656c07-5bf8-49bc-fa06-ef703ed27dad"
      },
      "source": [
        "print(idx_labeled_val,idx_unlabeled_val)"
      ],
      "execution_count": 257,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "range(0, 825) range(824, 4125)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hR67HnajYWUE"
      },
      "source": [
        "#اندیس مربوط به هر کلاسی را جدا می کنیم\n",
        "class_1=[i for i,x in enumerate(labels[idx_labeled_val]) if x==1] \n",
        "class_2=[i for i,x in enumerate(labels[idx_labeled_val]) if x==2] \n",
        "class_3=[i for i,x in enumerate(labels[idx_labeled_val]) if x==3]\n",
        "class_4=[i for i,x in enumerate(labels[idx_labeled_val]) if x==4] \n",
        "class_5=[i for i,x in enumerate(labels[idx_labeled_val]) if x==5] \n",
        "class_6=[i for i,x in enumerate(labels[idx_labeled_val]) if x==6] \n",
        "class_7=[i for i,x in enumerate(labels[idx_labeled_val]) if x==7] \n",
        "class_8=[i for i,x in enumerate(labels[idx_labeled_val]) if x==8] \n",
        "class_9=[i for i,x in enumerate(labels[idx_labeled_val]) if x==9] \n",
        "class_10=[i for i,x in enumerate(labels[idx_labeled_val]) if x==10] "
      ],
      "execution_count": 258,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9H3cAsQYWUF",
        "outputId": "2d98a1f1-d489-40f4-c515-016d0499bad6"
      },
      "source": [
        "# از هر کلاسی بتوانیم به صورت رندوم سمپل انتخاب کنیم\n",
        "import random\n",
        "Random_idx_val=[]\n",
        "\n",
        "for i in range(N_shot):\n",
        "  Random_idx_val.append(random.choice(class_1))\n",
        "  Random_idx_val.append(random.choice(class_2))\n",
        "  Random_idx_val.append(random.choice(class_3))\n",
        "  Random_idx_val.append(random.choice(class_4))\n",
        "  Random_idx_val.append(random.choice(class_5))\n",
        "  Random_idx_val.append(random.choice(class_6))\n",
        "  Random_idx_val.append(random.choice(class_7))\n",
        "  Random_idx_val.append(random.choice(class_8))\n",
        "  Random_idx_val.append(random.choice(class_9))\n",
        "  Random_idx_val.append(random.choice(class_10))\n",
        "\n",
        "Random_idx_val#اندیس "
      ],
      "execution_count": 259,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[307, 483, 695, 300, 71, 59, 493, 720, 774, 136]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 259
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0eRUuhJqYWUH",
        "outputId": "a5177d39-86a2-4a7d-a814-6a4f3c482f66"
      },
      "source": [
        "#اگر بخواهیم از هر کلاس یک سمپل انتخاب کنیم مجموعا 10 سمپل با لیبل داریم \n",
        "#اگر بخواهیم نسبت 20 به 80 را ارعایت کنیم پس لازم است 40 داده بدون لیبل هم انتخاب کنیم\n",
        "\n",
        "hiddenIndex_val=[]\n",
        "\n",
        "for i in range(40*N_shot):\n",
        "  hiddenIndex_val.append(random.choice(idx_unlabeled_val))\n",
        "hiddenIndex_val "
      ],
      "execution_count": 260,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2217,\n",
              " 2542,\n",
              " 1483,\n",
              " 3190,\n",
              " 1643,\n",
              " 3748,\n",
              " 1306,\n",
              " 919,\n",
              " 1806,\n",
              " 2528,\n",
              " 1565,\n",
              " 2602,\n",
              " 4041,\n",
              " 2260,\n",
              " 1668,\n",
              " 2868,\n",
              " 929,\n",
              " 1895,\n",
              " 3901,\n",
              " 1279,\n",
              " 1622,\n",
              " 2927,\n",
              " 2765,\n",
              " 1757,\n",
              " 3701,\n",
              " 4014,\n",
              " 2973,\n",
              " 1341,\n",
              " 1288,\n",
              " 1403,\n",
              " 3786,\n",
              " 962,\n",
              " 1221,\n",
              " 1338,\n",
              " 2101,\n",
              " 2446,\n",
              " 2752,\n",
              " 2110,\n",
              " 1620,\n",
              " 951]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 260
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GGJZ-_e4YWUI",
        "outputId": "3907f923-031d-4b1a-9e74-8994fd7e53ea"
      },
      "source": [
        "labels_val_hidden=torch.hstack((labels[Random_idx_val],labels[hiddenIndex_val]))\n",
        "labels_val_hidden.shape"
      ],
      "execution_count": 261,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([50])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 261
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEhEarXCYWUI"
      },
      "source": [
        "labels_val=labels_val_hidden\n",
        "#با توجه به اینکه از هر کلاس چند تا سمپل انتخاب کردیم از شماره آخری بقیه مربوط به هیدن ها میشود که صفر میذاریم\n",
        "labels_val[10*N_shot+1:]=0"
      ],
      "execution_count": 262,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3F00cnT2YWUJ",
        "outputId": "0cf3974b-0b9b-4c60-8cdb-8d40d7df4c7e"
      },
      "source": [
        "features_val=torch.vstack((features[Random_idx_val],features[hiddenIndex_val]))\n",
        "features_val.shape"
      ],
      "execution_count": 263,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([50, 767])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 263
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_oHe-7abYWUK",
        "outputId": "63b7db78-9086-4535-96b2-070d9dbf1eb9"
      },
      "source": [
        "features_val[0:10*N_shot]==features[Random_idx_val]\n",
        "features_val[10*N_shot:10*N_shot*4+10*N_shot]==features[hiddenIndex_val]"
      ],
      "execution_count": 264,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[True, True, True,  ..., True, True, True],\n",
              "        [True, True, True,  ..., True, True, True],\n",
              "        [True, True, True,  ..., True, True, True],\n",
              "        ...,\n",
              "        [True, True, True,  ..., True, True, True],\n",
              "        [True, True, True,  ..., True, True, True],\n",
              "        [True, True, True,  ..., True, True, True]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 264
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJhiH9xIYWUL"
      },
      "source": [
        "adj_val=torch.vstack((adj[Random_idx_val],adj[hiddenIndex_val]))\n",
        "\n",
        "for i in hiddenIndex_val :\n",
        "    Random_idx_val.append(i)\n",
        "\n",
        "adj_val=adj_val[0:10*N_shot*4+10*N_shot,Random_idx_val]"
      ],
      "execution_count": 265,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iha5MQaYDJpG"
      },
      "source": [
        "def accuracy(output, labels):\n",
        "    preds = output.max(1)[1].type_as(labels)\n",
        "    correct = preds.eq(labels).double()\n",
        "    correct = correct.sum()\n",
        "    return correct / len(labels)    "
      ],
      "execution_count": 266,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "us3XSHLHDWoP"
      },
      "source": [
        "#layers\n",
        "\n",
        "import math\n",
        "\n",
        "import torch\n",
        "\n",
        "from torch.nn.parameter import Parameter\n",
        "\n",
        "#class parameter ,param haro cache mikone\n",
        "\n",
        "from torch.nn.modules.module import Module\n",
        "\n",
        "\n",
        "class GraphConvolution(Module):\n",
        "    \"\"\"\n",
        "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features ,out_features, bias=True):\n",
        "        super(GraphConvolution, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        \n",
        "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
        "        if bias:\n",
        "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
        "        self.weight.data.uniform_(-stdv, stdv)\n",
        "        if self.bias is not None:\n",
        "            self.bias.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self, input, input_adj):\n",
        "        support = torch.mm(input, self.weight)\n",
        "        #Sparse matrix multiplication=spmm\n",
        "        output = torch.spmm(input_adj, support)\n",
        "        if self.bias is not None:\n",
        "            return output + self.bias\n",
        "        else:\n",
        "            return output\n"
      ],
      "execution_count": 267,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXDaJHg7DeHl"
      },
      "source": [
        "#models\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "#from layers import GraphConvolution\n",
        "\n",
        "\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self, nfeat, nhid,nclass, dropout):\n",
        "        super(GCN, self).__init__()\n",
        "\n",
        "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
        "        self.gc2 = GraphConvolution(nhid, nclass)\n",
        "   \n",
        "        self.dropout = dropout\n",
        "        \n",
        "    def forward(self, x, input_adj):\n",
        "        x = F.relu(self.gc1(x, input_adj))\n",
        "        x = F.dropout(x, self.dropout, training=self.training)\n",
        "        x = self.gc2(x, input_adj)\n",
        "        return F.log_softmax(x, dim=1)\n"
      ],
      "execution_count": 268,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kAvN1C2QDq89",
        "outputId": "79dab7d8-addc-45da-fca7-649c81340db3"
      },
      "source": [
        "#train\n",
        "\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from torchsummary import summary\n",
        "\n",
        "import time\n",
        "import argparse\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "# Training settings\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
        "                    help='Disables CUDA training.')\n",
        "parser.add_argument('--fastmode', action='store_true', default=False,\n",
        "                    help='Validate during training pass.')\n",
        "parser.add_argument('--seed', type=int, default=42, help='Random seed.')\n",
        "parser.add_argument('--epochs', type=int, default=1500,\n",
        "                    help='Number of epochs to train.')\n",
        "parser.add_argument('--lr', type=float, default=0.01,\n",
        "                    help='Initial learning rate.')\n",
        "parser.add_argument('--weight_decay', type=float, default=5e-4,\n",
        "                    help='Weight decay (L2 loss on parameters).')\n",
        "parser.add_argument('--hidden', type=int, default=16,\n",
        "                    help='Number of hidden units.')\n",
        "\n",
        "parser.add_argument('--dropout', type=float, default=0.5,\n",
        "                    help='Dropout rate (1 - keep probability).')\n",
        "\n",
        "#args = parser.parse_args()\n",
        "#error midad khate bala,khate paein jaigozin shod\n",
        "\n",
        "args = parser.parse_known_args()[0]\n",
        "\n",
        "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
        "\n",
        "np.random.seed(args.seed)\n",
        "torch.manual_seed(args.seed)\n",
        "if args.cuda:\n",
        "    torch.cuda.manual_seed(args.seed)\n",
        "\n",
        "\n",
        "# Model and optimizer\n",
        "model = GCN(nfeat=features.shape[1],\n",
        "            nhid=args.hidden,\n",
        "            nclass=class_names.size+1,\n",
        "            dropout=args.dropout)\n",
        "optimizer = optim.Adam(model.parameters(),\n",
        "                       lr=args.lr, weight_decay=args.weight_decay)\n",
        "\n",
        "\n",
        "# to set cuda as your device if possible\n",
        "#training on  GPU\n",
        "\n",
        "if args.cuda:\n",
        "    model.cuda()\n",
        "    features = features.cuda()\n",
        "    #adj = adj.cuda()\n",
        "    #labels = labels.cuda()\n",
        "    #idx_train = idx_train.cuda()\n",
        "    #idx_val = idx_val.cuda()\n",
        "    #idx_test = idx_test.cuda()\n",
        "    \n",
        "    # train:adjust the weights on the neural network\n",
        "    # validation:used to minimize overfitting\n",
        " \n",
        "def test():\n",
        "    # Turn on evaluation mode which disables dropout.\n",
        "    model.eval()\n",
        "    output = model(features_test\n",
        "                   ,adj_test)\n",
        "    loss_test = F.nll_loss(output, labels_test)\n",
        "    acc_test = accuracy(output, labels_test)\n",
        "    print(\"Test set results:\",\n",
        "          \"loss= {:.4f}\".format(loss_test.item()),\n",
        "          \"accuracy= {:.4f}\".format(acc_test.item()))\n",
        "\n",
        "def train(epoch):\n",
        "  model.train()\n",
        "  optimizer.zero_grad()\n",
        "  output = model(features_train,adj_train)\n",
        "  loss_train = F.nll_loss(output, labels_train)\n",
        "  acc_train = accuracy(output,labels_train)\n",
        "  # Computing the gradients necessary to adjust the weights\n",
        "  loss_train.backward()\n",
        "  # Updating the weights of the neural network\n",
        "  optimizer.step()\n",
        "  losses.append(loss_train.item())\n",
        "  acc.append(acc_train.item())\n",
        "\n",
        "  if not args.fastmode:\n",
        "    # Evaluate validation set performance separately,\n",
        "    # deactivates dropout during validation run.\n",
        "    model.eval()\n",
        "    output = model(features_val, adj_val)\n",
        "\n",
        "    loss_val = F.nll_loss(output, labels_val)\n",
        "    acc_val = accuracy(output, labels_val)\n",
        "    losses_val.append(loss_val.item())\n",
        "    acc_valid.append(acc_val.item())\n",
        "\n",
        "    print('Epoch: {:04d}'.format(epoch+1),\n",
        "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
        "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
        "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
        "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
        "          'time: {:.4f}s'.format(time.time() - t))\n",
        "\n",
        "# Train model\n",
        "t_total = time.time()\n",
        "losses = []\n",
        "acc=[]\n",
        "losses_val = []\n",
        "acc_valid=[]\n",
        "t = time.time()\n",
        "for epoch in range(args.epochs):\n",
        "    train(epoch)\n",
        "\n",
        "# Testing\n",
        "test()\n",
        "\n",
        "#plotting loss_train_val:\n",
        "\n",
        "plt.plot(np.array(losses),label ='loss_train Plot')\n",
        "plt.plot(np.array(losses_val),label ='loss_val Plot')\n",
        "plt.title('loss_train_validation Plot')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('value')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "#plotting acc_train_val:\n",
        "\n",
        "plt.plot(np.array(acc),label ='Accuracy_train Plot')\n",
        "plt.plot(np.array(acc_valid),label ='Accuracy_val Plot')\n",
        "plt.title('acc_train_validation Plot')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('value')\n",
        "plt.legend()\n",
        "plt.show()    \n",
        "\n",
        "print(\"Optimization Finished!\")\n",
        "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
        "\n",
        "print(model)"
      ],
      "execution_count": 269,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0001 loss_train: 2.3270 acc_train: 0.0400 loss_val: 2.3108 acc_val: 0.0800 time: 0.0024s\n",
            "Epoch: 0002 loss_train: 2.3069 acc_train: 0.0400 loss_val: 2.2866 acc_val: 0.2200 time: 0.0071s\n",
            "Epoch: 0003 loss_train: 2.2660 acc_train: 0.1800 loss_val: 2.2600 acc_val: 0.7800 time: 0.0140s\n",
            "Epoch: 0004 loss_train: 2.2412 acc_train: 0.7600 loss_val: 2.2315 acc_val: 0.7800 time: 0.0157s\n",
            "Epoch: 0005 loss_train: 2.2246 acc_train: 0.7600 loss_val: 2.2013 acc_val: 0.7800 time: 0.0172s\n",
            "Epoch: 0006 loss_train: 2.1941 acc_train: 0.7400 loss_val: 2.1701 acc_val: 0.7800 time: 0.0189s\n",
            "Epoch: 0007 loss_train: 2.1545 acc_train: 0.8000 loss_val: 2.1380 acc_val: 0.7800 time: 0.0507s\n",
            "Epoch: 0008 loss_train: 2.1253 acc_train: 0.8000 loss_val: 2.1052 acc_val: 0.7800 time: 0.0550s\n",
            "Epoch: 0009 loss_train: 2.0802 acc_train: 0.7800 loss_val: 2.0716 acc_val: 0.7800 time: 0.0591s\n",
            "Epoch: 0010 loss_train: 2.0473 acc_train: 0.8000 loss_val: 2.0378 acc_val: 0.7800 time: 0.0630s\n",
            "Epoch: 0011 loss_train: 1.9776 acc_train: 0.8000 loss_val: 2.0043 acc_val: 0.7800 time: 0.0669s\n",
            "Epoch: 0012 loss_train: 1.9720 acc_train: 0.8000 loss_val: 1.9709 acc_val: 0.7800 time: 0.0707s\n",
            "Epoch: 0013 loss_train: 1.9588 acc_train: 0.8000 loss_val: 1.9386 acc_val: 0.7800 time: 0.0748s\n",
            "Epoch: 0014 loss_train: 1.9327 acc_train: 0.8000 loss_val: 1.9071 acc_val: 0.7800 time: 0.0788s\n",
            "Epoch: 0015 loss_train: 1.9144 acc_train: 0.7800 loss_val: 1.8770 acc_val: 0.7800 time: 0.0826s\n",
            "Epoch: 0016 loss_train: 1.8765 acc_train: 0.7800 loss_val: 1.8478 acc_val: 0.7800 time: 0.0865s\n",
            "Epoch: 0017 loss_train: 1.8682 acc_train: 0.7800 loss_val: 1.8196 acc_val: 0.7800 time: 0.0902s\n",
            "Epoch: 0018 loss_train: 1.8282 acc_train: 0.7800 loss_val: 1.7919 acc_val: 0.7800 time: 0.0940s\n",
            "Epoch: 0019 loss_train: 1.7959 acc_train: 0.7800 loss_val: 1.7651 acc_val: 0.7800 time: 0.0991s\n",
            "Epoch: 0020 loss_train: 1.7906 acc_train: 0.7800 loss_val: 1.7390 acc_val: 0.7800 time: 0.1029s\n",
            "Epoch: 0021 loss_train: 1.7511 acc_train: 0.8000 loss_val: 1.7138 acc_val: 0.7800 time: 0.1067s\n",
            "Epoch: 0022 loss_train: 1.7415 acc_train: 0.7800 loss_val: 1.6902 acc_val: 0.7800 time: 0.1105s\n",
            "Epoch: 0023 loss_train: 1.7292 acc_train: 0.8000 loss_val: 1.6681 acc_val: 0.7800 time: 0.1143s\n",
            "Epoch: 0024 loss_train: 1.7015 acc_train: 0.7800 loss_val: 1.6469 acc_val: 0.7800 time: 0.1181s\n",
            "Epoch: 0025 loss_train: 1.6463 acc_train: 0.8000 loss_val: 1.6265 acc_val: 0.7800 time: 0.1218s\n",
            "Epoch: 0026 loss_train: 1.6266 acc_train: 0.8000 loss_val: 1.6071 acc_val: 0.7800 time: 0.1256s\n",
            "Epoch: 0027 loss_train: 1.6239 acc_train: 0.8000 loss_val: 1.5885 acc_val: 0.7800 time: 0.1295s\n",
            "Epoch: 0028 loss_train: 1.5826 acc_train: 0.8000 loss_val: 1.5709 acc_val: 0.7800 time: 0.1332s\n",
            "Epoch: 0029 loss_train: 1.6134 acc_train: 0.7800 loss_val: 1.5543 acc_val: 0.7800 time: 0.1372s\n",
            "Epoch: 0030 loss_train: 1.6002 acc_train: 0.7800 loss_val: 1.5390 acc_val: 0.7800 time: 0.1410s\n",
            "Epoch: 0031 loss_train: 1.5410 acc_train: 0.8000 loss_val: 1.5243 acc_val: 0.7800 time: 0.1448s\n",
            "Epoch: 0032 loss_train: 1.5760 acc_train: 0.7800 loss_val: 1.5113 acc_val: 0.7800 time: 0.1486s\n",
            "Epoch: 0033 loss_train: 1.5402 acc_train: 0.8000 loss_val: 1.4985 acc_val: 0.7800 time: 0.1526s\n",
            "Epoch: 0034 loss_train: 1.5328 acc_train: 0.8000 loss_val: 1.4866 acc_val: 0.7800 time: 0.1563s\n",
            "Epoch: 0035 loss_train: 1.5276 acc_train: 0.7800 loss_val: 1.4755 acc_val: 0.7800 time: 0.1603s\n",
            "Epoch: 0036 loss_train: 1.5202 acc_train: 0.8000 loss_val: 1.4652 acc_val: 0.7800 time: 0.1640s\n",
            "Epoch: 0037 loss_train: 1.4863 acc_train: 0.7800 loss_val: 1.4557 acc_val: 0.7800 time: 0.1676s\n",
            "Epoch: 0038 loss_train: 1.4845 acc_train: 0.8000 loss_val: 1.4467 acc_val: 0.7800 time: 0.1721s\n",
            "Epoch: 0039 loss_train: 1.4810 acc_train: 0.7800 loss_val: 1.4384 acc_val: 0.7800 time: 0.1765s\n",
            "Epoch: 0040 loss_train: 1.4020 acc_train: 0.8000 loss_val: 1.4305 acc_val: 0.7800 time: 0.1822s\n",
            "Epoch: 0041 loss_train: 1.4467 acc_train: 0.8000 loss_val: 1.4232 acc_val: 0.7800 time: 0.1859s\n",
            "Epoch: 0042 loss_train: 1.4766 acc_train: 0.7800 loss_val: 1.4166 acc_val: 0.7800 time: 0.1898s\n",
            "Epoch: 0043 loss_train: 1.4443 acc_train: 0.7800 loss_val: 1.4104 acc_val: 0.7800 time: 0.1936s\n",
            "Epoch: 0044 loss_train: 1.4240 acc_train: 0.7800 loss_val: 1.4044 acc_val: 0.7800 time: 0.2019s\n",
            "Epoch: 0045 loss_train: 1.4108 acc_train: 0.7800 loss_val: 1.3986 acc_val: 0.7800 time: 0.2061s\n",
            "Epoch: 0046 loss_train: 1.4141 acc_train: 0.8000 loss_val: 1.3931 acc_val: 0.7800 time: 0.2090s\n",
            "Epoch: 0047 loss_train: 1.3756 acc_train: 0.7800 loss_val: 1.3877 acc_val: 0.7800 time: 0.2261s\n",
            "Epoch: 0048 loss_train: 1.3833 acc_train: 0.8000 loss_val: 1.3825 acc_val: 0.7800 time: 0.2281s\n",
            "Epoch: 0049 loss_train: 1.3857 acc_train: 0.8000 loss_val: 1.3779 acc_val: 0.7800 time: 0.2297s\n",
            "Epoch: 0050 loss_train: 1.4064 acc_train: 0.7800 loss_val: 1.3739 acc_val: 0.7800 time: 0.2315s\n",
            "Epoch: 0051 loss_train: 1.3256 acc_train: 0.8000 loss_val: 1.3700 acc_val: 0.7800 time: 0.2336s\n",
            "Epoch: 0052 loss_train: 1.3812 acc_train: 0.7800 loss_val: 1.3661 acc_val: 0.7800 time: 0.2372s\n",
            "Epoch: 0053 loss_train: 1.3121 acc_train: 0.8000 loss_val: 1.3623 acc_val: 0.7800 time: 0.2528s\n",
            "Epoch: 0054 loss_train: 1.2985 acc_train: 0.7800 loss_val: 1.3584 acc_val: 0.7800 time: 0.2567s\n",
            "Epoch: 0055 loss_train: 1.3039 acc_train: 0.8000 loss_val: 1.3543 acc_val: 0.7800 time: 0.2607s\n",
            "Epoch: 0056 loss_train: 1.2753 acc_train: 0.8000 loss_val: 1.3505 acc_val: 0.7800 time: 0.2647s\n",
            "Epoch: 0057 loss_train: 1.2657 acc_train: 0.8000 loss_val: 1.3466 acc_val: 0.7800 time: 0.2719s\n",
            "Epoch: 0058 loss_train: 1.2953 acc_train: 0.7800 loss_val: 1.3428 acc_val: 0.7800 time: 0.2759s\n",
            "Epoch: 0059 loss_train: 1.2755 acc_train: 0.8000 loss_val: 1.3392 acc_val: 0.7800 time: 0.2800s\n",
            "Epoch: 0060 loss_train: 1.2551 acc_train: 0.8000 loss_val: 1.3354 acc_val: 0.7800 time: 0.2839s\n",
            "Epoch: 0061 loss_train: 1.2466 acc_train: 0.8000 loss_val: 1.3318 acc_val: 0.7800 time: 0.2878s\n",
            "Epoch: 0062 loss_train: 1.2401 acc_train: 0.8000 loss_val: 1.3280 acc_val: 0.7800 time: 0.2917s\n",
            "Epoch: 0063 loss_train: 1.2205 acc_train: 0.8000 loss_val: 1.3241 acc_val: 0.7800 time: 0.2957s\n",
            "Epoch: 0064 loss_train: 1.2166 acc_train: 0.8000 loss_val: 1.3203 acc_val: 0.7800 time: 0.2996s\n",
            "Epoch: 0065 loss_train: 1.2688 acc_train: 0.8000 loss_val: 1.3169 acc_val: 0.7800 time: 0.3035s\n",
            "Epoch: 0066 loss_train: 1.2446 acc_train: 0.8000 loss_val: 1.3136 acc_val: 0.7800 time: 0.3075s\n",
            "Epoch: 0067 loss_train: 1.2074 acc_train: 0.8000 loss_val: 1.3104 acc_val: 0.7800 time: 0.3123s\n",
            "Epoch: 0068 loss_train: 1.2164 acc_train: 0.8000 loss_val: 1.3073 acc_val: 0.7800 time: 0.3139s\n",
            "Epoch: 0069 loss_train: 1.1851 acc_train: 0.8000 loss_val: 1.3043 acc_val: 0.7800 time: 0.3203s\n",
            "Epoch: 0070 loss_train: 1.2275 acc_train: 0.7800 loss_val: 1.3012 acc_val: 0.7800 time: 0.3242s\n",
            "Epoch: 0071 loss_train: 1.1525 acc_train: 0.8000 loss_val: 1.2983 acc_val: 0.7800 time: 0.3281s\n",
            "Epoch: 0072 loss_train: 1.1687 acc_train: 0.8000 loss_val: 1.2955 acc_val: 0.7800 time: 0.3321s\n",
            "Epoch: 0073 loss_train: 1.1880 acc_train: 0.8000 loss_val: 1.2927 acc_val: 0.7800 time: 0.3359s\n",
            "Epoch: 0074 loss_train: 1.1699 acc_train: 0.8000 loss_val: 1.2900 acc_val: 0.7800 time: 0.3399s\n",
            "Epoch: 0075 loss_train: 1.1710 acc_train: 0.8000 loss_val: 1.2873 acc_val: 0.7800 time: 0.3439s\n",
            "Epoch: 0076 loss_train: 1.0979 acc_train: 0.8200 loss_val: 1.2846 acc_val: 0.7800 time: 0.3477s\n",
            "Epoch: 0077 loss_train: 1.1972 acc_train: 0.8000 loss_val: 1.2819 acc_val: 0.7800 time: 0.3516s\n",
            "Epoch: 0078 loss_train: 1.1377 acc_train: 0.8000 loss_val: 1.2793 acc_val: 0.7800 time: 0.3557s\n",
            "Epoch: 0079 loss_train: 1.1504 acc_train: 0.8000 loss_val: 1.2769 acc_val: 0.7800 time: 0.3597s\n",
            "Epoch: 0080 loss_train: 1.1376 acc_train: 0.8000 loss_val: 1.2745 acc_val: 0.7800 time: 0.3635s\n",
            "Epoch: 0081 loss_train: 1.1234 acc_train: 0.8000 loss_val: 1.2722 acc_val: 0.7800 time: 0.3674s\n",
            "Epoch: 0082 loss_train: 1.0746 acc_train: 0.8200 loss_val: 1.2699 acc_val: 0.7800 time: 0.3713s\n",
            "Epoch: 0083 loss_train: 1.1093 acc_train: 0.8000 loss_val: 1.2675 acc_val: 0.7800 time: 0.3750s\n",
            "Epoch: 0084 loss_train: 1.1036 acc_train: 0.8000 loss_val: 1.2652 acc_val: 0.7800 time: 0.3788s\n",
            "Epoch: 0085 loss_train: 1.0945 acc_train: 0.8000 loss_val: 1.2628 acc_val: 0.7800 time: 0.3825s\n",
            "Epoch: 0086 loss_train: 1.1263 acc_train: 0.7800 loss_val: 1.2604 acc_val: 0.7800 time: 0.3863s\n",
            "Epoch: 0087 loss_train: 1.1023 acc_train: 0.8000 loss_val: 1.2580 acc_val: 0.7800 time: 0.3900s\n",
            "Epoch: 0088 loss_train: 1.0693 acc_train: 0.8200 loss_val: 1.2555 acc_val: 0.7800 time: 0.3938s\n",
            "Epoch: 0089 loss_train: 1.0691 acc_train: 0.8200 loss_val: 1.2531 acc_val: 0.7800 time: 0.3976s\n",
            "Epoch: 0090 loss_train: 1.0688 acc_train: 0.8200 loss_val: 1.2509 acc_val: 0.7800 time: 0.4014s\n",
            "Epoch: 0091 loss_train: 1.0670 acc_train: 0.8200 loss_val: 1.2485 acc_val: 0.7800 time: 0.4051s\n",
            "Epoch: 0092 loss_train: 1.0702 acc_train: 0.8000 loss_val: 1.2462 acc_val: 0.7800 time: 0.4091s\n",
            "Epoch: 0093 loss_train: 1.1011 acc_train: 0.8000 loss_val: 1.2440 acc_val: 0.7800 time: 0.4140s\n",
            "Epoch: 0094 loss_train: 1.0108 acc_train: 0.8200 loss_val: 1.2418 acc_val: 0.7800 time: 0.4219s\n",
            "Epoch: 0095 loss_train: 1.1134 acc_train: 0.7800 loss_val: 1.2398 acc_val: 0.7800 time: 0.4258s\n",
            "Epoch: 0096 loss_train: 1.0524 acc_train: 0.8000 loss_val: 1.2379 acc_val: 0.7800 time: 0.4296s\n",
            "Epoch: 0097 loss_train: 0.9933 acc_train: 0.8200 loss_val: 1.2362 acc_val: 0.7800 time: 0.4334s\n",
            "Epoch: 0098 loss_train: 1.0150 acc_train: 0.8200 loss_val: 1.2346 acc_val: 0.7800 time: 0.4373s\n",
            "Epoch: 0099 loss_train: 1.0281 acc_train: 0.8000 loss_val: 1.2331 acc_val: 0.7800 time: 0.4412s\n",
            "Epoch: 0100 loss_train: 1.0599 acc_train: 0.8000 loss_val: 1.2317 acc_val: 0.7800 time: 0.4456s\n",
            "Epoch: 0101 loss_train: 1.0284 acc_train: 0.8200 loss_val: 1.2300 acc_val: 0.7800 time: 0.4494s\n",
            "Epoch: 0102 loss_train: 1.0353 acc_train: 0.8200 loss_val: 1.2284 acc_val: 0.7800 time: 0.4532s\n",
            "Epoch: 0103 loss_train: 1.0290 acc_train: 0.8000 loss_val: 1.2267 acc_val: 0.7800 time: 0.4572s\n",
            "Epoch: 0104 loss_train: 1.0660 acc_train: 0.8000 loss_val: 1.2251 acc_val: 0.7800 time: 0.4609s\n",
            "Epoch: 0105 loss_train: 0.9807 acc_train: 0.8200 loss_val: 1.2236 acc_val: 0.7800 time: 0.4646s\n",
            "Epoch: 0106 loss_train: 0.9983 acc_train: 0.8000 loss_val: 1.2220 acc_val: 0.7800 time: 0.4684s\n",
            "Epoch: 0107 loss_train: 1.0112 acc_train: 0.8000 loss_val: 1.2205 acc_val: 0.7800 time: 0.4723s\n",
            "Epoch: 0108 loss_train: 1.0012 acc_train: 0.8200 loss_val: 1.2190 acc_val: 0.7800 time: 0.4761s\n",
            "Epoch: 0109 loss_train: 1.0021 acc_train: 0.8200 loss_val: 1.2174 acc_val: 0.7800 time: 0.4800s\n",
            "Epoch: 0110 loss_train: 0.9739 acc_train: 0.8200 loss_val: 1.2158 acc_val: 0.7800 time: 0.4838s\n",
            "Epoch: 0111 loss_train: 0.9585 acc_train: 0.8200 loss_val: 1.2142 acc_val: 0.7800 time: 0.4874s\n",
            "Epoch: 0112 loss_train: 0.9775 acc_train: 0.8200 loss_val: 1.2125 acc_val: 0.7800 time: 0.4913s\n",
            "Epoch: 0113 loss_train: 0.9850 acc_train: 0.8200 loss_val: 1.2109 acc_val: 0.7800 time: 0.4965s\n",
            "Epoch: 0114 loss_train: 0.9777 acc_train: 0.8000 loss_val: 1.2091 acc_val: 0.7800 time: 0.5023s\n",
            "Epoch: 0115 loss_train: 1.0087 acc_train: 0.8000 loss_val: 1.2075 acc_val: 0.7800 time: 0.5063s\n",
            "Epoch: 0116 loss_train: 1.0277 acc_train: 0.8000 loss_val: 1.2058 acc_val: 0.7800 time: 0.5101s\n",
            "Epoch: 0117 loss_train: 0.9559 acc_train: 0.8200 loss_val: 1.2042 acc_val: 0.7800 time: 0.5138s\n",
            "Epoch: 0118 loss_train: 0.9631 acc_train: 0.8200 loss_val: 1.2028 acc_val: 0.7800 time: 0.5176s\n",
            "Epoch: 0119 loss_train: 0.9499 acc_train: 0.8200 loss_val: 1.2015 acc_val: 0.7800 time: 0.5213s\n",
            "Epoch: 0120 loss_train: 0.9437 acc_train: 0.8200 loss_val: 1.2003 acc_val: 0.7800 time: 0.5265s\n",
            "Epoch: 0121 loss_train: 0.9556 acc_train: 0.8200 loss_val: 1.1993 acc_val: 0.7800 time: 0.5306s\n",
            "Epoch: 0122 loss_train: 0.9374 acc_train: 0.8200 loss_val: 1.1983 acc_val: 0.7800 time: 0.5344s\n",
            "Epoch: 0123 loss_train: 0.9676 acc_train: 0.8000 loss_val: 1.1975 acc_val: 0.7800 time: 0.5384s\n",
            "Epoch: 0124 loss_train: 0.9769 acc_train: 0.8000 loss_val: 1.1967 acc_val: 0.7800 time: 0.5422s\n",
            "Epoch: 0125 loss_train: 1.1429 acc_train: 0.7800 loss_val: 1.1958 acc_val: 0.7800 time: 0.5460s\n",
            "Epoch: 0126 loss_train: 0.9800 acc_train: 0.8000 loss_val: 1.1949 acc_val: 0.7800 time: 0.5505s\n",
            "Epoch: 0127 loss_train: 0.9621 acc_train: 0.8000 loss_val: 1.1937 acc_val: 0.7800 time: 0.5552s\n",
            "Epoch: 0128 loss_train: 0.9753 acc_train: 0.8000 loss_val: 1.1927 acc_val: 0.7800 time: 0.5590s\n",
            "Epoch: 0129 loss_train: 0.9639 acc_train: 0.8000 loss_val: 1.1918 acc_val: 0.7800 time: 0.5627s\n",
            "Epoch: 0130 loss_train: 0.9450 acc_train: 0.8200 loss_val: 1.1908 acc_val: 0.7800 time: 0.5664s\n",
            "Epoch: 0131 loss_train: 0.9290 acc_train: 0.8000 loss_val: 1.1900 acc_val: 0.7800 time: 0.5702s\n",
            "Epoch: 0132 loss_train: 0.9181 acc_train: 0.8000 loss_val: 1.1892 acc_val: 0.7800 time: 0.5784s\n",
            "Epoch: 0133 loss_train: 1.0211 acc_train: 0.7800 loss_val: 1.1886 acc_val: 0.7800 time: 0.5824s\n",
            "Epoch: 0134 loss_train: 0.9412 acc_train: 0.8000 loss_val: 1.1881 acc_val: 0.7800 time: 0.5863s\n",
            "Epoch: 0135 loss_train: 0.9625 acc_train: 0.7800 loss_val: 1.1875 acc_val: 0.7800 time: 0.5900s\n",
            "Epoch: 0136 loss_train: 0.9120 acc_train: 0.8200 loss_val: 1.1869 acc_val: 0.7800 time: 0.5939s\n",
            "Epoch: 0137 loss_train: 0.9541 acc_train: 0.8000 loss_val: 1.1865 acc_val: 0.7800 time: 0.5977s\n",
            "Epoch: 0138 loss_train: 0.9111 acc_train: 0.8200 loss_val: 1.1862 acc_val: 0.7800 time: 0.6015s\n",
            "Epoch: 0139 loss_train: 0.9043 acc_train: 0.8200 loss_val: 1.1857 acc_val: 0.7800 time: 0.6053s\n",
            "Epoch: 0140 loss_train: 0.9494 acc_train: 0.8000 loss_val: 1.1853 acc_val: 0.7800 time: 0.6092s\n",
            "Epoch: 0141 loss_train: 0.9018 acc_train: 0.8200 loss_val: 1.1849 acc_val: 0.7800 time: 0.6137s\n",
            "Epoch: 0142 loss_train: 0.9108 acc_train: 0.8200 loss_val: 1.1845 acc_val: 0.7800 time: 0.6175s\n",
            "Epoch: 0143 loss_train: 0.9386 acc_train: 0.8000 loss_val: 1.1842 acc_val: 0.7800 time: 0.6224s\n",
            "Epoch: 0144 loss_train: 0.9143 acc_train: 0.8200 loss_val: 1.1838 acc_val: 0.7800 time: 0.6310s\n",
            "Epoch: 0145 loss_train: 0.9568 acc_train: 0.7800 loss_val: 1.1842 acc_val: 0.7800 time: 0.6349s\n",
            "Epoch: 0146 loss_train: 0.9027 acc_train: 0.8000 loss_val: 1.1846 acc_val: 0.7800 time: 0.6387s\n",
            "Epoch: 0147 loss_train: 0.8948 acc_train: 0.8000 loss_val: 1.1849 acc_val: 0.7800 time: 0.6424s\n",
            "Epoch: 0148 loss_train: 0.8967 acc_train: 0.8200 loss_val: 1.1851 acc_val: 0.7800 time: 0.6462s\n",
            "Epoch: 0149 loss_train: 0.9169 acc_train: 0.8200 loss_val: 1.1847 acc_val: 0.7800 time: 0.6500s\n",
            "Epoch: 0150 loss_train: 0.9205 acc_train: 0.8000 loss_val: 1.1840 acc_val: 0.7800 time: 0.6539s\n",
            "Epoch: 0151 loss_train: 0.9614 acc_train: 0.8000 loss_val: 1.1829 acc_val: 0.7800 time: 0.6579s\n",
            "Epoch: 0152 loss_train: 0.8981 acc_train: 0.8000 loss_val: 1.1815 acc_val: 0.7800 time: 0.6617s\n",
            "Epoch: 0153 loss_train: 0.9168 acc_train: 0.8000 loss_val: 1.1802 acc_val: 0.7800 time: 0.6654s\n",
            "Epoch: 0154 loss_train: 0.8829 acc_train: 0.8200 loss_val: 1.1789 acc_val: 0.7800 time: 0.6691s\n",
            "Epoch: 0155 loss_train: 0.9433 acc_train: 0.8000 loss_val: 1.1775 acc_val: 0.7800 time: 0.6729s\n",
            "Epoch: 0156 loss_train: 0.9257 acc_train: 0.8000 loss_val: 1.1763 acc_val: 0.7800 time: 0.6775s\n",
            "Epoch: 0157 loss_train: 0.8816 acc_train: 0.8000 loss_val: 1.1751 acc_val: 0.7800 time: 0.6814s\n",
            "Epoch: 0158 loss_train: 0.9231 acc_train: 0.8000 loss_val: 1.1740 acc_val: 0.7800 time: 0.6851s\n",
            "Epoch: 0159 loss_train: 0.8907 acc_train: 0.8200 loss_val: 1.1728 acc_val: 0.7800 time: 0.6888s\n",
            "Epoch: 0160 loss_train: 0.8759 acc_train: 0.8200 loss_val: 1.1718 acc_val: 0.7800 time: 0.6927s\n",
            "Epoch: 0161 loss_train: 0.9531 acc_train: 0.8000 loss_val: 1.1709 acc_val: 0.7800 time: 0.6965s\n",
            "Epoch: 0162 loss_train: 0.8957 acc_train: 0.8200 loss_val: 1.1700 acc_val: 0.7800 time: 0.7003s\n",
            "Epoch: 0163 loss_train: 0.8819 acc_train: 0.8200 loss_val: 1.1693 acc_val: 0.7800 time: 0.7041s\n",
            "Epoch: 0164 loss_train: 0.8731 acc_train: 0.8200 loss_val: 1.1688 acc_val: 0.7800 time: 0.7077s\n",
            "Epoch: 0165 loss_train: 0.8958 acc_train: 0.8200 loss_val: 1.1683 acc_val: 0.7800 time: 0.7113s\n",
            "Epoch: 0166 loss_train: 0.8952 acc_train: 0.8000 loss_val: 1.1680 acc_val: 0.7800 time: 0.7152s\n",
            "Epoch: 0167 loss_train: 0.8769 acc_train: 0.8200 loss_val: 1.1678 acc_val: 0.7800 time: 0.7198s\n",
            "Epoch: 0168 loss_train: 0.8693 acc_train: 0.8200 loss_val: 1.1678 acc_val: 0.7800 time: 0.7254s\n",
            "Epoch: 0169 loss_train: 0.8629 acc_train: 0.8200 loss_val: 1.1680 acc_val: 0.7800 time: 0.7300s\n",
            "Epoch: 0170 loss_train: 0.8712 acc_train: 0.8000 loss_val: 1.1681 acc_val: 0.7800 time: 0.7342s\n",
            "Epoch: 0171 loss_train: 0.9218 acc_train: 0.8000 loss_val: 1.1683 acc_val: 0.7800 time: 0.7382s\n",
            "Epoch: 0172 loss_train: 0.8859 acc_train: 0.8000 loss_val: 1.1686 acc_val: 0.7800 time: 0.7419s\n",
            "Epoch: 0173 loss_train: 0.8799 acc_train: 0.8000 loss_val: 1.1688 acc_val: 0.7800 time: 0.7458s\n",
            "Epoch: 0174 loss_train: 0.8911 acc_train: 0.8000 loss_val: 1.1688 acc_val: 0.7800 time: 0.7494s\n",
            "Epoch: 0175 loss_train: 0.8393 acc_train: 0.8200 loss_val: 1.1688 acc_val: 0.7800 time: 0.7532s\n",
            "Epoch: 0176 loss_train: 0.8377 acc_train: 0.8200 loss_val: 1.1686 acc_val: 0.7800 time: 0.7569s\n",
            "Epoch: 0177 loss_train: 0.9930 acc_train: 0.7800 loss_val: 1.1681 acc_val: 0.7800 time: 0.7607s\n",
            "Epoch: 0178 loss_train: 0.8510 acc_train: 0.8200 loss_val: 1.1678 acc_val: 0.7800 time: 0.7644s\n",
            "Epoch: 0179 loss_train: 0.8588 acc_train: 0.8200 loss_val: 1.1672 acc_val: 0.7800 time: 0.7680s\n",
            "Epoch: 0180 loss_train: 0.8716 acc_train: 0.8000 loss_val: 1.1666 acc_val: 0.7800 time: 0.7719s\n",
            "Epoch: 0181 loss_train: 0.9062 acc_train: 0.8000 loss_val: 1.1659 acc_val: 0.7800 time: 0.7760s\n",
            "Epoch: 0182 loss_train: 0.8390 acc_train: 0.8200 loss_val: 1.1653 acc_val: 0.7800 time: 0.7799s\n",
            "Epoch: 0183 loss_train: 0.8919 acc_train: 0.8000 loss_val: 1.1645 acc_val: 0.7800 time: 0.7839s\n",
            "Epoch: 0184 loss_train: 0.8753 acc_train: 0.8000 loss_val: 1.1636 acc_val: 0.7800 time: 0.7881s\n",
            "Epoch: 0185 loss_train: 0.9446 acc_train: 0.7800 loss_val: 1.1628 acc_val: 0.7800 time: 0.7919s\n",
            "Epoch: 0186 loss_train: 0.8537 acc_train: 0.8200 loss_val: 1.1621 acc_val: 0.7800 time: 0.7956s\n",
            "Epoch: 0187 loss_train: 0.8865 acc_train: 0.8000 loss_val: 1.1617 acc_val: 0.7800 time: 0.7993s\n",
            "Epoch: 0188 loss_train: 0.8852 acc_train: 0.8000 loss_val: 1.1615 acc_val: 0.7800 time: 0.8031s\n",
            "Epoch: 0189 loss_train: 0.8421 acc_train: 0.8200 loss_val: 1.1613 acc_val: 0.7800 time: 0.8069s\n",
            "Epoch: 0190 loss_train: 0.8431 acc_train: 0.8200 loss_val: 1.1613 acc_val: 0.7800 time: 0.8106s\n",
            "Epoch: 0191 loss_train: 0.8358 acc_train: 0.8200 loss_val: 1.1612 acc_val: 0.7800 time: 0.8142s\n",
            "Epoch: 0192 loss_train: 0.8626 acc_train: 0.8000 loss_val: 1.1610 acc_val: 0.7800 time: 0.8178s\n",
            "Epoch: 0193 loss_train: 0.8387 acc_train: 0.8200 loss_val: 1.1609 acc_val: 0.7800 time: 0.8219s\n",
            "Epoch: 0194 loss_train: 0.8313 acc_train: 0.8200 loss_val: 1.1611 acc_val: 0.7800 time: 0.8257s\n",
            "Epoch: 0195 loss_train: 0.8142 acc_train: 0.8200 loss_val: 1.1613 acc_val: 0.7800 time: 0.8295s\n",
            "Epoch: 0196 loss_train: 0.8488 acc_train: 0.8200 loss_val: 1.1615 acc_val: 0.7800 time: 0.8324s\n",
            "Epoch: 0197 loss_train: 0.8318 acc_train: 0.8000 loss_val: 1.1617 acc_val: 0.7800 time: 0.8425s\n",
            "Epoch: 0198 loss_train: 0.8318 acc_train: 0.8200 loss_val: 1.1621 acc_val: 0.7800 time: 0.8463s\n",
            "Epoch: 0199 loss_train: 0.9044 acc_train: 0.8000 loss_val: 1.1625 acc_val: 0.7800 time: 0.8502s\n",
            "Epoch: 0200 loss_train: 0.8566 acc_train: 0.8000 loss_val: 1.1628 acc_val: 0.7800 time: 0.8540s\n",
            "Epoch: 0201 loss_train: 0.8469 acc_train: 0.8200 loss_val: 1.1631 acc_val: 0.7800 time: 0.8578s\n",
            "Epoch: 0202 loss_train: 0.8699 acc_train: 0.8000 loss_val: 1.1633 acc_val: 0.7800 time: 0.8618s\n",
            "Epoch: 0203 loss_train: 0.8337 acc_train: 0.8000 loss_val: 1.1637 acc_val: 0.7800 time: 0.8657s\n",
            "Epoch: 0204 loss_train: 0.8593 acc_train: 0.8000 loss_val: 1.1640 acc_val: 0.7800 time: 0.8694s\n",
            "Epoch: 0205 loss_train: 0.8329 acc_train: 0.8200 loss_val: 1.1642 acc_val: 0.7800 time: 0.8731s\n",
            "Epoch: 0206 loss_train: 0.8351 acc_train: 0.8200 loss_val: 1.1640 acc_val: 0.7800 time: 0.8771s\n",
            "Epoch: 0207 loss_train: 0.8262 acc_train: 0.8200 loss_val: 1.1637 acc_val: 0.7800 time: 0.8808s\n",
            "Epoch: 0208 loss_train: 0.8784 acc_train: 0.8000 loss_val: 1.1633 acc_val: 0.7800 time: 0.8846s\n",
            "Epoch: 0209 loss_train: 0.8643 acc_train: 0.8000 loss_val: 1.1628 acc_val: 0.7800 time: 0.8884s\n",
            "Epoch: 0210 loss_train: 0.8098 acc_train: 0.8200 loss_val: 1.1623 acc_val: 0.7800 time: 0.8921s\n",
            "Epoch: 0211 loss_train: 0.8259 acc_train: 0.8200 loss_val: 1.1619 acc_val: 0.7800 time: 0.8957s\n",
            "Epoch: 0212 loss_train: 0.8762 acc_train: 0.8000 loss_val: 1.1618 acc_val: 0.7800 time: 0.8995s\n",
            "Epoch: 0213 loss_train: 0.8141 acc_train: 0.8200 loss_val: 1.1617 acc_val: 0.7800 time: 0.9032s\n",
            "Epoch: 0214 loss_train: 0.8301 acc_train: 0.8200 loss_val: 1.1617 acc_val: 0.7800 time: 0.9082s\n",
            "Epoch: 0215 loss_train: 0.8530 acc_train: 0.8000 loss_val: 1.1617 acc_val: 0.7800 time: 0.9166s\n",
            "Epoch: 0216 loss_train: 0.8315 acc_train: 0.8200 loss_val: 1.1614 acc_val: 0.7800 time: 0.9210s\n",
            "Epoch: 0217 loss_train: 0.8311 acc_train: 0.8000 loss_val: 1.1612 acc_val: 0.7800 time: 0.9227s\n",
            "Epoch: 0218 loss_train: 0.8890 acc_train: 0.8000 loss_val: 1.1609 acc_val: 0.7800 time: 0.9244s\n",
            "Epoch: 0219 loss_train: 0.9000 acc_train: 0.8000 loss_val: 1.1607 acc_val: 0.7800 time: 0.9334s\n",
            "Epoch: 0220 loss_train: 0.8244 acc_train: 0.8200 loss_val: 1.1607 acc_val: 0.7800 time: 0.9381s\n",
            "Epoch: 0221 loss_train: 0.8674 acc_train: 0.8000 loss_val: 1.1602 acc_val: 0.7800 time: 0.9422s\n",
            "Epoch: 0222 loss_train: 0.8894 acc_train: 0.8000 loss_val: 1.1598 acc_val: 0.7800 time: 0.9463s\n",
            "Epoch: 0223 loss_train: 0.8793 acc_train: 0.7800 loss_val: 1.1596 acc_val: 0.7800 time: 0.9502s\n",
            "Epoch: 0224 loss_train: 0.7933 acc_train: 0.8200 loss_val: 1.1602 acc_val: 0.7800 time: 0.9542s\n",
            "Epoch: 0225 loss_train: 0.8811 acc_train: 0.8000 loss_val: 1.1609 acc_val: 0.7800 time: 0.9580s\n",
            "Epoch: 0226 loss_train: 0.8436 acc_train: 0.8200 loss_val: 1.1616 acc_val: 0.7800 time: 0.9617s\n",
            "Epoch: 0227 loss_train: 0.8047 acc_train: 0.8200 loss_val: 1.1624 acc_val: 0.7800 time: 0.9654s\n",
            "Epoch: 0228 loss_train: 0.8297 acc_train: 0.8200 loss_val: 1.1630 acc_val: 0.7800 time: 0.9691s\n",
            "Epoch: 0229 loss_train: 0.8067 acc_train: 0.8200 loss_val: 1.1636 acc_val: 0.7800 time: 0.9729s\n",
            "Epoch: 0230 loss_train: 0.8155 acc_train: 0.8200 loss_val: 1.1636 acc_val: 0.7800 time: 0.9766s\n",
            "Epoch: 0231 loss_train: 0.8364 acc_train: 0.8000 loss_val: 1.1634 acc_val: 0.7800 time: 0.9804s\n",
            "Epoch: 0232 loss_train: 0.8112 acc_train: 0.8000 loss_val: 1.1649 acc_val: 0.7800 time: 0.9841s\n",
            "Epoch: 0233 loss_train: 0.8279 acc_train: 0.8200 loss_val: 1.1669 acc_val: 0.7800 time: 0.9878s\n",
            "Epoch: 0234 loss_train: 0.8321 acc_train: 0.8000 loss_val: 1.1691 acc_val: 0.7800 time: 0.9916s\n",
            "Epoch: 0235 loss_train: 0.8346 acc_train: 0.8200 loss_val: 1.1706 acc_val: 0.7800 time: 0.9954s\n",
            "Epoch: 0236 loss_train: 0.8152 acc_train: 0.8200 loss_val: 1.1715 acc_val: 0.7800 time: 0.9991s\n",
            "Epoch: 0237 loss_train: 0.8376 acc_train: 0.8000 loss_val: 1.1718 acc_val: 0.7800 time: 1.0029s\n",
            "Epoch: 0238 loss_train: 0.8305 acc_train: 0.8000 loss_val: 1.1718 acc_val: 0.7800 time: 1.0067s\n",
            "Epoch: 0239 loss_train: 0.8110 acc_train: 0.8200 loss_val: 1.1712 acc_val: 0.7800 time: 1.0105s\n",
            "Epoch: 0240 loss_train: 0.8144 acc_train: 0.8200 loss_val: 1.1704 acc_val: 0.7800 time: 1.0142s\n",
            "Epoch: 0241 loss_train: 0.8155 acc_train: 0.8200 loss_val: 1.1694 acc_val: 0.7800 time: 1.0179s\n",
            "Epoch: 0242 loss_train: 0.8773 acc_train: 0.8000 loss_val: 1.1684 acc_val: 0.7800 time: 1.0217s\n",
            "Epoch: 0243 loss_train: 0.8218 acc_train: 0.8200 loss_val: 1.1675 acc_val: 0.7800 time: 1.0255s\n",
            "Epoch: 0244 loss_train: 0.8689 acc_train: 0.8000 loss_val: 1.1667 acc_val: 0.7800 time: 1.0292s\n",
            "Epoch: 0245 loss_train: 0.8178 acc_train: 0.8200 loss_val: 1.1661 acc_val: 0.7800 time: 1.0330s\n",
            "Epoch: 0246 loss_train: 0.8142 acc_train: 0.8000 loss_val: 1.1658 acc_val: 0.7800 time: 1.0367s\n",
            "Epoch: 0247 loss_train: 0.8620 acc_train: 0.8000 loss_val: 1.1655 acc_val: 0.7800 time: 1.0453s\n",
            "Epoch: 0248 loss_train: 0.8037 acc_train: 0.8200 loss_val: 1.1655 acc_val: 0.7800 time: 1.0497s\n",
            "Epoch: 0249 loss_train: 0.8786 acc_train: 0.8000 loss_val: 1.1653 acc_val: 0.7800 time: 1.0537s\n",
            "Epoch: 0250 loss_train: 0.8284 acc_train: 0.8200 loss_val: 1.1649 acc_val: 0.7800 time: 1.0579s\n",
            "Epoch: 0251 loss_train: 0.8107 acc_train: 0.8200 loss_val: 1.1642 acc_val: 0.7800 time: 1.0619s\n",
            "Epoch: 0252 loss_train: 0.8222 acc_train: 0.8000 loss_val: 1.1637 acc_val: 0.7800 time: 1.0659s\n",
            "Epoch: 0253 loss_train: 0.8012 acc_train: 0.8200 loss_val: 1.1635 acc_val: 0.7800 time: 1.0700s\n",
            "Epoch: 0254 loss_train: 0.7963 acc_train: 0.8200 loss_val: 1.1632 acc_val: 0.7800 time: 1.0741s\n",
            "Epoch: 0255 loss_train: 0.8968 acc_train: 0.8000 loss_val: 1.1627 acc_val: 0.7800 time: 1.0781s\n",
            "Epoch: 0256 loss_train: 0.8474 acc_train: 0.8000 loss_val: 1.1623 acc_val: 0.7800 time: 1.0818s\n",
            "Epoch: 0257 loss_train: 0.8475 acc_train: 0.8000 loss_val: 1.1617 acc_val: 0.7800 time: 1.0858s\n",
            "Epoch: 0258 loss_train: 0.8446 acc_train: 0.8000 loss_val: 1.1613 acc_val: 0.7800 time: 1.0898s\n",
            "Epoch: 0259 loss_train: 0.8668 acc_train: 0.8000 loss_val: 1.1613 acc_val: 0.7800 time: 1.0938s\n",
            "Epoch: 0260 loss_train: 0.8200 acc_train: 0.8200 loss_val: 1.1611 acc_val: 0.7800 time: 1.0978s\n",
            "Epoch: 0261 loss_train: 0.8116 acc_train: 0.8200 loss_val: 1.1610 acc_val: 0.7800 time: 1.1018s\n",
            "Epoch: 0262 loss_train: 0.8548 acc_train: 0.8000 loss_val: 1.1606 acc_val: 0.7800 time: 1.1057s\n",
            "Epoch: 0263 loss_train: 0.8263 acc_train: 0.8000 loss_val: 1.1603 acc_val: 0.7800 time: 1.1097s\n",
            "Epoch: 0264 loss_train: 0.8093 acc_train: 0.8200 loss_val: 1.1600 acc_val: 0.7800 time: 1.1136s\n",
            "Epoch: 0265 loss_train: 0.8251 acc_train: 0.8200 loss_val: 1.1597 acc_val: 0.7800 time: 1.1175s\n",
            "Epoch: 0266 loss_train: 0.8251 acc_train: 0.8200 loss_val: 1.1592 acc_val: 0.7800 time: 1.1278s\n",
            "Epoch: 0267 loss_train: 0.7915 acc_train: 0.8200 loss_val: 1.1590 acc_val: 0.7800 time: 1.1318s\n",
            "Epoch: 0268 loss_train: 0.8183 acc_train: 0.8000 loss_val: 1.1587 acc_val: 0.7800 time: 1.1358s\n",
            "Epoch: 0269 loss_train: 0.7798 acc_train: 0.8200 loss_val: 1.1585 acc_val: 0.7800 time: 1.1398s\n",
            "Epoch: 0270 loss_train: 0.7534 acc_train: 0.8200 loss_val: 1.1584 acc_val: 0.7800 time: 1.1439s\n",
            "Epoch: 0271 loss_train: 0.8007 acc_train: 0.8000 loss_val: 1.1584 acc_val: 0.7800 time: 1.1478s\n",
            "Epoch: 0272 loss_train: 0.7931 acc_train: 0.8200 loss_val: 1.1584 acc_val: 0.7800 time: 1.1518s\n",
            "Epoch: 0273 loss_train: 0.7722 acc_train: 0.8200 loss_val: 1.1588 acc_val: 0.7800 time: 1.1557s\n",
            "Epoch: 0274 loss_train: 0.8175 acc_train: 0.8200 loss_val: 1.1592 acc_val: 0.7800 time: 1.1596s\n",
            "Epoch: 0275 loss_train: 0.7739 acc_train: 0.8200 loss_val: 1.1598 acc_val: 0.7800 time: 1.1634s\n",
            "Epoch: 0276 loss_train: 0.7810 acc_train: 0.8200 loss_val: 1.1604 acc_val: 0.7800 time: 1.1673s\n",
            "Epoch: 0277 loss_train: 0.8235 acc_train: 0.8200 loss_val: 1.1609 acc_val: 0.7800 time: 1.1710s\n",
            "Epoch: 0278 loss_train: 0.8244 acc_train: 0.8000 loss_val: 1.1615 acc_val: 0.7800 time: 1.1747s\n",
            "Epoch: 0279 loss_train: 0.7852 acc_train: 0.8200 loss_val: 1.1624 acc_val: 0.7800 time: 1.1808s\n",
            "Epoch: 0280 loss_train: 0.7916 acc_train: 0.8200 loss_val: 1.1632 acc_val: 0.7800 time: 1.1861s\n",
            "Epoch: 0281 loss_train: 0.8684 acc_train: 0.8000 loss_val: 1.1635 acc_val: 0.7800 time: 1.1900s\n",
            "Epoch: 0282 loss_train: 0.8113 acc_train: 0.8000 loss_val: 1.1636 acc_val: 0.7800 time: 1.1938s\n",
            "Epoch: 0283 loss_train: 0.8072 acc_train: 0.8000 loss_val: 1.1634 acc_val: 0.7800 time: 1.1974s\n",
            "Epoch: 0284 loss_train: 0.8214 acc_train: 0.8200 loss_val: 1.1635 acc_val: 0.7800 time: 1.2012s\n",
            "Epoch: 0285 loss_train: 0.7744 acc_train: 0.8200 loss_val: 1.1637 acc_val: 0.7800 time: 1.2052s\n",
            "Epoch: 0286 loss_train: 0.7987 acc_train: 0.8200 loss_val: 1.1639 acc_val: 0.7800 time: 1.2089s\n",
            "Epoch: 0287 loss_train: 0.7809 acc_train: 0.8200 loss_val: 1.1641 acc_val: 0.7800 time: 1.2128s\n",
            "Epoch: 0288 loss_train: 0.7760 acc_train: 0.8200 loss_val: 1.1646 acc_val: 0.7800 time: 1.2167s\n",
            "Epoch: 0289 loss_train: 0.8086 acc_train: 0.8200 loss_val: 1.1650 acc_val: 0.7800 time: 1.2205s\n",
            "Epoch: 0290 loss_train: 0.8183 acc_train: 0.8000 loss_val: 1.1654 acc_val: 0.7800 time: 1.2242s\n",
            "Epoch: 0291 loss_train: 0.9814 acc_train: 0.7800 loss_val: 1.1664 acc_val: 0.7800 time: 1.2278s\n",
            "Epoch: 0292 loss_train: 0.8079 acc_train: 0.8200 loss_val: 1.1671 acc_val: 0.7800 time: 1.2316s\n",
            "Epoch: 0293 loss_train: 0.8131 acc_train: 0.8000 loss_val: 1.1676 acc_val: 0.7800 time: 1.2352s\n",
            "Epoch: 0294 loss_train: 0.8035 acc_train: 0.8200 loss_val: 1.1682 acc_val: 0.7800 time: 1.2390s\n",
            "Epoch: 0295 loss_train: 0.7863 acc_train: 0.8000 loss_val: 1.1686 acc_val: 0.7800 time: 1.2427s\n",
            "Epoch: 0296 loss_train: 0.8177 acc_train: 0.8200 loss_val: 1.1693 acc_val: 0.7800 time: 1.2516s\n",
            "Epoch: 0297 loss_train: 0.7982 acc_train: 0.8200 loss_val: 1.1696 acc_val: 0.7800 time: 1.2561s\n",
            "Epoch: 0298 loss_train: 0.8447 acc_train: 0.8000 loss_val: 1.1697 acc_val: 0.7800 time: 1.2599s\n",
            "Epoch: 0299 loss_train: 0.8056 acc_train: 0.8200 loss_val: 1.1694 acc_val: 0.7800 time: 1.2652s\n",
            "Epoch: 0300 loss_train: 0.8073 acc_train: 0.8200 loss_val: 1.1690 acc_val: 0.7800 time: 1.2697s\n",
            "Epoch: 0301 loss_train: 0.8814 acc_train: 0.8000 loss_val: 1.1683 acc_val: 0.7800 time: 1.2742s\n",
            "Epoch: 0302 loss_train: 0.7941 acc_train: 0.8200 loss_val: 1.1678 acc_val: 0.7800 time: 1.2784s\n",
            "Epoch: 0303 loss_train: 0.8131 acc_train: 0.8200 loss_val: 1.1673 acc_val: 0.7800 time: 1.2825s\n",
            "Epoch: 0304 loss_train: 0.8035 acc_train: 0.8200 loss_val: 1.1666 acc_val: 0.7800 time: 1.2865s\n",
            "Epoch: 0305 loss_train: 0.7985 acc_train: 0.8200 loss_val: 1.1660 acc_val: 0.7800 time: 1.2904s\n",
            "Epoch: 0306 loss_train: 0.8485 acc_train: 0.8000 loss_val: 1.1655 acc_val: 0.7800 time: 1.2942s\n",
            "Epoch: 0307 loss_train: 0.8029 acc_train: 0.8200 loss_val: 1.1649 acc_val: 0.7800 time: 1.2979s\n",
            "Epoch: 0308 loss_train: 0.7931 acc_train: 0.8200 loss_val: 1.1643 acc_val: 0.7800 time: 1.3017s\n",
            "Epoch: 0309 loss_train: 0.7805 acc_train: 0.8200 loss_val: 1.1635 acc_val: 0.7800 time: 1.3055s\n",
            "Epoch: 0310 loss_train: 0.7819 acc_train: 0.8200 loss_val: 1.1626 acc_val: 0.7800 time: 1.3092s\n",
            "Epoch: 0311 loss_train: 0.8056 acc_train: 0.8000 loss_val: 1.1617 acc_val: 0.7800 time: 1.3129s\n",
            "Epoch: 0312 loss_train: 0.7784 acc_train: 0.8200 loss_val: 1.1608 acc_val: 0.7800 time: 1.3165s\n",
            "Epoch: 0313 loss_train: 0.7881 acc_train: 0.8200 loss_val: 1.1603 acc_val: 0.7800 time: 1.3201s\n",
            "Epoch: 0314 loss_train: 0.7837 acc_train: 0.8200 loss_val: 1.1598 acc_val: 0.7800 time: 1.3246s\n",
            "Epoch: 0315 loss_train: 0.7546 acc_train: 0.8200 loss_val: 1.1597 acc_val: 0.7800 time: 1.3295s\n",
            "Epoch: 0316 loss_train: 0.7871 acc_train: 0.8200 loss_val: 1.1597 acc_val: 0.7800 time: 1.3336s\n",
            "Epoch: 0317 loss_train: 0.8148 acc_train: 0.8200 loss_val: 1.1601 acc_val: 0.7800 time: 1.3373s\n",
            "Epoch: 0318 loss_train: 0.7885 acc_train: 0.8200 loss_val: 1.1604 acc_val: 0.7800 time: 1.3410s\n",
            "Epoch: 0319 loss_train: 0.8024 acc_train: 0.8200 loss_val: 1.1608 acc_val: 0.7800 time: 1.3447s\n",
            "Epoch: 0320 loss_train: 0.7721 acc_train: 0.8200 loss_val: 1.1613 acc_val: 0.7800 time: 1.3483s\n",
            "Epoch: 0321 loss_train: 0.7874 acc_train: 0.8200 loss_val: 1.1617 acc_val: 0.7800 time: 1.3520s\n",
            "Epoch: 0322 loss_train: 0.7853 acc_train: 0.8200 loss_val: 1.1621 acc_val: 0.7800 time: 1.3559s\n",
            "Epoch: 0323 loss_train: 0.7951 acc_train: 0.8200 loss_val: 1.1626 acc_val: 0.7800 time: 1.3595s\n",
            "Epoch: 0324 loss_train: 0.7719 acc_train: 0.8200 loss_val: 1.1625 acc_val: 0.7800 time: 1.3631s\n",
            "Epoch: 0325 loss_train: 0.7535 acc_train: 0.8200 loss_val: 1.1627 acc_val: 0.7800 time: 1.3668s\n",
            "Epoch: 0326 loss_train: 0.7750 acc_train: 0.8200 loss_val: 1.1632 acc_val: 0.7800 time: 1.3705s\n",
            "Epoch: 0327 loss_train: 0.8111 acc_train: 0.8200 loss_val: 1.1634 acc_val: 0.7800 time: 1.3743s\n",
            "Epoch: 0328 loss_train: 0.7533 acc_train: 0.8200 loss_val: 1.1636 acc_val: 0.7800 time: 1.3780s\n",
            "Epoch: 0329 loss_train: 0.7823 acc_train: 0.8200 loss_val: 1.1638 acc_val: 0.7800 time: 1.3825s\n",
            "Epoch: 0330 loss_train: 0.7737 acc_train: 0.8200 loss_val: 1.1637 acc_val: 0.7800 time: 1.3864s\n",
            "Epoch: 0331 loss_train: 0.7795 acc_train: 0.8200 loss_val: 1.1636 acc_val: 0.7800 time: 1.3902s\n",
            "Epoch: 0332 loss_train: 0.7541 acc_train: 0.8200 loss_val: 1.1639 acc_val: 0.7800 time: 1.3942s\n",
            "Epoch: 0333 loss_train: 0.7943 acc_train: 0.8000 loss_val: 1.1642 acc_val: 0.7800 time: 1.3979s\n",
            "Epoch: 0334 loss_train: 0.7529 acc_train: 0.8200 loss_val: 1.1654 acc_val: 0.7800 time: 1.4016s\n",
            "Epoch: 0335 loss_train: 0.7816 acc_train: 0.8200 loss_val: 1.1650 acc_val: 0.7800 time: 1.4053s\n",
            "Epoch: 0336 loss_train: 0.7625 acc_train: 0.8200 loss_val: 1.1646 acc_val: 0.7800 time: 1.4092s\n",
            "Epoch: 0337 loss_train: 0.7802 acc_train: 0.8200 loss_val: 1.1647 acc_val: 0.7800 time: 1.4130s\n",
            "Epoch: 0338 loss_train: 0.7813 acc_train: 0.8200 loss_val: 1.1650 acc_val: 0.7800 time: 1.4167s\n",
            "Epoch: 0339 loss_train: 0.7655 acc_train: 0.8200 loss_val: 1.1655 acc_val: 0.7800 time: 1.4205s\n",
            "Epoch: 0340 loss_train: 0.7588 acc_train: 0.8200 loss_val: 1.1664 acc_val: 0.7800 time: 1.4243s\n",
            "Epoch: 0341 loss_train: 0.7814 acc_train: 0.8000 loss_val: 1.1668 acc_val: 0.7800 time: 1.4280s\n",
            "Epoch: 0342 loss_train: 0.7697 acc_train: 0.8200 loss_val: 1.1673 acc_val: 0.7800 time: 1.4319s\n",
            "Epoch: 0343 loss_train: 0.7695 acc_train: 0.8000 loss_val: 1.1676 acc_val: 0.7800 time: 1.4380s\n",
            "Epoch: 0344 loss_train: 0.7828 acc_train: 0.8200 loss_val: 1.1676 acc_val: 0.7800 time: 1.4398s\n",
            "Epoch: 0345 loss_train: 0.7943 acc_train: 0.8000 loss_val: 1.1677 acc_val: 0.7800 time: 1.4415s\n",
            "Epoch: 0346 loss_train: 0.7516 acc_train: 0.8200 loss_val: 1.1681 acc_val: 0.7800 time: 1.4498s\n",
            "Epoch: 0347 loss_train: 0.8591 acc_train: 0.7800 loss_val: 1.1684 acc_val: 0.7800 time: 1.4516s\n",
            "Epoch: 0348 loss_train: 0.7602 acc_train: 0.8200 loss_val: 1.1687 acc_val: 0.7800 time: 1.4534s\n",
            "Epoch: 0349 loss_train: 0.7759 acc_train: 0.8200 loss_val: 1.1688 acc_val: 0.7800 time: 1.4552s\n",
            "Epoch: 0350 loss_train: 0.7142 acc_train: 0.8200 loss_val: 1.1688 acc_val: 0.7800 time: 1.4643s\n",
            "Epoch: 0351 loss_train: 0.8048 acc_train: 0.8000 loss_val: 1.1687 acc_val: 0.7800 time: 1.4662s\n",
            "Epoch: 0352 loss_train: 0.8565 acc_train: 0.8000 loss_val: 1.1686 acc_val: 0.7800 time: 1.4680s\n",
            "Epoch: 0353 loss_train: 0.7525 acc_train: 0.8200 loss_val: 1.1687 acc_val: 0.7800 time: 1.4767s\n",
            "Epoch: 0354 loss_train: 0.7762 acc_train: 0.8200 loss_val: 1.1688 acc_val: 0.7800 time: 1.4786s\n",
            "Epoch: 0355 loss_train: 0.7715 acc_train: 0.8200 loss_val: 1.1692 acc_val: 0.7800 time: 1.4828s\n",
            "Epoch: 0356 loss_train: 0.7675 acc_train: 0.8200 loss_val: 1.1699 acc_val: 0.7800 time: 1.4846s\n",
            "Epoch: 0357 loss_train: 0.7478 acc_train: 0.8200 loss_val: 1.1709 acc_val: 0.7800 time: 1.4864s\n",
            "Epoch: 0358 loss_train: 0.7610 acc_train: 0.8200 loss_val: 1.1722 acc_val: 0.7800 time: 1.4881s\n",
            "Epoch: 0359 loss_train: 0.7448 acc_train: 0.8200 loss_val: 1.1740 acc_val: 0.7800 time: 1.5020s\n",
            "Epoch: 0360 loss_train: 0.7801 acc_train: 0.8200 loss_val: 1.1764 acc_val: 0.7800 time: 1.5035s\n",
            "Epoch: 0361 loss_train: 0.7827 acc_train: 0.8200 loss_val: 1.1794 acc_val: 0.7800 time: 1.5181s\n",
            "Epoch: 0362 loss_train: 0.7447 acc_train: 0.8200 loss_val: 1.1811 acc_val: 0.7800 time: 1.5226s\n",
            "Epoch: 0363 loss_train: 0.7900 acc_train: 0.8200 loss_val: 1.1786 acc_val: 0.7800 time: 1.5278s\n",
            "Epoch: 0364 loss_train: 0.7895 acc_train: 0.8000 loss_val: 1.1769 acc_val: 0.7800 time: 1.5319s\n",
            "Epoch: 0365 loss_train: 0.7198 acc_train: 0.8200 loss_val: 1.1757 acc_val: 0.7800 time: 1.5377s\n",
            "Epoch: 0366 loss_train: 0.8177 acc_train: 0.8000 loss_val: 1.1755 acc_val: 0.7800 time: 1.5396s\n",
            "Epoch: 0367 loss_train: 0.7974 acc_train: 0.8000 loss_val: 1.1755 acc_val: 0.7800 time: 1.5413s\n",
            "Epoch: 0368 loss_train: 0.7633 acc_train: 0.8200 loss_val: 1.1754 acc_val: 0.7800 time: 1.5497s\n",
            "Epoch: 0369 loss_train: 0.7416 acc_train: 0.8200 loss_val: 1.1757 acc_val: 0.7800 time: 1.5539s\n",
            "Epoch: 0370 loss_train: 0.7357 acc_train: 0.8200 loss_val: 1.1757 acc_val: 0.7800 time: 1.5580s\n",
            "Epoch: 0371 loss_train: 0.8056 acc_train: 0.8000 loss_val: 1.1762 acc_val: 0.7800 time: 1.5599s\n",
            "Epoch: 0372 loss_train: 0.8283 acc_train: 0.8000 loss_val: 1.1767 acc_val: 0.7800 time: 1.5617s\n",
            "Epoch: 0373 loss_train: 0.8411 acc_train: 0.7800 loss_val: 1.1766 acc_val: 0.7800 time: 1.5634s\n",
            "Epoch: 0374 loss_train: 0.7803 acc_train: 0.8000 loss_val: 1.1783 acc_val: 0.7800 time: 1.5744s\n",
            "Epoch: 0375 loss_train: 0.8381 acc_train: 0.7800 loss_val: 1.1790 acc_val: 0.7800 time: 1.5763s\n",
            "Epoch: 0376 loss_train: 0.8057 acc_train: 0.8200 loss_val: 1.1803 acc_val: 0.7800 time: 1.5781s\n",
            "Epoch: 0377 loss_train: 0.7467 acc_train: 0.8200 loss_val: 1.1821 acc_val: 0.7800 time: 1.5797s\n",
            "Epoch: 0378 loss_train: 0.7556 acc_train: 0.8200 loss_val: 1.1818 acc_val: 0.7800 time: 1.5898s\n",
            "Epoch: 0379 loss_train: 0.7712 acc_train: 0.8200 loss_val: 1.1804 acc_val: 0.7800 time: 1.5931s\n",
            "Epoch: 0380 loss_train: 0.7331 acc_train: 0.8200 loss_val: 1.1790 acc_val: 0.7800 time: 1.5948s\n",
            "Epoch: 0381 loss_train: 0.7713 acc_train: 0.8000 loss_val: 1.1775 acc_val: 0.7800 time: 1.6045s\n",
            "Epoch: 0382 loss_train: 0.7821 acc_train: 0.8000 loss_val: 1.1750 acc_val: 0.7800 time: 1.6086s\n",
            "Epoch: 0383 loss_train: 0.7352 acc_train: 0.8200 loss_val: 1.1736 acc_val: 0.7800 time: 1.6127s\n",
            "Epoch: 0384 loss_train: 0.7394 acc_train: 0.8200 loss_val: 1.1731 acc_val: 0.7800 time: 1.6166s\n",
            "Epoch: 0385 loss_train: 0.8580 acc_train: 0.7800 loss_val: 1.1728 acc_val: 0.7800 time: 1.6206s\n",
            "Epoch: 0386 loss_train: 0.7725 acc_train: 0.8000 loss_val: 1.1728 acc_val: 0.7800 time: 1.6244s\n",
            "Epoch: 0387 loss_train: 0.7912 acc_train: 0.8000 loss_val: 1.1732 acc_val: 0.7800 time: 1.6281s\n",
            "Epoch: 0388 loss_train: 0.7510 acc_train: 0.8200 loss_val: 1.1738 acc_val: 0.7800 time: 1.6319s\n",
            "Epoch: 0389 loss_train: 0.7596 acc_train: 0.8200 loss_val: 1.1746 acc_val: 0.7800 time: 1.6357s\n",
            "Epoch: 0390 loss_train: 0.7832 acc_train: 0.8200 loss_val: 1.1756 acc_val: 0.7800 time: 1.6395s\n",
            "Epoch: 0391 loss_train: 0.7727 acc_train: 0.8200 loss_val: 1.1765 acc_val: 0.7800 time: 1.6433s\n",
            "Epoch: 0392 loss_train: 0.7424 acc_train: 0.8200 loss_val: 1.1777 acc_val: 0.7800 time: 1.6470s\n",
            "Epoch: 0393 loss_train: 0.7391 acc_train: 0.8200 loss_val: 1.1801 acc_val: 0.7800 time: 1.6509s\n",
            "Epoch: 0394 loss_train: 0.7205 acc_train: 0.8200 loss_val: 1.1827 acc_val: 0.7800 time: 1.6548s\n",
            "Epoch: 0395 loss_train: 0.7662 acc_train: 0.8200 loss_val: 1.1844 acc_val: 0.7800 time: 1.6585s\n",
            "Epoch: 0396 loss_train: 0.7582 acc_train: 0.8200 loss_val: 1.1855 acc_val: 0.7800 time: 1.6624s\n",
            "Epoch: 0397 loss_train: 0.7273 acc_train: 0.8200 loss_val: 1.1859 acc_val: 0.7800 time: 1.6661s\n",
            "Epoch: 0398 loss_train: 0.7272 acc_train: 0.8200 loss_val: 1.1847 acc_val: 0.7800 time: 1.6699s\n",
            "Epoch: 0399 loss_train: 0.8539 acc_train: 0.7800 loss_val: 1.1833 acc_val: 0.7800 time: 1.6770s\n",
            "Epoch: 0400 loss_train: 0.7806 acc_train: 0.8000 loss_val: 1.1782 acc_val: 0.7800 time: 1.6788s\n",
            "Epoch: 0401 loss_train: 0.7538 acc_train: 0.8200 loss_val: 1.1751 acc_val: 0.7800 time: 1.6883s\n",
            "Epoch: 0402 loss_train: 0.8497 acc_train: 0.7800 loss_val: 1.1734 acc_val: 0.7800 time: 1.6922s\n",
            "Epoch: 0403 loss_train: 0.7318 acc_train: 0.8200 loss_val: 1.1721 acc_val: 0.7800 time: 1.6960s\n",
            "Epoch: 0404 loss_train: 0.8050 acc_train: 0.8200 loss_val: 1.1710 acc_val: 0.7800 time: 1.6996s\n",
            "Epoch: 0405 loss_train: 0.7515 acc_train: 0.8200 loss_val: 1.1703 acc_val: 0.7800 time: 1.7034s\n",
            "Epoch: 0406 loss_train: 0.7669 acc_train: 0.8200 loss_val: 1.1697 acc_val: 0.7800 time: 1.7073s\n",
            "Epoch: 0407 loss_train: 0.7854 acc_train: 0.8200 loss_val: 1.1689 acc_val: 0.7800 time: 1.7112s\n",
            "Epoch: 0408 loss_train: 0.7512 acc_train: 0.8200 loss_val: 1.1681 acc_val: 0.7800 time: 1.7151s\n",
            "Epoch: 0409 loss_train: 0.7345 acc_train: 0.8200 loss_val: 1.1673 acc_val: 0.7800 time: 1.7189s\n",
            "Epoch: 0410 loss_train: 0.7891 acc_train: 0.8200 loss_val: 1.1668 acc_val: 0.7800 time: 1.7226s\n",
            "Epoch: 0411 loss_train: 0.7293 acc_train: 0.8400 loss_val: 1.1666 acc_val: 0.7800 time: 1.7264s\n",
            "Epoch: 0412 loss_train: 0.7289 acc_train: 0.8200 loss_val: 1.1666 acc_val: 0.7800 time: 1.7301s\n",
            "Epoch: 0413 loss_train: 0.7352 acc_train: 0.8400 loss_val: 1.1667 acc_val: 0.7800 time: 1.7339s\n",
            "Epoch: 0414 loss_train: 0.7575 acc_train: 0.8200 loss_val: 1.1672 acc_val: 0.7800 time: 1.7379s\n",
            "Epoch: 0415 loss_train: 0.7640 acc_train: 0.8200 loss_val: 1.1680 acc_val: 0.7800 time: 1.7417s\n",
            "Epoch: 0416 loss_train: 0.7162 acc_train: 0.8200 loss_val: 1.1687 acc_val: 0.7800 time: 1.7458s\n",
            "Epoch: 0417 loss_train: 0.7204 acc_train: 0.8400 loss_val: 1.1695 acc_val: 0.7800 time: 1.7495s\n",
            "Epoch: 0418 loss_train: 0.7625 acc_train: 0.8200 loss_val: 1.1696 acc_val: 0.7800 time: 1.7533s\n",
            "Epoch: 0419 loss_train: 0.7522 acc_train: 0.8200 loss_val: 1.1698 acc_val: 0.7800 time: 1.7569s\n",
            "Epoch: 0420 loss_train: 0.7773 acc_train: 0.8200 loss_val: 1.1701 acc_val: 0.7800 time: 1.7623s\n",
            "Epoch: 0421 loss_train: 0.7690 acc_train: 0.8200 loss_val: 1.1706 acc_val: 0.7800 time: 1.7669s\n",
            "Epoch: 0422 loss_train: 0.7928 acc_train: 0.8200 loss_val: 1.1708 acc_val: 0.7800 time: 1.7714s\n",
            "Epoch: 0423 loss_train: 0.8196 acc_train: 0.8000 loss_val: 1.1717 acc_val: 0.7800 time: 1.7753s\n",
            "Epoch: 0424 loss_train: 0.7840 acc_train: 0.8200 loss_val: 1.1731 acc_val: 0.7800 time: 1.7790s\n",
            "Epoch: 0425 loss_train: 0.7781 acc_train: 0.8200 loss_val: 1.1745 acc_val: 0.7800 time: 1.7843s\n",
            "Epoch: 0426 loss_train: 0.7351 acc_train: 0.8400 loss_val: 1.1760 acc_val: 0.7800 time: 1.7881s\n",
            "Epoch: 0427 loss_train: 0.8157 acc_train: 0.8000 loss_val: 1.1787 acc_val: 0.7800 time: 1.7918s\n",
            "Epoch: 0428 loss_train: 0.7864 acc_train: 0.8000 loss_val: 1.1799 acc_val: 0.7800 time: 1.7956s\n",
            "Epoch: 0429 loss_train: 0.7386 acc_train: 0.8200 loss_val: 1.1815 acc_val: 0.7800 time: 1.7995s\n",
            "Epoch: 0430 loss_train: 0.7431 acc_train: 0.8200 loss_val: 1.1818 acc_val: 0.7800 time: 1.8033s\n",
            "Epoch: 0431 loss_train: 0.7382 acc_train: 0.8200 loss_val: 1.1817 acc_val: 0.7800 time: 1.8070s\n",
            "Epoch: 0432 loss_train: 0.7719 acc_train: 0.8200 loss_val: 1.1813 acc_val: 0.7800 time: 1.8111s\n",
            "Epoch: 0433 loss_train: 0.7435 acc_train: 0.8200 loss_val: 1.1807 acc_val: 0.7800 time: 1.8149s\n",
            "Epoch: 0434 loss_train: 0.7643 acc_train: 0.8200 loss_val: 1.1798 acc_val: 0.7800 time: 1.8187s\n",
            "Epoch: 0435 loss_train: 0.7258 acc_train: 0.8400 loss_val: 1.1789 acc_val: 0.7800 time: 1.8226s\n",
            "Epoch: 0436 loss_train: 0.7028 acc_train: 0.8200 loss_val: 1.1783 acc_val: 0.7800 time: 1.8291s\n",
            "Epoch: 0437 loss_train: 0.7257 acc_train: 0.8200 loss_val: 1.1783 acc_val: 0.7800 time: 1.8330s\n",
            "Epoch: 0438 loss_train: 0.8131 acc_train: 0.8000 loss_val: 1.1791 acc_val: 0.7800 time: 1.8367s\n",
            "Epoch: 0439 loss_train: 0.7418 acc_train: 0.8200 loss_val: 1.1809 acc_val: 0.7800 time: 1.8406s\n",
            "Epoch: 0440 loss_train: 0.7612 acc_train: 0.8200 loss_val: 1.1831 acc_val: 0.7800 time: 1.8446s\n",
            "Epoch: 0441 loss_train: 0.7296 acc_train: 0.8400 loss_val: 1.1850 acc_val: 0.7800 time: 1.8484s\n",
            "Epoch: 0442 loss_train: 0.7165 acc_train: 0.8200 loss_val: 1.1883 acc_val: 0.7800 time: 1.8523s\n",
            "Epoch: 0443 loss_train: 0.7013 acc_train: 0.8200 loss_val: 1.1941 acc_val: 0.7800 time: 1.8562s\n",
            "Epoch: 0444 loss_train: 0.7434 acc_train: 0.8200 loss_val: 1.2025 acc_val: 0.7800 time: 1.8602s\n",
            "Epoch: 0445 loss_train: 0.7945 acc_train: 0.8200 loss_val: 1.2028 acc_val: 0.7800 time: 1.8639s\n",
            "Epoch: 0446 loss_train: 0.7922 acc_train: 0.8000 loss_val: 1.1994 acc_val: 0.7800 time: 1.8677s\n",
            "Epoch: 0447 loss_train: 0.8128 acc_train: 0.8000 loss_val: 1.1956 acc_val: 0.7800 time: 1.8716s\n",
            "Epoch: 0448 loss_train: 0.7708 acc_train: 0.8000 loss_val: 1.1922 acc_val: 0.7800 time: 1.8756s\n",
            "Epoch: 0449 loss_train: 0.7300 acc_train: 0.8400 loss_val: 1.1881 acc_val: 0.7800 time: 1.8794s\n",
            "Epoch: 0450 loss_train: 0.7199 acc_train: 0.8200 loss_val: 1.1851 acc_val: 0.7800 time: 1.8839s\n",
            "Epoch: 0451 loss_train: 0.7695 acc_train: 0.8200 loss_val: 1.1832 acc_val: 0.7800 time: 1.8856s\n",
            "Epoch: 0452 loss_train: 0.7672 acc_train: 0.8200 loss_val: 1.1808 acc_val: 0.7800 time: 1.8874s\n",
            "Epoch: 0453 loss_train: 0.7311 acc_train: 0.8200 loss_val: 1.1801 acc_val: 0.7800 time: 1.8899s\n",
            "Epoch: 0454 loss_train: 0.7198 acc_train: 0.8400 loss_val: 1.1797 acc_val: 0.7800 time: 1.8937s\n",
            "Epoch: 0455 loss_train: 0.8011 acc_train: 0.8000 loss_val: 1.1793 acc_val: 0.7800 time: 1.8960s\n",
            "Epoch: 0456 loss_train: 0.8726 acc_train: 0.7800 loss_val: 1.1790 acc_val: 0.7800 time: 1.9130s\n",
            "Epoch: 0457 loss_train: 0.7589 acc_train: 0.8200 loss_val: 1.1788 acc_val: 0.7800 time: 1.9170s\n",
            "Epoch: 0458 loss_train: 0.7894 acc_train: 0.8000 loss_val: 1.1786 acc_val: 0.7800 time: 1.9208s\n",
            "Epoch: 0459 loss_train: 0.7232 acc_train: 0.8200 loss_val: 1.1792 acc_val: 0.7800 time: 1.9246s\n",
            "Epoch: 0460 loss_train: 0.7405 acc_train: 0.8200 loss_val: 1.1794 acc_val: 0.7800 time: 1.9345s\n",
            "Epoch: 0461 loss_train: 0.7036 acc_train: 0.8400 loss_val: 1.1799 acc_val: 0.7800 time: 1.9392s\n",
            "Epoch: 0462 loss_train: 0.7200 acc_train: 0.8200 loss_val: 1.1805 acc_val: 0.7800 time: 1.9435s\n",
            "Epoch: 0463 loss_train: 0.7454 acc_train: 0.8000 loss_val: 1.1820 acc_val: 0.7800 time: 1.9477s\n",
            "Epoch: 0464 loss_train: 0.7461 acc_train: 0.8200 loss_val: 1.1809 acc_val: 0.7800 time: 1.9518s\n",
            "Epoch: 0465 loss_train: 0.7912 acc_train: 0.8000 loss_val: 1.1804 acc_val: 0.7800 time: 1.9560s\n",
            "Epoch: 0466 loss_train: 0.7697 acc_train: 0.8000 loss_val: 1.1808 acc_val: 0.7800 time: 1.9599s\n",
            "Epoch: 0467 loss_train: 0.7102 acc_train: 0.8200 loss_val: 1.1815 acc_val: 0.7800 time: 1.9638s\n",
            "Epoch: 0468 loss_train: 0.7440 acc_train: 0.8200 loss_val: 1.1821 acc_val: 0.7800 time: 1.9678s\n",
            "Epoch: 0469 loss_train: 0.7842 acc_train: 0.8000 loss_val: 1.1835 acc_val: 0.7800 time: 1.9718s\n",
            "Epoch: 0470 loss_train: 0.7699 acc_train: 0.8200 loss_val: 1.1850 acc_val: 0.7800 time: 1.9759s\n",
            "Epoch: 0471 loss_train: 0.7219 acc_train: 0.8200 loss_val: 1.1876 acc_val: 0.7800 time: 1.9798s\n",
            "Epoch: 0472 loss_train: 0.7568 acc_train: 0.8000 loss_val: 1.1907 acc_val: 0.7800 time: 1.9832s\n",
            "Epoch: 0473 loss_train: 0.7285 acc_train: 0.8400 loss_val: 1.1953 acc_val: 0.7800 time: 1.9888s\n",
            "Epoch: 0474 loss_train: 0.7419 acc_train: 0.8200 loss_val: 1.1975 acc_val: 0.7800 time: 1.9927s\n",
            "Epoch: 0475 loss_train: 0.7546 acc_train: 0.8400 loss_val: 1.1975 acc_val: 0.7800 time: 1.9967s\n",
            "Epoch: 0476 loss_train: 0.7616 acc_train: 0.8200 loss_val: 1.1965 acc_val: 0.7800 time: 2.0006s\n",
            "Epoch: 0477 loss_train: 0.6874 acc_train: 0.8400 loss_val: 1.1966 acc_val: 0.7800 time: 2.0046s\n",
            "Epoch: 0478 loss_train: 0.7788 acc_train: 0.8200 loss_val: 1.1900 acc_val: 0.7800 time: 2.0086s\n",
            "Epoch: 0479 loss_train: 0.7071 acc_train: 0.8200 loss_val: 1.1861 acc_val: 0.7800 time: 2.0126s\n",
            "Epoch: 0480 loss_train: 0.7295 acc_train: 0.8200 loss_val: 1.1844 acc_val: 0.7800 time: 2.0166s\n",
            "Epoch: 0481 loss_train: 0.7582 acc_train: 0.8200 loss_val: 1.1842 acc_val: 0.7800 time: 2.0207s\n",
            "Epoch: 0482 loss_train: 0.7274 acc_train: 0.8200 loss_val: 1.1848 acc_val: 0.7800 time: 2.0247s\n",
            "Epoch: 0483 loss_train: 0.7513 acc_train: 0.8200 loss_val: 1.1861 acc_val: 0.7800 time: 2.0292s\n",
            "Epoch: 0484 loss_train: 0.7781 acc_train: 0.8200 loss_val: 1.1881 acc_val: 0.7800 time: 2.0333s\n",
            "Epoch: 0485 loss_train: 0.7593 acc_train: 0.8200 loss_val: 1.1914 acc_val: 0.7800 time: 2.0372s\n",
            "Epoch: 0486 loss_train: 0.7239 acc_train: 0.8400 loss_val: 1.1948 acc_val: 0.7800 time: 2.0412s\n",
            "Epoch: 0487 loss_train: 0.7542 acc_train: 0.8200 loss_val: 1.1975 acc_val: 0.7800 time: 2.0451s\n",
            "Epoch: 0488 loss_train: 0.7718 acc_train: 0.8200 loss_val: 1.2025 acc_val: 0.7800 time: 2.0490s\n",
            "Epoch: 0489 loss_train: 0.7301 acc_train: 0.8200 loss_val: 1.2033 acc_val: 0.7800 time: 2.0529s\n",
            "Epoch: 0490 loss_train: 0.7210 acc_train: 0.8200 loss_val: 1.2049 acc_val: 0.7800 time: 2.0567s\n",
            "Epoch: 0491 loss_train: 0.7388 acc_train: 0.8200 loss_val: 1.2063 acc_val: 0.7800 time: 2.0604s\n",
            "Epoch: 0492 loss_train: 0.7413 acc_train: 0.8000 loss_val: 1.2105 acc_val: 0.7800 time: 2.0642s\n",
            "Epoch: 0493 loss_train: 0.7056 acc_train: 0.8200 loss_val: 1.2154 acc_val: 0.7600 time: 2.0682s\n",
            "Epoch: 0494 loss_train: 0.7719 acc_train: 0.8000 loss_val: 1.2207 acc_val: 0.7600 time: 2.0719s\n",
            "Epoch: 0495 loss_train: 0.7409 acc_train: 0.8200 loss_val: 1.2233 acc_val: 0.7600 time: 2.0756s\n",
            "Epoch: 0496 loss_train: 0.7410 acc_train: 0.8200 loss_val: 1.2228 acc_val: 0.7600 time: 2.0794s\n",
            "Epoch: 0497 loss_train: 0.8399 acc_train: 0.8000 loss_val: 1.2211 acc_val: 0.7600 time: 2.0834s\n",
            "Epoch: 0498 loss_train: 0.7075 acc_train: 0.8200 loss_val: 1.2202 acc_val: 0.7600 time: 2.0873s\n",
            "Epoch: 0499 loss_train: 0.8390 acc_train: 0.8200 loss_val: 1.2170 acc_val: 0.7800 time: 2.0967s\n",
            "Epoch: 0500 loss_train: 0.7386 acc_train: 0.8200 loss_val: 1.2149 acc_val: 0.7800 time: 2.1013s\n",
            "Epoch: 0501 loss_train: 0.7195 acc_train: 0.8400 loss_val: 1.2137 acc_val: 0.7800 time: 2.1055s\n",
            "Epoch: 0502 loss_train: 0.7962 acc_train: 0.8000 loss_val: 1.2149 acc_val: 0.7800 time: 2.1094s\n",
            "Epoch: 0503 loss_train: 0.7119 acc_train: 0.8400 loss_val: 1.2112 acc_val: 0.7800 time: 2.1135s\n",
            "Epoch: 0504 loss_train: 0.7138 acc_train: 0.8400 loss_val: 1.2101 acc_val: 0.7800 time: 2.1174s\n",
            "Epoch: 0505 loss_train: 0.7035 acc_train: 0.8400 loss_val: 1.2103 acc_val: 0.7800 time: 2.1213s\n",
            "Epoch: 0506 loss_train: 0.7638 acc_train: 0.8000 loss_val: 1.1946 acc_val: 0.7800 time: 2.1255s\n",
            "Epoch: 0507 loss_train: 0.6953 acc_train: 0.8200 loss_val: 1.1875 acc_val: 0.7800 time: 2.1297s\n",
            "Epoch: 0508 loss_train: 0.7934 acc_train: 0.8000 loss_val: 1.1837 acc_val: 0.7800 time: 2.1336s\n",
            "Epoch: 0509 loss_train: 0.7038 acc_train: 0.8200 loss_val: 1.1819 acc_val: 0.7800 time: 2.1376s\n",
            "Epoch: 0510 loss_train: 0.7533 acc_train: 0.8200 loss_val: 1.1812 acc_val: 0.7800 time: 2.1414s\n",
            "Epoch: 0511 loss_train: 0.7332 acc_train: 0.8200 loss_val: 1.1810 acc_val: 0.7800 time: 2.1453s\n",
            "Epoch: 0512 loss_train: 0.8102 acc_train: 0.8000 loss_val: 1.1803 acc_val: 0.7800 time: 2.1496s\n",
            "Epoch: 0513 loss_train: 0.7851 acc_train: 0.8000 loss_val: 1.1792 acc_val: 0.7800 time: 2.1537s\n",
            "Epoch: 0514 loss_train: 0.6974 acc_train: 0.8400 loss_val: 1.1786 acc_val: 0.7800 time: 2.1582s\n",
            "Epoch: 0515 loss_train: 0.7296 acc_train: 0.8400 loss_val: 1.1782 acc_val: 0.7800 time: 2.1626s\n",
            "Epoch: 0516 loss_train: 0.7691 acc_train: 0.8200 loss_val: 1.1779 acc_val: 0.7800 time: 2.1665s\n",
            "Epoch: 0517 loss_train: 0.7188 acc_train: 0.8400 loss_val: 1.1780 acc_val: 0.7800 time: 2.1705s\n",
            "Epoch: 0518 loss_train: 0.7339 acc_train: 0.8200 loss_val: 1.1783 acc_val: 0.7800 time: 2.1748s\n",
            "Epoch: 0519 loss_train: 0.7415 acc_train: 0.8200 loss_val: 1.1786 acc_val: 0.7800 time: 2.1798s\n",
            "Epoch: 0520 loss_train: 0.7830 acc_train: 0.8000 loss_val: 1.1785 acc_val: 0.7800 time: 2.1857s\n",
            "Epoch: 0521 loss_train: 0.7711 acc_train: 0.8200 loss_val: 1.1781 acc_val: 0.7800 time: 2.1884s\n",
            "Epoch: 0522 loss_train: 0.7049 acc_train: 0.8200 loss_val: 1.1775 acc_val: 0.7800 time: 2.1916s\n",
            "Epoch: 0523 loss_train: 0.7081 acc_train: 0.8200 loss_val: 1.1768 acc_val: 0.7800 time: 2.1956s\n",
            "Epoch: 0524 loss_train: 0.7624 acc_train: 0.8200 loss_val: 1.1761 acc_val: 0.7800 time: 2.1995s\n",
            "Epoch: 0525 loss_train: 0.7450 acc_train: 0.8400 loss_val: 1.1758 acc_val: 0.7800 time: 2.2042s\n",
            "Epoch: 0526 loss_train: 0.7186 acc_train: 0.8400 loss_val: 1.1765 acc_val: 0.7800 time: 2.2082s\n",
            "Epoch: 0527 loss_train: 0.7682 acc_train: 0.8200 loss_val: 1.1781 acc_val: 0.7800 time: 2.2120s\n",
            "Epoch: 0528 loss_train: 0.7177 acc_train: 0.8400 loss_val: 1.1800 acc_val: 0.7800 time: 2.2159s\n",
            "Epoch: 0529 loss_train: 0.8034 acc_train: 0.8000 loss_val: 1.1834 acc_val: 0.7800 time: 2.2199s\n",
            "Epoch: 0530 loss_train: 0.7612 acc_train: 0.8200 loss_val: 1.1876 acc_val: 0.7800 time: 2.2237s\n",
            "Epoch: 0531 loss_train: 0.7666 acc_train: 0.8200 loss_val: 1.1931 acc_val: 0.7800 time: 2.2273s\n",
            "Epoch: 0532 loss_train: 0.7322 acc_train: 0.8400 loss_val: 1.2018 acc_val: 0.7600 time: 2.2311s\n",
            "Epoch: 0533 loss_train: 0.6796 acc_train: 0.8400 loss_val: 1.2107 acc_val: 0.7600 time: 2.2349s\n",
            "Epoch: 0534 loss_train: 0.7424 acc_train: 0.8200 loss_val: 1.2222 acc_val: 0.7600 time: 2.2386s\n",
            "Epoch: 0535 loss_train: 0.7024 acc_train: 0.8400 loss_val: 1.2274 acc_val: 0.7600 time: 2.2422s\n",
            "Epoch: 0536 loss_train: 0.7121 acc_train: 0.8400 loss_val: 1.2349 acc_val: 0.7600 time: 2.2460s\n",
            "Epoch: 0537 loss_train: 0.7555 acc_train: 0.8000 loss_val: 1.2361 acc_val: 0.7600 time: 2.2497s\n",
            "Epoch: 0538 loss_train: 0.7315 acc_train: 0.8200 loss_val: 1.2402 acc_val: 0.7600 time: 2.2534s\n",
            "Epoch: 0539 loss_train: 0.6942 acc_train: 0.8400 loss_val: 1.2457 acc_val: 0.7600 time: 2.2574s\n",
            "Epoch: 0540 loss_train: 0.7634 acc_train: 0.8200 loss_val: 1.2509 acc_val: 0.7600 time: 2.2613s\n",
            "Epoch: 0541 loss_train: 0.7410 acc_train: 0.8000 loss_val: 1.2588 acc_val: 0.7600 time: 2.2651s\n",
            "Epoch: 0542 loss_train: 0.7452 acc_train: 0.8200 loss_val: 1.2642 acc_val: 0.7600 time: 2.2690s\n",
            "Epoch: 0543 loss_train: 0.7051 acc_train: 0.8400 loss_val: 1.2658 acc_val: 0.7600 time: 2.2728s\n",
            "Epoch: 0544 loss_train: 0.6956 acc_train: 0.8400 loss_val: 1.2656 acc_val: 0.7600 time: 2.2765s\n",
            "Epoch: 0545 loss_train: 0.6841 acc_train: 0.8400 loss_val: 1.2664 acc_val: 0.7600 time: 2.2803s\n",
            "Epoch: 0546 loss_train: 0.7009 acc_train: 0.8400 loss_val: 1.2663 acc_val: 0.7600 time: 2.2841s\n",
            "Epoch: 0547 loss_train: 0.7141 acc_train: 0.8400 loss_val: 1.2648 acc_val: 0.7600 time: 2.2882s\n",
            "Epoch: 0548 loss_train: 0.7538 acc_train: 0.8200 loss_val: 1.2654 acc_val: 0.7600 time: 2.2932s\n",
            "Epoch: 0549 loss_train: 0.7109 acc_train: 0.8400 loss_val: 1.2594 acc_val: 0.7600 time: 2.3022s\n",
            "Epoch: 0550 loss_train: 0.7697 acc_train: 0.8200 loss_val: 1.2470 acc_val: 0.7600 time: 2.3062s\n",
            "Epoch: 0551 loss_train: 0.6970 acc_train: 0.8400 loss_val: 1.2433 acc_val: 0.7600 time: 2.3101s\n",
            "Epoch: 0552 loss_train: 0.7250 acc_train: 0.8200 loss_val: 1.2419 acc_val: 0.7600 time: 2.3138s\n",
            "Epoch: 0553 loss_train: 0.7689 acc_train: 0.8000 loss_val: 1.2432 acc_val: 0.7600 time: 2.3176s\n",
            "Epoch: 0554 loss_train: 0.7165 acc_train: 0.8200 loss_val: 1.2381 acc_val: 0.7600 time: 2.3214s\n",
            "Epoch: 0555 loss_train: 0.7046 acc_train: 0.8200 loss_val: 1.2329 acc_val: 0.7600 time: 2.3252s\n",
            "Epoch: 0556 loss_train: 0.7499 acc_train: 0.8200 loss_val: 1.2312 acc_val: 0.7600 time: 2.3290s\n",
            "Epoch: 0557 loss_train: 0.7216 acc_train: 0.8200 loss_val: 1.2334 acc_val: 0.7600 time: 2.3332s\n",
            "Epoch: 0558 loss_train: 0.7113 acc_train: 0.8400 loss_val: 1.2363 acc_val: 0.7600 time: 2.3373s\n",
            "Epoch: 0559 loss_train: 0.6882 acc_train: 0.8400 loss_val: 1.2417 acc_val: 0.7600 time: 2.3412s\n",
            "Epoch: 0560 loss_train: 0.7284 acc_train: 0.8200 loss_val: 1.2457 acc_val: 0.7600 time: 2.3449s\n",
            "Epoch: 0561 loss_train: 0.7155 acc_train: 0.8400 loss_val: 1.2485 acc_val: 0.7600 time: 2.3488s\n",
            "Epoch: 0562 loss_train: 0.7361 acc_train: 0.8200 loss_val: 1.2506 acc_val: 0.7600 time: 2.3528s\n",
            "Epoch: 0563 loss_train: 0.7880 acc_train: 0.8000 loss_val: 1.2519 acc_val: 0.7600 time: 2.3565s\n",
            "Epoch: 0564 loss_train: 0.7229 acc_train: 0.8200 loss_val: 1.2420 acc_val: 0.7600 time: 2.3602s\n",
            "Epoch: 0565 loss_train: 0.7186 acc_train: 0.8400 loss_val: 1.2340 acc_val: 0.7600 time: 2.3640s\n",
            "Epoch: 0566 loss_train: 0.7008 acc_train: 0.8400 loss_val: 1.2152 acc_val: 0.7800 time: 2.3678s\n",
            "Epoch: 0567 loss_train: 0.7005 acc_train: 0.8400 loss_val: 1.2081 acc_val: 0.7800 time: 2.3716s\n",
            "Epoch: 0568 loss_train: 0.7349 acc_train: 0.8200 loss_val: 1.2026 acc_val: 0.7800 time: 2.3754s\n",
            "Epoch: 0569 loss_train: 0.6917 acc_train: 0.8200 loss_val: 1.2002 acc_val: 0.7800 time: 2.3793s\n",
            "Epoch: 0570 loss_train: 0.7369 acc_train: 0.8200 loss_val: 1.1987 acc_val: 0.7800 time: 2.3830s\n",
            "Epoch: 0571 loss_train: 0.7245 acc_train: 0.8200 loss_val: 1.1978 acc_val: 0.7800 time: 2.3891s\n",
            "Epoch: 0572 loss_train: 0.7041 acc_train: 0.8400 loss_val: 1.1974 acc_val: 0.7800 time: 2.3944s\n",
            "Epoch: 0573 loss_train: 0.6974 acc_train: 0.8400 loss_val: 1.1974 acc_val: 0.7800 time: 2.3991s\n",
            "Epoch: 0574 loss_train: 0.6970 acc_train: 0.8200 loss_val: 1.1979 acc_val: 0.7800 time: 2.4038s\n",
            "Epoch: 0575 loss_train: 0.6999 acc_train: 0.8200 loss_val: 1.1987 acc_val: 0.7800 time: 2.4058s\n",
            "Epoch: 0576 loss_train: 0.7592 acc_train: 0.8000 loss_val: 1.1968 acc_val: 0.7800 time: 2.4124s\n",
            "Epoch: 0577 loss_train: 0.7339 acc_train: 0.8000 loss_val: 1.1967 acc_val: 0.7800 time: 2.4166s\n",
            "Epoch: 0578 loss_train: 0.7745 acc_train: 0.8000 loss_val: 1.1983 acc_val: 0.7800 time: 2.4204s\n",
            "Epoch: 0579 loss_train: 0.7754 acc_train: 0.8000 loss_val: 1.1936 acc_val: 0.7800 time: 2.4242s\n",
            "Epoch: 0580 loss_train: 0.7369 acc_train: 0.8200 loss_val: 1.1921 acc_val: 0.7800 time: 2.4281s\n",
            "Epoch: 0581 loss_train: 0.7090 acc_train: 0.8400 loss_val: 1.1912 acc_val: 0.7800 time: 2.4320s\n",
            "Epoch: 0582 loss_train: 0.7646 acc_train: 0.8000 loss_val: 1.1906 acc_val: 0.7800 time: 2.4358s\n",
            "Epoch: 0583 loss_train: 0.8369 acc_train: 0.8000 loss_val: 1.1901 acc_val: 0.7800 time: 2.4399s\n",
            "Epoch: 0584 loss_train: 0.7332 acc_train: 0.8200 loss_val: 1.1896 acc_val: 0.7800 time: 2.4438s\n",
            "Epoch: 0585 loss_train: 0.7089 acc_train: 0.8400 loss_val: 1.1893 acc_val: 0.7800 time: 2.4477s\n",
            "Epoch: 0586 loss_train: 0.7203 acc_train: 0.8200 loss_val: 1.1889 acc_val: 0.7800 time: 2.4515s\n",
            "Epoch: 0587 loss_train: 0.7063 acc_train: 0.8400 loss_val: 1.1880 acc_val: 0.7800 time: 2.4553s\n",
            "Epoch: 0588 loss_train: 0.7382 acc_train: 0.8200 loss_val: 1.1877 acc_val: 0.7800 time: 2.4590s\n",
            "Epoch: 0589 loss_train: 0.7277 acc_train: 0.8200 loss_val: 1.1882 acc_val: 0.7800 time: 2.4628s\n",
            "Epoch: 0590 loss_train: 0.8102 acc_train: 0.8000 loss_val: 1.1886 acc_val: 0.7800 time: 2.4666s\n",
            "Epoch: 0591 loss_train: 0.7769 acc_train: 0.8200 loss_val: 1.1889 acc_val: 0.7800 time: 2.4703s\n",
            "Epoch: 0592 loss_train: 0.7315 acc_train: 0.8200 loss_val: 1.1899 acc_val: 0.7800 time: 2.4741s\n",
            "Epoch: 0593 loss_train: 0.6818 acc_train: 0.8400 loss_val: 1.1922 acc_val: 0.7800 time: 2.4777s\n",
            "Epoch: 0594 loss_train: 0.7481 acc_train: 0.8200 loss_val: 1.1970 acc_val: 0.7800 time: 2.4814s\n",
            "Epoch: 0595 loss_train: 0.8510 acc_train: 0.7800 loss_val: 1.1948 acc_val: 0.7800 time: 2.4853s\n",
            "Epoch: 0596 loss_train: 0.7110 acc_train: 0.8400 loss_val: 1.1947 acc_val: 0.7800 time: 2.4890s\n",
            "Epoch: 0597 loss_train: 0.6888 acc_train: 0.8400 loss_val: 1.1949 acc_val: 0.7800 time: 2.4927s\n",
            "Epoch: 0598 loss_train: 0.7094 acc_train: 0.8200 loss_val: 1.1958 acc_val: 0.7800 time: 2.4963s\n",
            "Epoch: 0599 loss_train: 0.6761 acc_train: 0.8400 loss_val: 1.1970 acc_val: 0.7800 time: 2.5037s\n",
            "Epoch: 0600 loss_train: 0.6981 acc_train: 0.8400 loss_val: 1.1977 acc_val: 0.7800 time: 2.5053s\n",
            "Epoch: 0601 loss_train: 0.7212 acc_train: 0.8200 loss_val: 1.1989 acc_val: 0.7800 time: 2.5143s\n",
            "Epoch: 0602 loss_train: 0.6766 acc_train: 0.8400 loss_val: 1.2024 acc_val: 0.7800 time: 2.5182s\n",
            "Epoch: 0603 loss_train: 0.7418 acc_train: 0.8200 loss_val: 1.2100 acc_val: 0.7800 time: 2.5221s\n",
            "Epoch: 0604 loss_train: 0.6838 acc_train: 0.8400 loss_val: 1.2173 acc_val: 0.7600 time: 2.5259s\n",
            "Epoch: 0605 loss_train: 0.7037 acc_train: 0.8400 loss_val: 1.2264 acc_val: 0.7600 time: 2.5305s\n",
            "Epoch: 0606 loss_train: 0.7128 acc_train: 0.8200 loss_val: 1.2407 acc_val: 0.7600 time: 2.5341s\n",
            "Epoch: 0607 loss_train: 0.6939 acc_train: 0.8400 loss_val: 1.2538 acc_val: 0.7600 time: 2.5379s\n",
            "Epoch: 0608 loss_train: 0.7282 acc_train: 0.8200 loss_val: 1.2621 acc_val: 0.7600 time: 2.5421s\n",
            "Epoch: 0609 loss_train: 0.8509 acc_train: 0.8000 loss_val: 1.2185 acc_val: 0.7800 time: 2.5461s\n",
            "Epoch: 0610 loss_train: 0.7422 acc_train: 0.8200 loss_val: 1.1995 acc_val: 0.7800 time: 2.5500s\n",
            "Epoch: 0611 loss_train: 0.7604 acc_train: 0.8200 loss_val: 1.1920 acc_val: 0.7800 time: 2.5538s\n",
            "Epoch: 0612 loss_train: 0.7699 acc_train: 0.8200 loss_val: 1.1899 acc_val: 0.7800 time: 2.5575s\n",
            "Epoch: 0613 loss_train: 0.7326 acc_train: 0.8400 loss_val: 1.1900 acc_val: 0.7800 time: 2.5611s\n",
            "Epoch: 0614 loss_train: 0.7661 acc_train: 0.8200 loss_val: 1.1914 acc_val: 0.7800 time: 2.5647s\n",
            "Epoch: 0615 loss_train: 0.6861 acc_train: 0.8400 loss_val: 1.1923 acc_val: 0.7800 time: 2.5688s\n",
            "Epoch: 0616 loss_train: 0.7170 acc_train: 0.8200 loss_val: 1.1926 acc_val: 0.7800 time: 2.5726s\n",
            "Epoch: 0617 loss_train: 0.7413 acc_train: 0.8200 loss_val: 1.1928 acc_val: 0.7800 time: 2.5765s\n",
            "Epoch: 0618 loss_train: 0.7781 acc_train: 0.8000 loss_val: 1.1932 acc_val: 0.7800 time: 2.5802s\n",
            "Epoch: 0619 loss_train: 0.7254 acc_train: 0.8200 loss_val: 1.1943 acc_val: 0.7800 time: 2.5840s\n",
            "Epoch: 0620 loss_train: 0.6869 acc_train: 0.8400 loss_val: 1.1958 acc_val: 0.7800 time: 2.5878s\n",
            "Epoch: 0621 loss_train: 0.7130 acc_train: 0.8200 loss_val: 1.1976 acc_val: 0.7800 time: 2.5916s\n",
            "Epoch: 0622 loss_train: 0.6784 acc_train: 0.8400 loss_val: 1.1998 acc_val: 0.7800 time: 2.5956s\n",
            "Epoch: 0623 loss_train: 0.7384 acc_train: 0.8200 loss_val: 1.2024 acc_val: 0.7800 time: 2.5995s\n",
            "Epoch: 0624 loss_train: 0.7973 acc_train: 0.8000 loss_val: 1.2060 acc_val: 0.7800 time: 2.6034s\n",
            "Epoch: 0625 loss_train: 0.7239 acc_train: 0.8200 loss_val: 1.2089 acc_val: 0.7600 time: 2.6071s\n",
            "Epoch: 0626 loss_train: 0.7189 acc_train: 0.8200 loss_val: 1.2101 acc_val: 0.7600 time: 2.6109s\n",
            "Epoch: 0627 loss_train: 0.7205 acc_train: 0.8200 loss_val: 1.2112 acc_val: 0.7600 time: 2.6147s\n",
            "Epoch: 0628 loss_train: 0.7406 acc_train: 0.8200 loss_val: 1.2142 acc_val: 0.7600 time: 2.6199s\n",
            "Epoch: 0629 loss_train: 0.6995 acc_train: 0.8200 loss_val: 1.2166 acc_val: 0.7600 time: 2.6257s\n",
            "Epoch: 0630 loss_train: 0.7496 acc_train: 0.8200 loss_val: 1.2159 acc_val: 0.7600 time: 2.6295s\n",
            "Epoch: 0631 loss_train: 0.8087 acc_train: 0.8000 loss_val: 1.2098 acc_val: 0.7600 time: 2.6333s\n",
            "Epoch: 0632 loss_train: 0.7151 acc_train: 0.8200 loss_val: 1.2059 acc_val: 0.7600 time: 2.6370s\n",
            "Epoch: 0633 loss_train: 0.7320 acc_train: 0.8200 loss_val: 1.2025 acc_val: 0.7800 time: 2.6408s\n",
            "Epoch: 0634 loss_train: 0.7387 acc_train: 0.8200 loss_val: 1.1991 acc_val: 0.7800 time: 2.6445s\n",
            "Epoch: 0635 loss_train: 0.7053 acc_train: 0.8200 loss_val: 1.1964 acc_val: 0.7800 time: 2.6483s\n",
            "Epoch: 0636 loss_train: 0.7162 acc_train: 0.8200 loss_val: 1.1944 acc_val: 0.7800 time: 2.6520s\n",
            "Epoch: 0637 loss_train: 0.7241 acc_train: 0.8200 loss_val: 1.1932 acc_val: 0.7800 time: 2.6559s\n",
            "Epoch: 0638 loss_train: 0.6893 acc_train: 0.8400 loss_val: 1.1921 acc_val: 0.7800 time: 2.6596s\n",
            "Epoch: 0639 loss_train: 0.7461 acc_train: 0.8200 loss_val: 1.1891 acc_val: 0.7800 time: 2.6633s\n",
            "Epoch: 0640 loss_train: 0.6999 acc_train: 0.8400 loss_val: 1.1877 acc_val: 0.7800 time: 2.6670s\n",
            "Epoch: 0641 loss_train: 0.7632 acc_train: 0.8200 loss_val: 1.1870 acc_val: 0.7800 time: 2.6708s\n",
            "Epoch: 0642 loss_train: 0.7070 acc_train: 0.8400 loss_val: 1.1867 acc_val: 0.7800 time: 2.6745s\n",
            "Epoch: 0643 loss_train: 0.8167 acc_train: 0.8000 loss_val: 1.1863 acc_val: 0.7800 time: 2.6783s\n",
            "Epoch: 0644 loss_train: 0.6984 acc_train: 0.8200 loss_val: 1.1857 acc_val: 0.7800 time: 2.6822s\n",
            "Epoch: 0645 loss_train: 0.7714 acc_train: 0.8000 loss_val: 1.1855 acc_val: 0.7800 time: 2.6859s\n",
            "Epoch: 0646 loss_train: 0.7634 acc_train: 0.8000 loss_val: 1.1860 acc_val: 0.7800 time: 2.6900s\n",
            "Epoch: 0647 loss_train: 0.7129 acc_train: 0.8200 loss_val: 1.1874 acc_val: 0.7800 time: 2.6936s\n",
            "Epoch: 0648 loss_train: 0.7668 acc_train: 0.8200 loss_val: 1.1898 acc_val: 0.7800 time: 2.6975s\n",
            "Epoch: 0649 loss_train: 0.8308 acc_train: 0.8000 loss_val: 1.1916 acc_val: 0.7800 time: 2.7012s\n",
            "Epoch: 0650 loss_train: 0.7322 acc_train: 0.8200 loss_val: 1.1963 acc_val: 0.7800 time: 2.7050s\n",
            "Epoch: 0651 loss_train: 0.6762 acc_train: 0.8400 loss_val: 1.2027 acc_val: 0.7600 time: 2.7088s\n",
            "Epoch: 0652 loss_train: 0.7057 acc_train: 0.8200 loss_val: 1.2146 acc_val: 0.7600 time: 2.7192s\n",
            "Epoch: 0653 loss_train: 0.7961 acc_train: 0.8200 loss_val: 1.2314 acc_val: 0.7600 time: 2.7231s\n",
            "Epoch: 0654 loss_train: 0.7478 acc_train: 0.8000 loss_val: 1.2458 acc_val: 0.7600 time: 2.7269s\n",
            "Epoch: 0655 loss_train: 0.6960 acc_train: 0.8400 loss_val: 1.2615 acc_val: 0.7400 time: 2.7308s\n",
            "Epoch: 0656 loss_train: 0.6990 acc_train: 0.8400 loss_val: 1.2759 acc_val: 0.7400 time: 2.7346s\n",
            "Epoch: 0657 loss_train: 0.6778 acc_train: 0.8400 loss_val: 1.2880 acc_val: 0.7400 time: 2.7383s\n",
            "Epoch: 0658 loss_train: 0.7023 acc_train: 0.8200 loss_val: 1.2970 acc_val: 0.7400 time: 2.7425s\n",
            "Epoch: 0659 loss_train: 0.7896 acc_train: 0.8200 loss_val: 1.2961 acc_val: 0.7400 time: 2.7465s\n",
            "Epoch: 0660 loss_train: 0.7041 acc_train: 0.8200 loss_val: 1.2959 acc_val: 0.7400 time: 2.7503s\n",
            "Epoch: 0661 loss_train: 0.7278 acc_train: 0.8200 loss_val: 1.2991 acc_val: 0.7400 time: 2.7541s\n",
            "Epoch: 0662 loss_train: 0.7383 acc_train: 0.8200 loss_val: 1.3040 acc_val: 0.7400 time: 2.7578s\n",
            "Epoch: 0663 loss_train: 0.6568 acc_train: 0.8400 loss_val: 1.3066 acc_val: 0.7400 time: 2.7616s\n",
            "Epoch: 0664 loss_train: 0.6925 acc_train: 0.8200 loss_val: 1.3082 acc_val: 0.7400 time: 2.7652s\n",
            "Epoch: 0665 loss_train: 0.6983 acc_train: 0.8400 loss_val: 1.3131 acc_val: 0.7400 time: 2.7689s\n",
            "Epoch: 0666 loss_train: 0.7239 acc_train: 0.8400 loss_val: 1.3175 acc_val: 0.7400 time: 2.7728s\n",
            "Epoch: 0667 loss_train: 0.6666 acc_train: 0.8400 loss_val: 1.3193 acc_val: 0.7400 time: 2.7765s\n",
            "Epoch: 0668 loss_train: 0.6647 acc_train: 0.8400 loss_val: 1.3207 acc_val: 0.7400 time: 2.7803s\n",
            "Epoch: 0669 loss_train: 0.7481 acc_train: 0.8200 loss_val: 1.3183 acc_val: 0.7400 time: 2.7842s\n",
            "Epoch: 0670 loss_train: 0.6992 acc_train: 0.8400 loss_val: 1.2996 acc_val: 0.7400 time: 2.7879s\n",
            "Epoch: 0671 loss_train: 0.7045 acc_train: 0.8200 loss_val: 1.2806 acc_val: 0.7400 time: 2.7918s\n",
            "Epoch: 0672 loss_train: 0.6536 acc_train: 0.8400 loss_val: 1.2558 acc_val: 0.7600 time: 2.7958s\n",
            "Epoch: 0673 loss_train: 0.8365 acc_train: 0.8000 loss_val: 1.2389 acc_val: 0.7600 time: 2.8004s\n",
            "Epoch: 0674 loss_train: 0.7529 acc_train: 0.8000 loss_val: 1.2234 acc_val: 0.7600 time: 2.8050s\n",
            "Epoch: 0675 loss_train: 0.7435 acc_train: 0.8200 loss_val: 1.2169 acc_val: 0.7600 time: 2.8091s\n",
            "Epoch: 0676 loss_train: 0.7477 acc_train: 0.8200 loss_val: 1.2126 acc_val: 0.7600 time: 2.8129s\n",
            "Epoch: 0677 loss_train: 0.8296 acc_train: 0.8200 loss_val: 1.2077 acc_val: 0.7800 time: 2.8171s\n",
            "Epoch: 0678 loss_train: 0.7598 acc_train: 0.8200 loss_val: 1.2062 acc_val: 0.7800 time: 2.8211s\n",
            "Epoch: 0679 loss_train: 0.6791 acc_train: 0.8400 loss_val: 1.2054 acc_val: 0.7800 time: 2.8248s\n",
            "Epoch: 0680 loss_train: 0.7495 acc_train: 0.8000 loss_val: 1.2034 acc_val: 0.7800 time: 2.8285s\n",
            "Epoch: 0681 loss_train: 0.7150 acc_train: 0.8000 loss_val: 1.2034 acc_val: 0.7800 time: 2.8323s\n",
            "Epoch: 0682 loss_train: 0.6960 acc_train: 0.8400 loss_val: 1.2040 acc_val: 0.7800 time: 2.8361s\n",
            "Epoch: 0683 loss_train: 0.6720 acc_train: 0.8400 loss_val: 1.2046 acc_val: 0.7800 time: 2.8398s\n",
            "Epoch: 0684 loss_train: 0.7452 acc_train: 0.8200 loss_val: 1.2054 acc_val: 0.7800 time: 2.8439s\n",
            "Epoch: 0685 loss_train: 0.6503 acc_train: 0.8400 loss_val: 1.2035 acc_val: 0.7800 time: 2.8477s\n",
            "Epoch: 0686 loss_train: 0.7108 acc_train: 0.8400 loss_val: 1.2020 acc_val: 0.7800 time: 2.8514s\n",
            "Epoch: 0687 loss_train: 0.7088 acc_train: 0.8200 loss_val: 1.2016 acc_val: 0.7800 time: 2.8551s\n",
            "Epoch: 0688 loss_train: 0.7727 acc_train: 0.8200 loss_val: 1.2021 acc_val: 0.7800 time: 2.8590s\n",
            "Epoch: 0689 loss_train: 0.6782 acc_train: 0.8400 loss_val: 1.2036 acc_val: 0.7800 time: 2.8626s\n",
            "Epoch: 0690 loss_train: 0.7109 acc_train: 0.8200 loss_val: 1.2050 acc_val: 0.7800 time: 2.8663s\n",
            "Epoch: 0691 loss_train: 0.6967 acc_train: 0.8200 loss_val: 1.2069 acc_val: 0.7800 time: 2.8701s\n",
            "Epoch: 0692 loss_train: 0.7168 acc_train: 0.8400 loss_val: 1.2086 acc_val: 0.7800 time: 2.8739s\n",
            "Epoch: 0693 loss_train: 0.7260 acc_train: 0.8200 loss_val: 1.2102 acc_val: 0.7800 time: 2.8777s\n",
            "Epoch: 0694 loss_train: 0.7363 acc_train: 0.8200 loss_val: 1.2164 acc_val: 0.7600 time: 2.8816s\n",
            "Epoch: 0695 loss_train: 0.6707 acc_train: 0.8400 loss_val: 1.2235 acc_val: 0.7600 time: 2.8852s\n",
            "Epoch: 0696 loss_train: 0.7003 acc_train: 0.8400 loss_val: 1.2305 acc_val: 0.7600 time: 2.8888s\n",
            "Epoch: 0697 loss_train: 0.7389 acc_train: 0.8200 loss_val: 1.2380 acc_val: 0.7600 time: 2.8927s\n",
            "Epoch: 0698 loss_train: 0.7120 acc_train: 0.8400 loss_val: 1.2481 acc_val: 0.7600 time: 2.8964s\n",
            "Epoch: 0699 loss_train: 0.6748 acc_train: 0.8400 loss_val: 1.2594 acc_val: 0.7600 time: 2.8999s\n",
            "Epoch: 0700 loss_train: 0.7169 acc_train: 0.8200 loss_val: 1.2727 acc_val: 0.7400 time: 2.9037s\n",
            "Epoch: 0701 loss_train: 0.6742 acc_train: 0.8400 loss_val: 1.2840 acc_val: 0.7400 time: 2.9074s\n",
            "Epoch: 0702 loss_train: 0.7135 acc_train: 0.8200 loss_val: 1.2926 acc_val: 0.7400 time: 2.9113s\n",
            "Epoch: 0703 loss_train: 0.7435 acc_train: 0.8200 loss_val: 1.3003 acc_val: 0.7400 time: 2.9152s\n",
            "Epoch: 0704 loss_train: 0.7063 acc_train: 0.8000 loss_val: 1.3025 acc_val: 0.7400 time: 2.9260s\n",
            "Epoch: 0705 loss_train: 0.7019 acc_train: 0.8400 loss_val: 1.3003 acc_val: 0.7400 time: 2.9299s\n",
            "Epoch: 0706 loss_train: 0.7214 acc_train: 0.8200 loss_val: 1.2939 acc_val: 0.7400 time: 2.9337s\n",
            "Epoch: 0707 loss_train: 0.8128 acc_train: 0.8000 loss_val: 1.2955 acc_val: 0.7400 time: 2.9376s\n",
            "Epoch: 0708 loss_train: 0.7066 acc_train: 0.8200 loss_val: 1.2970 acc_val: 0.7400 time: 2.9436s\n",
            "Epoch: 0709 loss_train: 0.6512 acc_train: 0.8400 loss_val: 1.2981 acc_val: 0.7400 time: 2.9453s\n",
            "Epoch: 0710 loss_train: 0.7925 acc_train: 0.8200 loss_val: 1.2923 acc_val: 0.7400 time: 2.9470s\n",
            "Epoch: 0711 loss_train: 0.7081 acc_train: 0.8200 loss_val: 1.2929 acc_val: 0.7400 time: 2.9492s\n",
            "Epoch: 0712 loss_train: 0.7147 acc_train: 0.8200 loss_val: 1.3014 acc_val: 0.7400 time: 2.9518s\n",
            "Epoch: 0713 loss_train: 0.7187 acc_train: 0.8200 loss_val: 1.3035 acc_val: 0.7400 time: 2.9539s\n",
            "Epoch: 0714 loss_train: 0.6956 acc_train: 0.8400 loss_val: 1.3052 acc_val: 0.7400 time: 2.9639s\n",
            "Epoch: 0715 loss_train: 0.7518 acc_train: 0.8000 loss_val: 1.3068 acc_val: 0.7400 time: 2.9725s\n",
            "Epoch: 0716 loss_train: 0.6624 acc_train: 0.8400 loss_val: 1.3067 acc_val: 0.7400 time: 2.9763s\n",
            "Epoch: 0717 loss_train: 0.7078 acc_train: 0.8200 loss_val: 1.3020 acc_val: 0.7400 time: 2.9800s\n",
            "Epoch: 0718 loss_train: 0.6758 acc_train: 0.8400 loss_val: 1.2983 acc_val: 0.7400 time: 2.9838s\n",
            "Epoch: 0719 loss_train: 0.7211 acc_train: 0.8200 loss_val: 1.2963 acc_val: 0.7400 time: 2.9875s\n",
            "Epoch: 0720 loss_train: 0.6819 acc_train: 0.8400 loss_val: 1.2938 acc_val: 0.7400 time: 2.9938s\n",
            "Epoch: 0721 loss_train: 0.7697 acc_train: 0.8000 loss_val: 1.2957 acc_val: 0.7400 time: 3.0006s\n",
            "Epoch: 0722 loss_train: 0.7657 acc_train: 0.8200 loss_val: 1.2941 acc_val: 0.7400 time: 3.0057s\n",
            "Epoch: 0723 loss_train: 0.6811 acc_train: 0.8400 loss_val: 1.2927 acc_val: 0.7400 time: 3.0123s\n",
            "Epoch: 0724 loss_train: 0.6537 acc_train: 0.8400 loss_val: 1.2914 acc_val: 0.7400 time: 3.0166s\n",
            "Epoch: 0725 loss_train: 0.7188 acc_train: 0.8200 loss_val: 1.2901 acc_val: 0.7400 time: 3.0211s\n",
            "Epoch: 0726 loss_train: 0.7061 acc_train: 0.8200 loss_val: 1.2869 acc_val: 0.7400 time: 3.0257s\n",
            "Epoch: 0727 loss_train: 0.7416 acc_train: 0.8000 loss_val: 1.2911 acc_val: 0.7400 time: 3.0299s\n",
            "Epoch: 0728 loss_train: 0.6620 acc_train: 0.8400 loss_val: 1.2949 acc_val: 0.7400 time: 3.0338s\n",
            "Epoch: 0729 loss_train: 0.6870 acc_train: 0.8200 loss_val: 1.2929 acc_val: 0.7400 time: 3.0409s\n",
            "Epoch: 0730 loss_train: 0.6942 acc_train: 0.8200 loss_val: 1.2938 acc_val: 0.7400 time: 3.0448s\n",
            "Epoch: 0731 loss_train: 0.7703 acc_train: 0.8000 loss_val: 1.2973 acc_val: 0.7400 time: 3.0489s\n",
            "Epoch: 0732 loss_train: 0.8159 acc_train: 0.7800 loss_val: 1.2863 acc_val: 0.7400 time: 3.0530s\n",
            "Epoch: 0733 loss_train: 0.7074 acc_train: 0.8000 loss_val: 1.2790 acc_val: 0.7400 time: 3.0569s\n",
            "Epoch: 0734 loss_train: 0.6970 acc_train: 0.8200 loss_val: 1.2788 acc_val: 0.7400 time: 3.0609s\n",
            "Epoch: 0735 loss_train: 0.7385 acc_train: 0.8000 loss_val: 1.2760 acc_val: 0.7400 time: 3.0649s\n",
            "Epoch: 0736 loss_train: 0.7079 acc_train: 0.8200 loss_val: 1.2769 acc_val: 0.7400 time: 3.0688s\n",
            "Epoch: 0737 loss_train: 0.6333 acc_train: 0.8600 loss_val: 1.2751 acc_val: 0.7400 time: 3.0726s\n",
            "Epoch: 0738 loss_train: 0.7108 acc_train: 0.8200 loss_val: 1.2783 acc_val: 0.7400 time: 3.0767s\n",
            "Epoch: 0739 loss_train: 0.7416 acc_train: 0.8000 loss_val: 1.2773 acc_val: 0.7400 time: 3.0807s\n",
            "Epoch: 0740 loss_train: 0.7211 acc_train: 0.8200 loss_val: 1.2758 acc_val: 0.7400 time: 3.0838s\n",
            "Epoch: 0741 loss_train: 0.7127 acc_train: 0.8200 loss_val: 1.2764 acc_val: 0.7400 time: 3.0911s\n",
            "Epoch: 0742 loss_train: 0.7118 acc_train: 0.8000 loss_val: 1.2821 acc_val: 0.7400 time: 3.0952s\n",
            "Epoch: 0743 loss_train: 0.6760 acc_train: 0.8200 loss_val: 1.2854 acc_val: 0.7400 time: 3.0991s\n",
            "Epoch: 0744 loss_train: 0.7167 acc_train: 0.8200 loss_val: 1.2846 acc_val: 0.7400 time: 3.1031s\n",
            "Epoch: 0745 loss_train: 0.7905 acc_train: 0.8000 loss_val: 1.2790 acc_val: 0.7400 time: 3.1088s\n",
            "Epoch: 0746 loss_train: 0.7262 acc_train: 0.8000 loss_val: 1.2804 acc_val: 0.7400 time: 3.1149s\n",
            "Epoch: 0747 loss_train: 0.6858 acc_train: 0.8400 loss_val: 1.2823 acc_val: 0.7400 time: 3.1189s\n",
            "Epoch: 0748 loss_train: 0.6924 acc_train: 0.8200 loss_val: 1.2843 acc_val: 0.7400 time: 3.1338s\n",
            "Epoch: 0749 loss_train: 0.6959 acc_train: 0.8400 loss_val: 1.2876 acc_val: 0.7400 time: 3.1380s\n",
            "Epoch: 0750 loss_train: 0.6726 acc_train: 0.8200 loss_val: 1.2935 acc_val: 0.7400 time: 3.1418s\n",
            "Epoch: 0751 loss_train: 0.6882 acc_train: 0.8200 loss_val: 1.3014 acc_val: 0.7400 time: 3.1460s\n",
            "Epoch: 0752 loss_train: 0.7099 acc_train: 0.8200 loss_val: 1.3117 acc_val: 0.7400 time: 3.1498s\n",
            "Epoch: 0753 loss_train: 0.7117 acc_train: 0.8200 loss_val: 1.3120 acc_val: 0.7400 time: 3.1541s\n",
            "Epoch: 0754 loss_train: 0.6871 acc_train: 0.8400 loss_val: 1.3124 acc_val: 0.7400 time: 3.1581s\n",
            "Epoch: 0755 loss_train: 0.6994 acc_train: 0.8000 loss_val: 1.3057 acc_val: 0.7400 time: 3.1620s\n",
            "Epoch: 0756 loss_train: 0.7267 acc_train: 0.8200 loss_val: 1.3022 acc_val: 0.7200 time: 3.1658s\n",
            "Epoch: 0757 loss_train: 0.6339 acc_train: 0.8600 loss_val: 1.2996 acc_val: 0.7200 time: 3.1697s\n",
            "Epoch: 0758 loss_train: 0.6634 acc_train: 0.8400 loss_val: 1.2915 acc_val: 0.7400 time: 3.1736s\n",
            "Epoch: 0759 loss_train: 0.6860 acc_train: 0.8200 loss_val: 1.2866 acc_val: 0.7400 time: 3.1757s\n",
            "Epoch: 0760 loss_train: 0.6960 acc_train: 0.8400 loss_val: 1.2490 acc_val: 0.7400 time: 3.1779s\n",
            "Epoch: 0761 loss_train: 0.6990 acc_train: 0.8200 loss_val: 1.2260 acc_val: 0.7800 time: 3.1868s\n",
            "Epoch: 0762 loss_train: 0.6625 acc_train: 0.8400 loss_val: 1.2122 acc_val: 0.7800 time: 3.1906s\n",
            "Epoch: 0763 loss_train: 0.6416 acc_train: 0.8400 loss_val: 1.2053 acc_val: 0.7800 time: 3.1945s\n",
            "Epoch: 0764 loss_train: 0.6792 acc_train: 0.8200 loss_val: 1.2021 acc_val: 0.7800 time: 3.1982s\n",
            "Epoch: 0765 loss_train: 0.7664 acc_train: 0.8200 loss_val: 1.2007 acc_val: 0.7800 time: 3.2020s\n",
            "Epoch: 0766 loss_train: 0.7634 acc_train: 0.7800 loss_val: 1.1993 acc_val: 0.7800 time: 3.2068s\n",
            "Epoch: 0767 loss_train: 0.6942 acc_train: 0.8400 loss_val: 1.1990 acc_val: 0.7800 time: 3.2107s\n",
            "Epoch: 0768 loss_train: 0.6826 acc_train: 0.8400 loss_val: 1.2004 acc_val: 0.7800 time: 3.2144s\n",
            "Epoch: 0769 loss_train: 0.7670 acc_train: 0.8000 loss_val: 1.2016 acc_val: 0.7800 time: 3.2182s\n",
            "Epoch: 0770 loss_train: 0.7784 acc_train: 0.8200 loss_val: 1.2024 acc_val: 0.7800 time: 3.2257s\n",
            "Epoch: 0771 loss_train: 0.6938 acc_train: 0.8200 loss_val: 1.2046 acc_val: 0.7800 time: 3.2313s\n",
            "Epoch: 0772 loss_train: 0.6898 acc_train: 0.8200 loss_val: 1.2075 acc_val: 0.7800 time: 3.2352s\n",
            "Epoch: 0773 loss_train: 0.6855 acc_train: 0.8400 loss_val: 1.2104 acc_val: 0.7800 time: 3.2404s\n",
            "Epoch: 0774 loss_train: 0.7929 acc_train: 0.8000 loss_val: 1.2118 acc_val: 0.7800 time: 3.2451s\n",
            "Epoch: 0775 loss_train: 0.6827 acc_train: 0.8200 loss_val: 1.2158 acc_val: 0.7800 time: 3.2492s\n",
            "Epoch: 0776 loss_train: 0.7671 acc_train: 0.8200 loss_val: 1.2195 acc_val: 0.7800 time: 3.2529s\n",
            "Epoch: 0777 loss_train: 0.6764 acc_train: 0.8200 loss_val: 1.2232 acc_val: 0.7800 time: 3.2567s\n",
            "Epoch: 0778 loss_train: 0.7506 acc_train: 0.8200 loss_val: 1.2235 acc_val: 0.7800 time: 3.2621s\n",
            "Epoch: 0779 loss_train: 0.7002 acc_train: 0.8200 loss_val: 1.2228 acc_val: 0.7800 time: 3.2659s\n",
            "Epoch: 0780 loss_train: 0.6885 acc_train: 0.8400 loss_val: 1.2221 acc_val: 0.7800 time: 3.2747s\n",
            "Epoch: 0781 loss_train: 0.6562 acc_train: 0.8400 loss_val: 1.2211 acc_val: 0.7800 time: 3.2788s\n",
            "Epoch: 0782 loss_train: 0.7160 acc_train: 0.8200 loss_val: 1.2211 acc_val: 0.7800 time: 3.2826s\n",
            "Epoch: 0783 loss_train: 0.6948 acc_train: 0.8400 loss_val: 1.2220 acc_val: 0.7800 time: 3.2864s\n",
            "Epoch: 0784 loss_train: 0.7115 acc_train: 0.8200 loss_val: 1.2287 acc_val: 0.7600 time: 3.2901s\n",
            "Epoch: 0785 loss_train: 0.7575 acc_train: 0.8000 loss_val: 1.2316 acc_val: 0.7600 time: 3.2940s\n",
            "Epoch: 0786 loss_train: 0.7000 acc_train: 0.8200 loss_val: 1.2388 acc_val: 0.7400 time: 3.2979s\n",
            "Epoch: 0787 loss_train: 0.6673 acc_train: 0.8200 loss_val: 1.2458 acc_val: 0.7400 time: 3.3016s\n",
            "Epoch: 0788 loss_train: 0.6710 acc_train: 0.8400 loss_val: 1.2366 acc_val: 0.7600 time: 3.3053s\n",
            "Epoch: 0789 loss_train: 0.7652 acc_train: 0.8200 loss_val: 1.2346 acc_val: 0.7600 time: 3.3092s\n",
            "Epoch: 0790 loss_train: 0.6898 acc_train: 0.8200 loss_val: 1.2366 acc_val: 0.7600 time: 3.3131s\n",
            "Epoch: 0791 loss_train: 0.6843 acc_train: 0.8400 loss_val: 1.2464 acc_val: 0.7400 time: 3.3195s\n",
            "Epoch: 0792 loss_train: 0.7014 acc_train: 0.8200 loss_val: 1.2594 acc_val: 0.7400 time: 3.3234s\n",
            "Epoch: 0793 loss_train: 0.6481 acc_train: 0.8400 loss_val: 1.2777 acc_val: 0.7400 time: 3.3271s\n",
            "Epoch: 0794 loss_train: 0.6818 acc_train: 0.8400 loss_val: 1.3068 acc_val: 0.7200 time: 3.3303s\n",
            "Epoch: 0795 loss_train: 0.6778 acc_train: 0.8200 loss_val: 1.3387 acc_val: 0.7200 time: 3.3326s\n",
            "Epoch: 0796 loss_train: 0.6842 acc_train: 0.8400 loss_val: 1.3643 acc_val: 0.7200 time: 3.3348s\n",
            "Epoch: 0797 loss_train: 0.7068 acc_train: 0.8200 loss_val: 1.3830 acc_val: 0.7200 time: 3.3370s\n",
            "Epoch: 0798 loss_train: 0.7171 acc_train: 0.8200 loss_val: 1.3905 acc_val: 0.7200 time: 3.3391s\n",
            "Epoch: 0799 loss_train: 0.7150 acc_train: 0.8400 loss_val: 1.3868 acc_val: 0.7200 time: 3.3448s\n",
            "Epoch: 0800 loss_train: 0.6840 acc_train: 0.8400 loss_val: 1.3757 acc_val: 0.7200 time: 3.3639s\n",
            "Epoch: 0801 loss_train: 0.7361 acc_train: 0.8200 loss_val: 1.3657 acc_val: 0.7200 time: 3.3658s\n",
            "Epoch: 0802 loss_train: 0.6722 acc_train: 0.8400 loss_val: 1.3533 acc_val: 0.7200 time: 3.3676s\n",
            "Epoch: 0803 loss_train: 0.7336 acc_train: 0.8200 loss_val: 1.3520 acc_val: 0.7200 time: 3.3694s\n",
            "Epoch: 0804 loss_train: 0.7944 acc_train: 0.8000 loss_val: 1.3644 acc_val: 0.7200 time: 3.3806s\n",
            "Epoch: 0805 loss_train: 0.6970 acc_train: 0.8400 loss_val: 1.3718 acc_val: 0.7200 time: 3.3846s\n",
            "Epoch: 0806 loss_train: 0.7151 acc_train: 0.8200 loss_val: 1.3730 acc_val: 0.7200 time: 3.3859s\n",
            "Epoch: 0807 loss_train: 0.6881 acc_train: 0.8200 loss_val: 1.3600 acc_val: 0.7200 time: 3.3922s\n",
            "Epoch: 0808 loss_train: 0.6933 acc_train: 0.8200 loss_val: 1.3495 acc_val: 0.7400 time: 3.3962s\n",
            "Epoch: 0809 loss_train: 0.7408 acc_train: 0.8000 loss_val: 1.3197 acc_val: 0.7400 time: 3.4004s\n",
            "Epoch: 0810 loss_train: 0.7057 acc_train: 0.8200 loss_val: 1.2930 acc_val: 0.7400 time: 3.4056s\n",
            "Epoch: 0811 loss_train: 0.6567 acc_train: 0.8400 loss_val: 1.2705 acc_val: 0.7400 time: 3.4078s\n",
            "Epoch: 0812 loss_train: 0.7066 acc_train: 0.8400 loss_val: 1.2520 acc_val: 0.7400 time: 3.4136s\n",
            "Epoch: 0813 loss_train: 0.7132 acc_train: 0.8200 loss_val: 1.2391 acc_val: 0.7600 time: 3.4176s\n",
            "Epoch: 0814 loss_train: 0.6769 acc_train: 0.8200 loss_val: 1.2325 acc_val: 0.7600 time: 3.4216s\n",
            "Epoch: 0815 loss_train: 0.6867 acc_train: 0.8400 loss_val: 1.2286 acc_val: 0.7800 time: 3.4255s\n",
            "Epoch: 0816 loss_train: 0.6803 acc_train: 0.8400 loss_val: 1.2300 acc_val: 0.7600 time: 3.4294s\n",
            "Epoch: 0817 loss_train: 0.7237 acc_train: 0.8200 loss_val: 1.2338 acc_val: 0.7600 time: 3.4332s\n",
            "Epoch: 0818 loss_train: 0.7219 acc_train: 0.8200 loss_val: 1.2415 acc_val: 0.7600 time: 3.4370s\n",
            "Epoch: 0819 loss_train: 0.6828 acc_train: 0.8400 loss_val: 1.2536 acc_val: 0.7400 time: 3.4419s\n",
            "Epoch: 0820 loss_train: 0.7161 acc_train: 0.8200 loss_val: 1.2693 acc_val: 0.7400 time: 3.4461s\n",
            "Epoch: 0821 loss_train: 0.6597 acc_train: 0.8400 loss_val: 1.2866 acc_val: 0.7400 time: 3.4537s\n",
            "Epoch: 0822 loss_train: 0.7666 acc_train: 0.8000 loss_val: 1.3037 acc_val: 0.7400 time: 3.4577s\n",
            "Epoch: 0823 loss_train: 0.6519 acc_train: 0.8400 loss_val: 1.3213 acc_val: 0.7400 time: 3.4617s\n",
            "Epoch: 0824 loss_train: 0.7350 acc_train: 0.8000 loss_val: 1.3115 acc_val: 0.7400 time: 3.4655s\n",
            "Epoch: 0825 loss_train: 0.6571 acc_train: 0.8400 loss_val: 1.3041 acc_val: 0.7400 time: 3.4694s\n",
            "Epoch: 0826 loss_train: 0.7471 acc_train: 0.8200 loss_val: 1.2991 acc_val: 0.7400 time: 3.4733s\n",
            "Epoch: 0827 loss_train: 0.7712 acc_train: 0.8200 loss_val: 1.2942 acc_val: 0.7400 time: 3.4772s\n",
            "Epoch: 0828 loss_train: 0.6761 acc_train: 0.8400 loss_val: 1.2432 acc_val: 0.7600 time: 3.4811s\n",
            "Epoch: 0829 loss_train: 0.7320 acc_train: 0.8200 loss_val: 1.2165 acc_val: 0.7800 time: 3.4902s\n",
            "Epoch: 0830 loss_train: 0.6936 acc_train: 0.8400 loss_val: 1.2042 acc_val: 0.7800 time: 3.4976s\n",
            "Epoch: 0831 loss_train: 0.6831 acc_train: 0.8200 loss_val: 1.1999 acc_val: 0.7800 time: 3.5014s\n",
            "Epoch: 0832 loss_train: 0.7602 acc_train: 0.8000 loss_val: 1.1979 acc_val: 0.7800 time: 3.5051s\n",
            "Epoch: 0833 loss_train: 0.6799 acc_train: 0.8200 loss_val: 1.1965 acc_val: 0.7800 time: 3.5089s\n",
            "Epoch: 0834 loss_train: 0.6703 acc_train: 0.8400 loss_val: 1.1955 acc_val: 0.7800 time: 3.5127s\n",
            "Epoch: 0835 loss_train: 0.7385 acc_train: 0.8200 loss_val: 1.1947 acc_val: 0.7800 time: 3.5163s\n",
            "Epoch: 0836 loss_train: 0.6920 acc_train: 0.8400 loss_val: 1.1941 acc_val: 0.7800 time: 3.5205s\n",
            "Epoch: 0837 loss_train: 0.7300 acc_train: 0.8000 loss_val: 1.1943 acc_val: 0.7800 time: 3.5244s\n",
            "Epoch: 0838 loss_train: 0.6793 acc_train: 0.8400 loss_val: 1.1951 acc_val: 0.7800 time: 3.5283s\n",
            "Epoch: 0839 loss_train: 0.6795 acc_train: 0.8400 loss_val: 1.1967 acc_val: 0.7800 time: 3.5328s\n",
            "Epoch: 0840 loss_train: 0.6793 acc_train: 0.8400 loss_val: 1.1994 acc_val: 0.7800 time: 3.5340s\n",
            "Epoch: 0841 loss_train: 0.8401 acc_train: 0.7800 loss_val: 1.1977 acc_val: 0.7800 time: 3.5351s\n",
            "Epoch: 0842 loss_train: 0.7563 acc_train: 0.8000 loss_val: 1.1975 acc_val: 0.7800 time: 3.5362s\n",
            "Epoch: 0843 loss_train: 0.7141 acc_train: 0.8000 loss_val: 1.1987 acc_val: 0.7800 time: 3.5493s\n",
            "Epoch: 0844 loss_train: 0.6975 acc_train: 0.8400 loss_val: 1.2008 acc_val: 0.7800 time: 3.5596s\n",
            "Epoch: 0845 loss_train: 0.7112 acc_train: 0.8000 loss_val: 1.2036 acc_val: 0.7800 time: 3.5665s\n",
            "Epoch: 0846 loss_train: 0.7166 acc_train: 0.8200 loss_val: 1.2080 acc_val: 0.7800 time: 3.5707s\n",
            "Epoch: 0847 loss_train: 0.7261 acc_train: 0.8200 loss_val: 1.2143 acc_val: 0.7800 time: 3.5747s\n",
            "Epoch: 0848 loss_train: 0.7012 acc_train: 0.8200 loss_val: 1.2220 acc_val: 0.7800 time: 3.5787s\n",
            "Epoch: 0849 loss_train: 0.6450 acc_train: 0.8400 loss_val: 1.2329 acc_val: 0.7600 time: 3.5831s\n",
            "Epoch: 0850 loss_train: 0.7486 acc_train: 0.8000 loss_val: 1.2462 acc_val: 0.7400 time: 3.5902s\n",
            "Epoch: 0851 loss_train: 0.7129 acc_train: 0.8200 loss_val: 1.2621 acc_val: 0.7400 time: 3.5993s\n",
            "Epoch: 0852 loss_train: 0.6493 acc_train: 0.8400 loss_val: 1.2774 acc_val: 0.7400 time: 3.6035s\n",
            "Epoch: 0853 loss_train: 0.6707 acc_train: 0.8400 loss_val: 1.2992 acc_val: 0.7400 time: 3.6075s\n",
            "Epoch: 0854 loss_train: 0.6441 acc_train: 0.8400 loss_val: 1.3169 acc_val: 0.7400 time: 3.6115s\n",
            "Epoch: 0855 loss_train: 0.6755 acc_train: 0.8400 loss_val: 1.3321 acc_val: 0.7200 time: 3.6156s\n",
            "Epoch: 0856 loss_train: 0.7184 acc_train: 0.8200 loss_val: 1.3486 acc_val: 0.7200 time: 3.6195s\n",
            "Epoch: 0857 loss_train: 0.6959 acc_train: 0.8200 loss_val: 1.3631 acc_val: 0.7200 time: 3.6236s\n",
            "Epoch: 0858 loss_train: 0.7489 acc_train: 0.8000 loss_val: 1.3520 acc_val: 0.7200 time: 3.6287s\n",
            "Epoch: 0859 loss_train: 0.6653 acc_train: 0.8400 loss_val: 1.3331 acc_val: 0.7200 time: 3.6362s\n",
            "Epoch: 0860 loss_train: 0.6833 acc_train: 0.8400 loss_val: 1.3189 acc_val: 0.7200 time: 3.6406s\n",
            "Epoch: 0861 loss_train: 0.6678 acc_train: 0.8200 loss_val: 1.3071 acc_val: 0.7400 time: 3.6446s\n",
            "Epoch: 0862 loss_train: 0.6909 acc_train: 0.8400 loss_val: 1.3001 acc_val: 0.7400 time: 3.6486s\n",
            "Epoch: 0863 loss_train: 0.6380 acc_train: 0.8400 loss_val: 1.2927 acc_val: 0.7400 time: 3.6527s\n",
            "Epoch: 0864 loss_train: 0.7600 acc_train: 0.8200 loss_val: 1.2874 acc_val: 0.7400 time: 3.6565s\n",
            "Epoch: 0865 loss_train: 0.6479 acc_train: 0.8200 loss_val: 1.2839 acc_val: 0.7400 time: 3.6604s\n",
            "Epoch: 0866 loss_train: 0.7001 acc_train: 0.8400 loss_val: 1.2806 acc_val: 0.7400 time: 3.6689s\n",
            "Epoch: 0867 loss_train: 0.6714 acc_train: 0.8200 loss_val: 1.2729 acc_val: 0.7400 time: 3.6776s\n",
            "Epoch: 0868 loss_train: 0.6855 acc_train: 0.8200 loss_val: 1.2671 acc_val: 0.7400 time: 3.6819s\n",
            "Epoch: 0869 loss_train: 0.6735 acc_train: 0.8400 loss_val: 1.2639 acc_val: 0.7400 time: 3.6863s\n",
            "Epoch: 0870 loss_train: 0.7039 acc_train: 0.8400 loss_val: 1.2582 acc_val: 0.7400 time: 3.6906s\n",
            "Epoch: 0871 loss_train: 0.6463 acc_train: 0.8600 loss_val: 1.2491 acc_val: 0.7400 time: 3.6949s\n",
            "Epoch: 0872 loss_train: 0.6559 acc_train: 0.8400 loss_val: 1.2392 acc_val: 0.7400 time: 3.6990s\n",
            "Epoch: 0873 loss_train: 0.6550 acc_train: 0.8400 loss_val: 1.2340 acc_val: 0.7600 time: 3.7032s\n",
            "Epoch: 0874 loss_train: 0.6903 acc_train: 0.8200 loss_val: 1.2336 acc_val: 0.7600 time: 3.7074s\n",
            "Epoch: 0875 loss_train: 0.7309 acc_train: 0.8200 loss_val: 1.2385 acc_val: 0.7600 time: 3.7116s\n",
            "Epoch: 0876 loss_train: 0.6876 acc_train: 0.8400 loss_val: 1.2370 acc_val: 0.7600 time: 3.7158s\n",
            "Epoch: 0877 loss_train: 0.6385 acc_train: 0.8600 loss_val: 1.2353 acc_val: 0.7600 time: 3.7200s\n",
            "Epoch: 0878 loss_train: 0.7051 acc_train: 0.8400 loss_val: 1.2373 acc_val: 0.7600 time: 3.7242s\n",
            "Epoch: 0879 loss_train: 0.7390 acc_train: 0.8200 loss_val: 1.2412 acc_val: 0.7400 time: 3.7283s\n",
            "Epoch: 0880 loss_train: 0.6603 acc_train: 0.8400 loss_val: 1.2462 acc_val: 0.7400 time: 3.7325s\n",
            "Epoch: 0881 loss_train: 0.7093 acc_train: 0.8200 loss_val: 1.2503 acc_val: 0.7400 time: 3.7367s\n",
            "Epoch: 0882 loss_train: 0.7566 acc_train: 0.8200 loss_val: 1.2551 acc_val: 0.7400 time: 3.7407s\n",
            "Epoch: 0883 loss_train: 0.7052 acc_train: 0.8000 loss_val: 1.2567 acc_val: 0.7400 time: 3.7446s\n",
            "Epoch: 0884 loss_train: 0.6737 acc_train: 0.8400 loss_val: 1.2585 acc_val: 0.7400 time: 3.7487s\n",
            "Epoch: 0885 loss_train: 0.6757 acc_train: 0.8200 loss_val: 1.2629 acc_val: 0.7400 time: 3.7529s\n",
            "Epoch: 0886 loss_train: 0.6974 acc_train: 0.8200 loss_val: 1.2708 acc_val: 0.7400 time: 3.7582s\n",
            "Epoch: 0887 loss_train: 0.6735 acc_train: 0.8400 loss_val: 1.2697 acc_val: 0.7400 time: 3.7708s\n",
            "Epoch: 0888 loss_train: 0.6704 acc_train: 0.8400 loss_val: 1.2632 acc_val: 0.7400 time: 3.7753s\n",
            "Epoch: 0889 loss_train: 0.7946 acc_train: 0.8200 loss_val: 1.2659 acc_val: 0.7400 time: 3.7794s\n",
            "Epoch: 0890 loss_train: 0.7605 acc_train: 0.8200 loss_val: 1.2580 acc_val: 0.7400 time: 3.7835s\n",
            "Epoch: 0891 loss_train: 0.6968 acc_train: 0.8200 loss_val: 1.2510 acc_val: 0.7400 time: 3.7875s\n",
            "Epoch: 0892 loss_train: 0.6945 acc_train: 0.8200 loss_val: 1.2473 acc_val: 0.7400 time: 3.7913s\n",
            "Epoch: 0893 loss_train: 0.6862 acc_train: 0.8200 loss_val: 1.2481 acc_val: 0.7400 time: 3.7955s\n",
            "Epoch: 0894 loss_train: 0.6618 acc_train: 0.8200 loss_val: 1.2488 acc_val: 0.7400 time: 3.8008s\n",
            "Epoch: 0895 loss_train: 0.6546 acc_train: 0.8400 loss_val: 1.2521 acc_val: 0.7400 time: 3.8047s\n",
            "Epoch: 0896 loss_train: 0.7814 acc_train: 0.8000 loss_val: 1.2542 acc_val: 0.7400 time: 3.8087s\n",
            "Epoch: 0897 loss_train: 0.6897 acc_train: 0.8400 loss_val: 1.2558 acc_val: 0.7400 time: 3.8127s\n",
            "Epoch: 0898 loss_train: 0.6850 acc_train: 0.8200 loss_val: 1.2564 acc_val: 0.7400 time: 3.8168s\n",
            "Epoch: 0899 loss_train: 0.6825 acc_train: 0.8400 loss_val: 1.2541 acc_val: 0.7400 time: 3.8206s\n",
            "Epoch: 0900 loss_train: 0.7473 acc_train: 0.8000 loss_val: 1.2538 acc_val: 0.7400 time: 3.8245s\n",
            "Epoch: 0901 loss_train: 0.6942 acc_train: 0.8400 loss_val: 1.2568 acc_val: 0.7400 time: 3.8282s\n",
            "Epoch: 0902 loss_train: 0.7457 acc_train: 0.8200 loss_val: 1.2631 acc_val: 0.7400 time: 3.8319s\n",
            "Epoch: 0903 loss_train: 0.6742 acc_train: 0.8200 loss_val: 1.2717 acc_val: 0.7400 time: 3.8357s\n",
            "Epoch: 0904 loss_train: 0.7624 acc_train: 0.8200 loss_val: 1.2805 acc_val: 0.7400 time: 3.8396s\n",
            "Epoch: 0905 loss_train: 0.6639 acc_train: 0.8600 loss_val: 1.2906 acc_val: 0.7400 time: 3.8438s\n",
            "Epoch: 0906 loss_train: 0.7017 acc_train: 0.8200 loss_val: 1.2973 acc_val: 0.7400 time: 3.8475s\n",
            "Epoch: 0907 loss_train: 0.6515 acc_train: 0.8400 loss_val: 1.2952 acc_val: 0.7400 time: 3.8523s\n",
            "Epoch: 0908 loss_train: 0.6645 acc_train: 0.8400 loss_val: 1.2953 acc_val: 0.7400 time: 3.8569s\n",
            "Epoch: 0909 loss_train: 0.7801 acc_train: 0.8000 loss_val: 1.2970 acc_val: 0.7400 time: 3.8609s\n",
            "Epoch: 0910 loss_train: 0.6996 acc_train: 0.8000 loss_val: 1.2799 acc_val: 0.7400 time: 3.8657s\n",
            "Epoch: 0911 loss_train: 0.6665 acc_train: 0.8400 loss_val: 1.2686 acc_val: 0.7400 time: 3.8711s\n",
            "Epoch: 0912 loss_train: 0.7159 acc_train: 0.8200 loss_val: 1.2615 acc_val: 0.7400 time: 3.8754s\n",
            "Epoch: 0913 loss_train: 0.6599 acc_train: 0.8400 loss_val: 1.2576 acc_val: 0.7400 time: 3.8794s\n",
            "Epoch: 0914 loss_train: 0.6806 acc_train: 0.8200 loss_val: 1.2595 acc_val: 0.7400 time: 3.8832s\n",
            "Epoch: 0915 loss_train: 0.7328 acc_train: 0.8000 loss_val: 1.2700 acc_val: 0.7400 time: 3.8870s\n",
            "Epoch: 0916 loss_train: 0.8185 acc_train: 0.7800 loss_val: 1.2876 acc_val: 0.7400 time: 3.8910s\n",
            "Epoch: 0917 loss_train: 0.7696 acc_train: 0.8200 loss_val: 1.3096 acc_val: 0.7400 time: 3.8945s\n",
            "Epoch: 0918 loss_train: 0.7520 acc_train: 0.8000 loss_val: 1.2778 acc_val: 0.7400 time: 3.9019s\n",
            "Epoch: 0919 loss_train: 0.7284 acc_train: 0.8200 loss_val: 1.2539 acc_val: 0.7400 time: 3.9057s\n",
            "Epoch: 0920 loss_train: 0.7102 acc_train: 0.8200 loss_val: 1.2384 acc_val: 0.7400 time: 3.9093s\n",
            "Epoch: 0921 loss_train: 0.6903 acc_train: 0.8400 loss_val: 1.2279 acc_val: 0.7600 time: 3.9130s\n",
            "Epoch: 0922 loss_train: 0.6852 acc_train: 0.8200 loss_val: 1.2232 acc_val: 0.7800 time: 3.9168s\n",
            "Epoch: 0923 loss_train: 0.7749 acc_train: 0.8200 loss_val: 1.2208 acc_val: 0.7800 time: 3.9209s\n",
            "Epoch: 0924 loss_train: 0.6920 acc_train: 0.8400 loss_val: 1.2211 acc_val: 0.7800 time: 3.9247s\n",
            "Epoch: 0925 loss_train: 0.6408 acc_train: 0.8400 loss_val: 1.2240 acc_val: 0.7800 time: 3.9284s\n",
            "Epoch: 0926 loss_train: 0.7475 acc_train: 0.8000 loss_val: 1.2264 acc_val: 0.7800 time: 3.9324s\n",
            "Epoch: 0927 loss_train: 0.6614 acc_train: 0.8400 loss_val: 1.2287 acc_val: 0.7800 time: 3.9362s\n",
            "Epoch: 0928 loss_train: 0.6582 acc_train: 0.8200 loss_val: 1.2324 acc_val: 0.7600 time: 3.9404s\n",
            "Epoch: 0929 loss_train: 0.6816 acc_train: 0.8200 loss_val: 1.2399 acc_val: 0.7400 time: 3.9442s\n",
            "Epoch: 0930 loss_train: 0.6717 acc_train: 0.8400 loss_val: 1.2474 acc_val: 0.7400 time: 3.9480s\n",
            "Epoch: 0931 loss_train: 0.6672 acc_train: 0.8200 loss_val: 1.2638 acc_val: 0.7400 time: 3.9517s\n",
            "Epoch: 0932 loss_train: 0.6898 acc_train: 0.8400 loss_val: 1.2832 acc_val: 0.7400 time: 3.9555s\n",
            "Epoch: 0933 loss_train: 0.6885 acc_train: 0.8400 loss_val: 1.2991 acc_val: 0.7400 time: 3.9611s\n",
            "Epoch: 0934 loss_train: 0.6737 acc_train: 0.8200 loss_val: 1.2957 acc_val: 0.7400 time: 3.9628s\n",
            "Epoch: 0935 loss_train: 0.6838 acc_train: 0.8600 loss_val: 1.2929 acc_val: 0.7400 time: 3.9648s\n",
            "Epoch: 0936 loss_train: 0.7274 acc_train: 0.8000 loss_val: 1.2586 acc_val: 0.7400 time: 3.9665s\n",
            "Epoch: 0937 loss_train: 0.6735 acc_train: 0.8400 loss_val: 1.2370 acc_val: 0.7400 time: 3.9705s\n",
            "Epoch: 0938 loss_train: 0.7024 acc_train: 0.8200 loss_val: 1.2397 acc_val: 0.7400 time: 3.9778s\n",
            "Epoch: 0939 loss_train: 0.6892 acc_train: 0.8400 loss_val: 1.2506 acc_val: 0.7400 time: 3.9794s\n",
            "Epoch: 0940 loss_train: 0.7278 acc_train: 0.8200 loss_val: 1.2668 acc_val: 0.7400 time: 3.9806s\n",
            "Epoch: 0941 loss_train: 0.7182 acc_train: 0.8200 loss_val: 1.2738 acc_val: 0.7400 time: 3.9969s\n",
            "Epoch: 0942 loss_train: 0.6614 acc_train: 0.8200 loss_val: 1.2779 acc_val: 0.7400 time: 4.0008s\n",
            "Epoch: 0943 loss_train: 0.7139 acc_train: 0.8200 loss_val: 1.2903 acc_val: 0.7400 time: 4.0048s\n",
            "Epoch: 0944 loss_train: 0.6828 acc_train: 0.8400 loss_val: 1.3123 acc_val: 0.7400 time: 4.0086s\n",
            "Epoch: 0945 loss_train: 0.6694 acc_train: 0.8200 loss_val: 1.3293 acc_val: 0.7200 time: 4.0123s\n",
            "Epoch: 0946 loss_train: 0.7170 acc_train: 0.8200 loss_val: 1.3478 acc_val: 0.7200 time: 4.0160s\n",
            "Epoch: 0947 loss_train: 0.7065 acc_train: 0.8400 loss_val: 1.3609 acc_val: 0.7200 time: 4.0198s\n",
            "Epoch: 0948 loss_train: 0.6855 acc_train: 0.8400 loss_val: 1.3799 acc_val: 0.7200 time: 4.0235s\n",
            "Epoch: 0949 loss_train: 0.6979 acc_train: 0.8200 loss_val: 1.3788 acc_val: 0.7200 time: 4.0275s\n",
            "Epoch: 0950 loss_train: 0.6811 acc_train: 0.8400 loss_val: 1.3728 acc_val: 0.7200 time: 4.0312s\n",
            "Epoch: 0951 loss_train: 0.7062 acc_train: 0.8400 loss_val: 1.3641 acc_val: 0.7200 time: 4.0351s\n",
            "Epoch: 0952 loss_train: 0.6498 acc_train: 0.8200 loss_val: 1.3545 acc_val: 0.7200 time: 4.0390s\n",
            "Epoch: 0953 loss_train: 0.7166 acc_train: 0.8200 loss_val: 1.3141 acc_val: 0.7400 time: 4.0428s\n",
            "Epoch: 0954 loss_train: 0.7556 acc_train: 0.8400 loss_val: 1.2815 acc_val: 0.7400 time: 4.0467s\n",
            "Epoch: 0955 loss_train: 0.8643 acc_train: 0.8000 loss_val: 1.2585 acc_val: 0.7400 time: 4.0505s\n",
            "Epoch: 0956 loss_train: 0.6604 acc_train: 0.8600 loss_val: 1.2413 acc_val: 0.7400 time: 4.0542s\n",
            "Epoch: 0957 loss_train: 0.6868 acc_train: 0.8200 loss_val: 1.2272 acc_val: 0.7800 time: 4.0587s\n",
            "Epoch: 0958 loss_train: 0.7155 acc_train: 0.8200 loss_val: 1.2177 acc_val: 0.7800 time: 4.0626s\n",
            "Epoch: 0959 loss_train: 0.6830 acc_train: 0.8400 loss_val: 1.2116 acc_val: 0.7800 time: 4.0663s\n",
            "Epoch: 0960 loss_train: 0.7226 acc_train: 0.8000 loss_val: 1.2085 acc_val: 0.7800 time: 4.0699s\n",
            "Epoch: 0961 loss_train: 0.6975 acc_train: 0.8200 loss_val: 1.2068 acc_val: 0.7800 time: 4.0737s\n",
            "Epoch: 0962 loss_train: 0.7531 acc_train: 0.8000 loss_val: 1.2065 acc_val: 0.7800 time: 4.0776s\n",
            "Epoch: 0963 loss_train: 0.7237 acc_train: 0.8200 loss_val: 1.2069 acc_val: 0.7800 time: 4.0815s\n",
            "Epoch: 0964 loss_train: 0.7038 acc_train: 0.8200 loss_val: 1.2086 acc_val: 0.7800 time: 4.0852s\n",
            "Epoch: 0965 loss_train: 0.7149 acc_train: 0.8400 loss_val: 1.2116 acc_val: 0.7800 time: 4.0891s\n",
            "Epoch: 0966 loss_train: 0.7381 acc_train: 0.8200 loss_val: 1.2165 acc_val: 0.7800 time: 4.0929s\n",
            "Epoch: 0967 loss_train: 0.7414 acc_train: 0.8200 loss_val: 1.2214 acc_val: 0.7800 time: 4.0969s\n",
            "Epoch: 0968 loss_train: 0.6956 acc_train: 0.8400 loss_val: 1.2250 acc_val: 0.7800 time: 4.1007s\n",
            "Epoch: 0969 loss_train: 0.6598 acc_train: 0.8400 loss_val: 1.2291 acc_val: 0.7800 time: 4.1045s\n",
            "Epoch: 0970 loss_train: 0.6605 acc_train: 0.8400 loss_val: 1.2346 acc_val: 0.7400 time: 4.1083s\n",
            "Epoch: 0971 loss_train: 0.6927 acc_train: 0.8200 loss_val: 1.2422 acc_val: 0.7400 time: 4.1122s\n",
            "Epoch: 0972 loss_train: 0.6770 acc_train: 0.8400 loss_val: 1.2535 acc_val: 0.7400 time: 4.1160s\n",
            "Epoch: 0973 loss_train: 0.6664 acc_train: 0.8200 loss_val: 1.2683 acc_val: 0.7400 time: 4.1197s\n",
            "Epoch: 0974 loss_train: 0.7142 acc_train: 0.8200 loss_val: 1.2858 acc_val: 0.7400 time: 4.1240s\n",
            "Epoch: 0975 loss_train: 0.6574 acc_train: 0.8200 loss_val: 1.3019 acc_val: 0.7400 time: 4.1279s\n",
            "Epoch: 0976 loss_train: 0.6914 acc_train: 0.8400 loss_val: 1.3073 acc_val: 0.7400 time: 4.1316s\n",
            "Epoch: 0977 loss_train: 0.8171 acc_train: 0.8000 loss_val: 1.3153 acc_val: 0.7400 time: 4.1354s\n",
            "Epoch: 0978 loss_train: 0.7148 acc_train: 0.8200 loss_val: 1.3244 acc_val: 0.7400 time: 4.1392s\n",
            "Epoch: 0979 loss_train: 0.7054 acc_train: 0.8200 loss_val: 1.3298 acc_val: 0.7400 time: 4.1429s\n",
            "Epoch: 0980 loss_train: 0.7387 acc_train: 0.8200 loss_val: 1.3364 acc_val: 0.7400 time: 4.1465s\n",
            "Epoch: 0981 loss_train: 0.7178 acc_train: 0.8200 loss_val: 1.3436 acc_val: 0.7400 time: 4.1502s\n",
            "Epoch: 0982 loss_train: 0.7041 acc_train: 0.8200 loss_val: 1.3495 acc_val: 0.7400 time: 4.1541s\n",
            "Epoch: 0983 loss_train: 0.7338 acc_train: 0.8200 loss_val: 1.3525 acc_val: 0.7400 time: 4.1578s\n",
            "Epoch: 0984 loss_train: 0.7049 acc_train: 0.8200 loss_val: 1.3551 acc_val: 0.7400 time: 4.1616s\n",
            "Epoch: 0985 loss_train: 0.6834 acc_train: 0.8400 loss_val: 1.3526 acc_val: 0.7400 time: 4.1656s\n",
            "Epoch: 0986 loss_train: 0.6961 acc_train: 0.8400 loss_val: 1.3563 acc_val: 0.7200 time: 4.1694s\n",
            "Epoch: 0987 loss_train: 0.6876 acc_train: 0.8200 loss_val: 1.3529 acc_val: 0.7200 time: 4.1731s\n",
            "Epoch: 0988 loss_train: 0.6919 acc_train: 0.8200 loss_val: 1.3480 acc_val: 0.7200 time: 4.1761s\n",
            "Epoch: 0989 loss_train: 0.6719 acc_train: 0.8200 loss_val: 1.3496 acc_val: 0.7200 time: 4.1824s\n",
            "Epoch: 0990 loss_train: 0.6913 acc_train: 0.8200 loss_val: 1.3549 acc_val: 0.7200 time: 4.1873s\n",
            "Epoch: 0991 loss_train: 0.6596 acc_train: 0.8400 loss_val: 1.3681 acc_val: 0.7200 time: 4.1911s\n",
            "Epoch: 0992 loss_train: 0.6980 acc_train: 0.8200 loss_val: 1.3818 acc_val: 0.7200 time: 4.1969s\n",
            "Epoch: 0993 loss_train: 0.6658 acc_train: 0.8200 loss_val: 1.3551 acc_val: 0.7400 time: 4.2005s\n",
            "Epoch: 0994 loss_train: 0.7302 acc_train: 0.8200 loss_val: 1.3325 acc_val: 0.7400 time: 4.2042s\n",
            "Epoch: 0995 loss_train: 0.6957 acc_train: 0.8400 loss_val: 1.3089 acc_val: 0.7400 time: 4.2080s\n",
            "Epoch: 0996 loss_train: 0.6981 acc_train: 0.8200 loss_val: 1.2829 acc_val: 0.7400 time: 4.2118s\n",
            "Epoch: 0997 loss_train: 0.6484 acc_train: 0.8400 loss_val: 1.2649 acc_val: 0.7400 time: 4.2158s\n",
            "Epoch: 0998 loss_train: 0.6917 acc_train: 0.8400 loss_val: 1.2518 acc_val: 0.7400 time: 4.2196s\n",
            "Epoch: 0999 loss_train: 0.7017 acc_train: 0.8200 loss_val: 1.2441 acc_val: 0.7400 time: 4.2233s\n",
            "Epoch: 1000 loss_train: 0.7551 acc_train: 0.7800 loss_val: 1.2413 acc_val: 0.7400 time: 4.2270s\n",
            "Epoch: 1001 loss_train: 0.6740 acc_train: 0.8400 loss_val: 1.2401 acc_val: 0.7400 time: 4.2308s\n",
            "Epoch: 1002 loss_train: 0.7067 acc_train: 0.8000 loss_val: 1.2424 acc_val: 0.7400 time: 4.2346s\n",
            "Epoch: 1003 loss_train: 0.7182 acc_train: 0.8200 loss_val: 1.2483 acc_val: 0.7400 time: 4.2447s\n",
            "Epoch: 1004 loss_train: 0.7044 acc_train: 0.8200 loss_val: 1.2591 acc_val: 0.7400 time: 4.2487s\n",
            "Epoch: 1005 loss_train: 0.6655 acc_train: 0.8400 loss_val: 1.2738 acc_val: 0.7400 time: 4.2527s\n",
            "Epoch: 1006 loss_train: 0.6976 acc_train: 0.8200 loss_val: 1.2924 acc_val: 0.7400 time: 4.2564s\n",
            "Epoch: 1007 loss_train: 0.6676 acc_train: 0.8400 loss_val: 1.3156 acc_val: 0.7400 time: 4.2602s\n",
            "Epoch: 1008 loss_train: 0.6957 acc_train: 0.8400 loss_val: 1.3419 acc_val: 0.7400 time: 4.2640s\n",
            "Epoch: 1009 loss_train: 0.7663 acc_train: 0.8200 loss_val: 1.3644 acc_val: 0.7400 time: 4.2678s\n",
            "Epoch: 1010 loss_train: 0.6447 acc_train: 0.8400 loss_val: 1.3825 acc_val: 0.7400 time: 4.2722s\n",
            "Epoch: 1011 loss_train: 0.6885 acc_train: 0.8400 loss_val: 1.4044 acc_val: 0.7200 time: 4.2769s\n",
            "Epoch: 1012 loss_train: 0.6523 acc_train: 0.8400 loss_val: 1.4226 acc_val: 0.7200 time: 4.2786s\n",
            "Epoch: 1013 loss_train: 0.6758 acc_train: 0.8400 loss_val: 1.4340 acc_val: 0.7200 time: 4.2852s\n",
            "Epoch: 1014 loss_train: 0.6859 acc_train: 0.8400 loss_val: 1.4391 acc_val: 0.7200 time: 4.2893s\n",
            "Epoch: 1015 loss_train: 0.6551 acc_train: 0.8400 loss_val: 1.4373 acc_val: 0.7200 time: 4.2933s\n",
            "Epoch: 1016 loss_train: 0.7214 acc_train: 0.8200 loss_val: 1.4453 acc_val: 0.7200 time: 4.2973s\n",
            "Epoch: 1017 loss_train: 0.7142 acc_train: 0.8200 loss_val: 1.4483 acc_val: 0.7200 time: 4.3016s\n",
            "Epoch: 1018 loss_train: 0.6697 acc_train: 0.8400 loss_val: 1.4183 acc_val: 0.7200 time: 4.3056s\n",
            "Epoch: 1019 loss_train: 0.6294 acc_train: 0.8600 loss_val: 1.3913 acc_val: 0.7200 time: 4.3097s\n",
            "Epoch: 1020 loss_train: 0.6601 acc_train: 0.8400 loss_val: 1.3648 acc_val: 0.7200 time: 4.3135s\n",
            "Epoch: 1021 loss_train: 0.6889 acc_train: 0.8400 loss_val: 1.3469 acc_val: 0.7200 time: 4.3176s\n",
            "Epoch: 1022 loss_train: 0.7090 acc_train: 0.8200 loss_val: 1.3362 acc_val: 0.7200 time: 4.3215s\n",
            "Epoch: 1023 loss_train: 0.7118 acc_train: 0.8200 loss_val: 1.3385 acc_val: 0.7200 time: 4.3257s\n",
            "Epoch: 1024 loss_train: 0.6962 acc_train: 0.8400 loss_val: 1.3463 acc_val: 0.7200 time: 4.3302s\n",
            "Epoch: 1025 loss_train: 0.6182 acc_train: 0.8600 loss_val: 1.3532 acc_val: 0.7200 time: 4.3342s\n",
            "Epoch: 1026 loss_train: 0.6745 acc_train: 0.8400 loss_val: 1.3158 acc_val: 0.7400 time: 4.3382s\n",
            "Epoch: 1027 loss_train: 0.6385 acc_train: 0.8400 loss_val: 1.2907 acc_val: 0.7400 time: 4.3422s\n",
            "Epoch: 1028 loss_train: 0.7655 acc_train: 0.8000 loss_val: 1.2738 acc_val: 0.7400 time: 4.3462s\n",
            "Epoch: 1029 loss_train: 0.6977 acc_train: 0.8200 loss_val: 1.2617 acc_val: 0.7400 time: 4.3502s\n",
            "Epoch: 1030 loss_train: 0.7360 acc_train: 0.8200 loss_val: 1.2401 acc_val: 0.7400 time: 4.3541s\n",
            "Epoch: 1031 loss_train: 0.6523 acc_train: 0.8400 loss_val: 1.2262 acc_val: 0.7600 time: 4.3580s\n",
            "Epoch: 1032 loss_train: 0.6598 acc_train: 0.8400 loss_val: 1.2192 acc_val: 0.7600 time: 4.3622s\n",
            "Epoch: 1033 loss_train: 0.6410 acc_train: 0.8400 loss_val: 1.2157 acc_val: 0.7600 time: 4.3662s\n",
            "Epoch: 1034 loss_train: 0.6809 acc_train: 0.8400 loss_val: 1.2149 acc_val: 0.7600 time: 4.3701s\n",
            "Epoch: 1035 loss_train: 0.6763 acc_train: 0.8400 loss_val: 1.2164 acc_val: 0.7600 time: 4.3742s\n",
            "Epoch: 1036 loss_train: 0.7004 acc_train: 0.8400 loss_val: 1.2194 acc_val: 0.7600 time: 4.3782s\n",
            "Epoch: 1037 loss_train: 0.6292 acc_train: 0.8400 loss_val: 1.2230 acc_val: 0.7600 time: 4.3987s\n",
            "Epoch: 1038 loss_train: 0.6707 acc_train: 0.8400 loss_val: 1.2268 acc_val: 0.7600 time: 4.4030s\n",
            "Epoch: 1039 loss_train: 0.6476 acc_train: 0.8400 loss_val: 1.2284 acc_val: 0.7600 time: 4.4069s\n",
            "Epoch: 1040 loss_train: 0.8161 acc_train: 0.8000 loss_val: 1.2062 acc_val: 0.7800 time: 4.4108s\n",
            "Epoch: 1041 loss_train: 0.6681 acc_train: 0.8400 loss_val: 1.1972 acc_val: 0.7800 time: 4.4149s\n",
            "Epoch: 1042 loss_train: 0.6796 acc_train: 0.8200 loss_val: 1.1937 acc_val: 0.7800 time: 4.4189s\n",
            "Epoch: 1043 loss_train: 0.6835 acc_train: 0.8400 loss_val: 1.1966 acc_val: 0.7800 time: 4.4227s\n",
            "Epoch: 1044 loss_train: 0.6759 acc_train: 0.8400 loss_val: 1.2020 acc_val: 0.7800 time: 4.4268s\n",
            "Epoch: 1045 loss_train: 0.6736 acc_train: 0.8400 loss_val: 1.2112 acc_val: 0.7800 time: 4.4309s\n",
            "Epoch: 1046 loss_train: 0.6740 acc_train: 0.8400 loss_val: 1.2253 acc_val: 0.7800 time: 4.4348s\n",
            "Epoch: 1047 loss_train: 0.6174 acc_train: 0.8400 loss_val: 1.2434 acc_val: 0.7600 time: 4.4393s\n",
            "Epoch: 1048 loss_train: 0.7308 acc_train: 0.8200 loss_val: 1.2635 acc_val: 0.7400 time: 4.4432s\n",
            "Epoch: 1049 loss_train: 0.6873 acc_train: 0.8400 loss_val: 1.2899 acc_val: 0.7400 time: 4.4471s\n",
            "Epoch: 1050 loss_train: 0.7206 acc_train: 0.8200 loss_val: 1.3202 acc_val: 0.7400 time: 4.4512s\n",
            "Epoch: 1051 loss_train: 0.6307 acc_train: 0.8400 loss_val: 1.3440 acc_val: 0.7400 time: 4.4552s\n",
            "Epoch: 1052 loss_train: 0.7265 acc_train: 0.8200 loss_val: 1.3625 acc_val: 0.7400 time: 4.4592s\n",
            "Epoch: 1053 loss_train: 0.6829 acc_train: 0.8200 loss_val: 1.3736 acc_val: 0.7400 time: 4.4632s\n",
            "Epoch: 1054 loss_train: 0.7520 acc_train: 0.8200 loss_val: 1.3785 acc_val: 0.7400 time: 4.4671s\n",
            "Epoch: 1055 loss_train: 0.6829 acc_train: 0.8200 loss_val: 1.3745 acc_val: 0.7400 time: 4.4710s\n",
            "Epoch: 1056 loss_train: 0.6554 acc_train: 0.8400 loss_val: 1.3707 acc_val: 0.7400 time: 4.4749s\n",
            "Epoch: 1057 loss_train: 0.6600 acc_train: 0.8400 loss_val: 1.3721 acc_val: 0.7200 time: 4.4789s\n",
            "Epoch: 1058 loss_train: 0.7964 acc_train: 0.8000 loss_val: 1.3793 acc_val: 0.7200 time: 4.4839s\n",
            "Epoch: 1059 loss_train: 0.6699 acc_train: 0.8200 loss_val: 1.3934 acc_val: 0.7200 time: 4.4895s\n",
            "Epoch: 1060 loss_train: 0.6388 acc_train: 0.8400 loss_val: 1.4035 acc_val: 0.7200 time: 4.4937s\n",
            "Epoch: 1061 loss_train: 0.6575 acc_train: 0.8200 loss_val: 1.3875 acc_val: 0.7200 time: 4.4979s\n",
            "Epoch: 1062 loss_train: 0.6798 acc_train: 0.8400 loss_val: 1.3612 acc_val: 0.7400 time: 4.5018s\n",
            "Epoch: 1063 loss_train: 0.6508 acc_train: 0.8600 loss_val: 1.3223 acc_val: 0.7400 time: 4.5059s\n",
            "Epoch: 1064 loss_train: 0.6736 acc_train: 0.8400 loss_val: 1.2846 acc_val: 0.7400 time: 4.5100s\n",
            "Epoch: 1065 loss_train: 0.6621 acc_train: 0.8400 loss_val: 1.2568 acc_val: 0.7400 time: 4.5139s\n",
            "Epoch: 1066 loss_train: 0.6955 acc_train: 0.8000 loss_val: 1.2409 acc_val: 0.7800 time: 4.5184s\n",
            "Epoch: 1067 loss_train: 0.7096 acc_train: 0.8400 loss_val: 1.2305 acc_val: 0.7800 time: 4.5225s\n",
            "Epoch: 1068 loss_train: 0.8087 acc_train: 0.8000 loss_val: 1.2312 acc_val: 0.7800 time: 4.5263s\n",
            "Epoch: 1069 loss_train: 0.7191 acc_train: 0.8400 loss_val: 1.2345 acc_val: 0.7800 time: 4.5302s\n",
            "Epoch: 1070 loss_train: 0.7188 acc_train: 0.8000 loss_val: 1.2327 acc_val: 0.7600 time: 4.5343s\n",
            "Epoch: 1071 loss_train: 0.7288 acc_train: 0.8200 loss_val: 1.2309 acc_val: 0.7600 time: 4.5355s\n",
            "Epoch: 1072 loss_train: 0.6508 acc_train: 0.8400 loss_val: 1.2318 acc_val: 0.7600 time: 4.5427s\n",
            "Epoch: 1073 loss_train: 0.8453 acc_train: 0.8000 loss_val: 1.2316 acc_val: 0.7600 time: 4.5465s\n",
            "Epoch: 1074 loss_train: 0.6910 acc_train: 0.8200 loss_val: 1.2304 acc_val: 0.7600 time: 4.5503s\n",
            "Epoch: 1075 loss_train: 0.7349 acc_train: 0.8000 loss_val: 1.2280 acc_val: 0.7600 time: 4.5539s\n",
            "Epoch: 1076 loss_train: 0.6984 acc_train: 0.8400 loss_val: 1.2234 acc_val: 0.7800 time: 4.5577s\n",
            "Epoch: 1077 loss_train: 0.6940 acc_train: 0.8400 loss_val: 1.2184 acc_val: 0.7800 time: 4.5614s\n",
            "Epoch: 1078 loss_train: 0.6780 acc_train: 0.8400 loss_val: 1.2164 acc_val: 0.7800 time: 4.5652s\n",
            "Epoch: 1079 loss_train: 0.6730 acc_train: 0.8400 loss_val: 1.2159 acc_val: 0.7800 time: 4.5690s\n",
            "Epoch: 1080 loss_train: 0.6692 acc_train: 0.8400 loss_val: 1.2165 acc_val: 0.7800 time: 4.5728s\n",
            "Epoch: 1081 loss_train: 0.6499 acc_train: 0.8400 loss_val: 1.2178 acc_val: 0.7800 time: 4.5766s\n",
            "Epoch: 1082 loss_train: 0.7058 acc_train: 0.8000 loss_val: 1.2231 acc_val: 0.7600 time: 4.5803s\n",
            "Epoch: 1083 loss_train: 0.6895 acc_train: 0.8400 loss_val: 1.2305 acc_val: 0.7400 time: 4.5841s\n",
            "Epoch: 1084 loss_train: 0.6063 acc_train: 0.8600 loss_val: 1.2394 acc_val: 0.7400 time: 4.5879s\n",
            "Epoch: 1085 loss_train: 0.6577 acc_train: 0.8200 loss_val: 1.2534 acc_val: 0.7400 time: 4.5918s\n",
            "Epoch: 1086 loss_train: 0.7236 acc_train: 0.8200 loss_val: 1.2658 acc_val: 0.7400 time: 4.5957s\n",
            "Epoch: 1087 loss_train: 0.6888 acc_train: 0.8200 loss_val: 1.2845 acc_val: 0.7400 time: 4.6033s\n",
            "Epoch: 1088 loss_train: 0.6830 acc_train: 0.8200 loss_val: 1.2957 acc_val: 0.7400 time: 4.6054s\n",
            "Epoch: 1089 loss_train: 0.6475 acc_train: 0.8200 loss_val: 1.3104 acc_val: 0.7400 time: 4.6074s\n",
            "Epoch: 1090 loss_train: 0.6924 acc_train: 0.8200 loss_val: 1.3237 acc_val: 0.7400 time: 4.6168s\n",
            "Epoch: 1091 loss_train: 0.7784 acc_train: 0.8200 loss_val: 1.3297 acc_val: 0.7400 time: 4.6206s\n",
            "Epoch: 1092 loss_train: 0.6613 acc_train: 0.8200 loss_val: 1.3280 acc_val: 0.7400 time: 4.6244s\n",
            "Epoch: 1093 loss_train: 0.6490 acc_train: 0.8400 loss_val: 1.3220 acc_val: 0.7400 time: 4.6282s\n",
            "Epoch: 1094 loss_train: 0.6560 acc_train: 0.8400 loss_val: 1.3196 acc_val: 0.7400 time: 4.6320s\n",
            "Epoch: 1095 loss_train: 0.6862 acc_train: 0.8200 loss_val: 1.3258 acc_val: 0.7400 time: 4.6357s\n",
            "Epoch: 1096 loss_train: 0.7036 acc_train: 0.8400 loss_val: 1.3400 acc_val: 0.7400 time: 4.6397s\n",
            "Epoch: 1097 loss_train: 0.6594 acc_train: 0.8600 loss_val: 1.3540 acc_val: 0.7400 time: 4.6435s\n",
            "Epoch: 1098 loss_train: 0.6543 acc_train: 0.8400 loss_val: 1.3656 acc_val: 0.7400 time: 4.6471s\n",
            "Epoch: 1099 loss_train: 0.6813 acc_train: 0.8400 loss_val: 1.3825 acc_val: 0.7400 time: 4.6510s\n",
            "Epoch: 1100 loss_train: 0.6275 acc_train: 0.8400 loss_val: 1.3965 acc_val: 0.7400 time: 4.6548s\n",
            "Epoch: 1101 loss_train: 0.7125 acc_train: 0.8200 loss_val: 1.3036 acc_val: 0.7400 time: 4.6585s\n",
            "Epoch: 1102 loss_train: 0.7434 acc_train: 0.8200 loss_val: 1.2358 acc_val: 0.7400 time: 4.6622s\n",
            "Epoch: 1103 loss_train: 0.6584 acc_train: 0.8400 loss_val: 1.1987 acc_val: 0.7800 time: 4.6660s\n",
            "Epoch: 1104 loss_train: 0.6774 acc_train: 0.8200 loss_val: 1.1875 acc_val: 0.7800 time: 4.6698s\n",
            "Epoch: 1105 loss_train: 0.6553 acc_train: 0.8400 loss_val: 1.1869 acc_val: 0.7800 time: 4.6736s\n",
            "Epoch: 1106 loss_train: 0.6872 acc_train: 0.8200 loss_val: 1.1888 acc_val: 0.7800 time: 4.6774s\n",
            "Epoch: 1107 loss_train: 0.6682 acc_train: 0.8400 loss_val: 1.1911 acc_val: 0.7800 time: 4.6812s\n",
            "Epoch: 1108 loss_train: 0.7082 acc_train: 0.8200 loss_val: 1.1934 acc_val: 0.7800 time: 4.6850s\n",
            "Epoch: 1109 loss_train: 0.6669 acc_train: 0.8200 loss_val: 1.1950 acc_val: 0.7800 time: 4.6888s\n",
            "Epoch: 1110 loss_train: 0.6585 acc_train: 0.8400 loss_val: 1.1982 acc_val: 0.7800 time: 4.6926s\n",
            "Epoch: 1111 loss_train: 0.6632 acc_train: 0.8200 loss_val: 1.2053 acc_val: 0.7800 time: 4.6967s\n",
            "Epoch: 1112 loss_train: 0.6740 acc_train: 0.8200 loss_val: 1.2171 acc_val: 0.7600 time: 4.7014s\n",
            "Epoch: 1113 loss_train: 0.7052 acc_train: 0.8200 loss_val: 1.2332 acc_val: 0.7400 time: 4.7057s\n",
            "Epoch: 1114 loss_train: 0.7398 acc_train: 0.8200 loss_val: 1.2379 acc_val: 0.7400 time: 4.7099s\n",
            "Epoch: 1115 loss_train: 0.7065 acc_train: 0.8200 loss_val: 1.2373 acc_val: 0.7400 time: 4.7138s\n",
            "Epoch: 1116 loss_train: 0.7163 acc_train: 0.8000 loss_val: 1.2352 acc_val: 0.7400 time: 4.7176s\n",
            "Epoch: 1117 loss_train: 0.6779 acc_train: 0.8400 loss_val: 1.2276 acc_val: 0.7400 time: 4.7214s\n",
            "Epoch: 1118 loss_train: 0.7087 acc_train: 0.8200 loss_val: 1.2178 acc_val: 0.7400 time: 4.7253s\n",
            "Epoch: 1119 loss_train: 0.7909 acc_train: 0.8000 loss_val: 1.2101 acc_val: 0.7800 time: 4.7291s\n",
            "Epoch: 1120 loss_train: 0.7501 acc_train: 0.8200 loss_val: 1.2054 acc_val: 0.7800 time: 4.7329s\n",
            "Epoch: 1121 loss_train: 0.7275 acc_train: 0.8200 loss_val: 1.2040 acc_val: 0.7800 time: 4.7372s\n",
            "Epoch: 1122 loss_train: 0.6716 acc_train: 0.8200 loss_val: 1.2081 acc_val: 0.7800 time: 4.7410s\n",
            "Epoch: 1123 loss_train: 0.6449 acc_train: 0.8400 loss_val: 1.2149 acc_val: 0.7600 time: 4.7448s\n",
            "Epoch: 1124 loss_train: 0.7716 acc_train: 0.8000 loss_val: 1.2285 acc_val: 0.7400 time: 4.7485s\n",
            "Epoch: 1125 loss_train: 0.6843 acc_train: 0.8000 loss_val: 1.2278 acc_val: 0.7400 time: 4.7522s\n",
            "Epoch: 1126 loss_train: 0.6485 acc_train: 0.8600 loss_val: 1.2205 acc_val: 0.7400 time: 4.7558s\n",
            "Epoch: 1127 loss_train: 0.7992 acc_train: 0.8000 loss_val: 1.1944 acc_val: 0.7800 time: 4.7596s\n",
            "Epoch: 1128 loss_train: 0.7699 acc_train: 0.8200 loss_val: 1.1859 acc_val: 0.7800 time: 4.7634s\n",
            "Epoch: 1129 loss_train: 0.8192 acc_train: 0.8000 loss_val: 1.1848 acc_val: 0.7800 time: 4.7671s\n",
            "Epoch: 1130 loss_train: 0.6572 acc_train: 0.8200 loss_val: 1.1854 acc_val: 0.7800 time: 4.7711s\n",
            "Epoch: 1131 loss_train: 0.6759 acc_train: 0.8400 loss_val: 1.1846 acc_val: 0.7800 time: 4.7750s\n",
            "Epoch: 1132 loss_train: 0.6810 acc_train: 0.8200 loss_val: 1.1844 acc_val: 0.7800 time: 4.7789s\n",
            "Epoch: 1133 loss_train: 0.7099 acc_train: 0.8200 loss_val: 1.1848 acc_val: 0.7800 time: 4.7825s\n",
            "Epoch: 1134 loss_train: 0.7285 acc_train: 0.8200 loss_val: 1.1856 acc_val: 0.7800 time: 4.7863s\n",
            "Epoch: 1135 loss_train: 0.7761 acc_train: 0.8000 loss_val: 1.1869 acc_val: 0.7800 time: 4.7900s\n",
            "Epoch: 1136 loss_train: 0.6664 acc_train: 0.8200 loss_val: 1.1899 acc_val: 0.7800 time: 4.7937s\n",
            "Epoch: 1137 loss_train: 0.7188 acc_train: 0.8200 loss_val: 1.1953 acc_val: 0.7800 time: 4.8008s\n",
            "Epoch: 1138 loss_train: 0.6855 acc_train: 0.8400 loss_val: 1.2046 acc_val: 0.7600 time: 4.8075s\n",
            "Epoch: 1139 loss_train: 0.6898 acc_train: 0.8200 loss_val: 1.2208 acc_val: 0.7600 time: 4.8145s\n",
            "Epoch: 1140 loss_train: 0.6864 acc_train: 0.8200 loss_val: 1.2463 acc_val: 0.7400 time: 4.8167s\n",
            "Epoch: 1141 loss_train: 0.7066 acc_train: 0.8200 loss_val: 1.2768 acc_val: 0.7400 time: 4.8259s\n",
            "Epoch: 1142 loss_train: 0.7078 acc_train: 0.8200 loss_val: 1.3073 acc_val: 0.7400 time: 4.8302s\n",
            "Epoch: 1143 loss_train: 0.7814 acc_train: 0.8000 loss_val: 1.3411 acc_val: 0.7400 time: 4.8343s\n",
            "Epoch: 1144 loss_train: 0.6722 acc_train: 0.8400 loss_val: 1.3672 acc_val: 0.7400 time: 4.8382s\n",
            "Epoch: 1145 loss_train: 0.6478 acc_train: 0.8400 loss_val: 1.3862 acc_val: 0.7200 time: 4.8420s\n",
            "Epoch: 1146 loss_train: 0.7659 acc_train: 0.8000 loss_val: 1.3531 acc_val: 0.7400 time: 4.8462s\n",
            "Epoch: 1147 loss_train: 0.6520 acc_train: 0.8600 loss_val: 1.3239 acc_val: 0.7400 time: 4.8501s\n",
            "Epoch: 1148 loss_train: 0.7186 acc_train: 0.8200 loss_val: 1.2971 acc_val: 0.7400 time: 4.8539s\n",
            "Epoch: 1149 loss_train: 0.7092 acc_train: 0.8200 loss_val: 1.2717 acc_val: 0.7400 time: 4.8577s\n",
            "Epoch: 1150 loss_train: 0.7581 acc_train: 0.8000 loss_val: 1.2530 acc_val: 0.7400 time: 4.8620s\n",
            "Epoch: 1151 loss_train: 0.7869 acc_train: 0.8000 loss_val: 1.2406 acc_val: 0.7400 time: 4.8662s\n",
            "Epoch: 1152 loss_train: 0.6763 acc_train: 0.8400 loss_val: 1.2305 acc_val: 0.7400 time: 4.8702s\n",
            "Epoch: 1153 loss_train: 0.6452 acc_train: 0.8400 loss_val: 1.2244 acc_val: 0.7600 time: 4.8743s\n",
            "Epoch: 1154 loss_train: 0.7604 acc_train: 0.8200 loss_val: 1.2227 acc_val: 0.7600 time: 4.8782s\n",
            "Epoch: 1155 loss_train: 0.6971 acc_train: 0.8400 loss_val: 1.2184 acc_val: 0.7600 time: 4.8822s\n",
            "Epoch: 1156 loss_train: 0.7868 acc_train: 0.8000 loss_val: 1.2189 acc_val: 0.7600 time: 4.8874s\n",
            "Epoch: 1157 loss_train: 0.7292 acc_train: 0.8200 loss_val: 1.2219 acc_val: 0.7400 time: 4.8914s\n",
            "Epoch: 1158 loss_train: 0.6649 acc_train: 0.8200 loss_val: 1.2253 acc_val: 0.7400 time: 4.8955s\n",
            "Epoch: 1159 loss_train: 0.7203 acc_train: 0.8400 loss_val: 1.2331 acc_val: 0.7400 time: 4.8994s\n",
            "Epoch: 1160 loss_train: 0.7145 acc_train: 0.8200 loss_val: 1.2446 acc_val: 0.7400 time: 4.9025s\n",
            "Epoch: 1161 loss_train: 0.8274 acc_train: 0.8000 loss_val: 1.2540 acc_val: 0.7400 time: 4.9064s\n",
            "Epoch: 1162 loss_train: 0.6862 acc_train: 0.8200 loss_val: 1.2607 acc_val: 0.7400 time: 4.9116s\n",
            "Epoch: 1163 loss_train: 0.6928 acc_train: 0.8200 loss_val: 1.2678 acc_val: 0.7400 time: 4.9184s\n",
            "Epoch: 1164 loss_train: 0.6698 acc_train: 0.8400 loss_val: 1.2723 acc_val: 0.7400 time: 4.9223s\n",
            "Epoch: 1165 loss_train: 0.7216 acc_train: 0.8200 loss_val: 1.2622 acc_val: 0.7400 time: 4.9261s\n",
            "Epoch: 1166 loss_train: 0.6722 acc_train: 0.8200 loss_val: 1.2498 acc_val: 0.7400 time: 4.9299s\n",
            "Epoch: 1167 loss_train: 0.7306 acc_train: 0.8400 loss_val: 1.2400 acc_val: 0.7600 time: 4.9337s\n",
            "Epoch: 1168 loss_train: 0.7927 acc_train: 0.8000 loss_val: 1.2387 acc_val: 0.7600 time: 4.9375s\n",
            "Epoch: 1169 loss_train: 0.6339 acc_train: 0.8400 loss_val: 1.2384 acc_val: 0.7600 time: 4.9413s\n",
            "Epoch: 1170 loss_train: 0.6888 acc_train: 0.8400 loss_val: 1.2415 acc_val: 0.7600 time: 4.9450s\n",
            "Epoch: 1171 loss_train: 0.6716 acc_train: 0.8200 loss_val: 1.2435 acc_val: 0.7600 time: 4.9488s\n",
            "Epoch: 1172 loss_train: 0.6209 acc_train: 0.8600 loss_val: 1.2454 acc_val: 0.7600 time: 4.9525s\n",
            "Epoch: 1173 loss_train: 0.6299 acc_train: 0.8600 loss_val: 1.2446 acc_val: 0.7600 time: 4.9563s\n",
            "Epoch: 1174 loss_train: 0.6612 acc_train: 0.8600 loss_val: 1.2436 acc_val: 0.7600 time: 4.9600s\n",
            "Epoch: 1175 loss_train: 0.6650 acc_train: 0.8400 loss_val: 1.2416 acc_val: 0.7600 time: 4.9637s\n",
            "Epoch: 1176 loss_train: 0.7053 acc_train: 0.8400 loss_val: 1.2386 acc_val: 0.7600 time: 4.9675s\n",
            "Epoch: 1177 loss_train: 0.7213 acc_train: 0.8200 loss_val: 1.2470 acc_val: 0.7600 time: 4.9712s\n",
            "Epoch: 1178 loss_train: 0.6569 acc_train: 0.8400 loss_val: 1.2555 acc_val: 0.7600 time: 4.9749s\n",
            "Epoch: 1179 loss_train: 0.6778 acc_train: 0.8400 loss_val: 1.2651 acc_val: 0.7400 time: 4.9811s\n",
            "Epoch: 1180 loss_train: 0.6511 acc_train: 0.8400 loss_val: 1.2717 acc_val: 0.7400 time: 4.9828s\n",
            "Epoch: 1181 loss_train: 0.6686 acc_train: 0.8400 loss_val: 1.2749 acc_val: 0.7400 time: 4.9846s\n",
            "Epoch: 1182 loss_train: 0.6677 acc_train: 0.8400 loss_val: 1.2771 acc_val: 0.7400 time: 4.9863s\n",
            "Epoch: 1183 loss_train: 0.7213 acc_train: 0.8400 loss_val: 1.2842 acc_val: 0.7400 time: 4.9973s\n",
            "Epoch: 1184 loss_train: 0.7111 acc_train: 0.8200 loss_val: 1.2901 acc_val: 0.7400 time: 5.0011s\n",
            "Epoch: 1185 loss_train: 0.6673 acc_train: 0.8200 loss_val: 1.2935 acc_val: 0.7400 time: 5.0049s\n",
            "Epoch: 1186 loss_train: 0.8103 acc_train: 0.8000 loss_val: 1.3005 acc_val: 0.7400 time: 5.0087s\n",
            "Epoch: 1187 loss_train: 0.6591 acc_train: 0.8400 loss_val: 1.2916 acc_val: 0.7400 time: 5.0125s\n",
            "Epoch: 1188 loss_train: 0.6719 acc_train: 0.8400 loss_val: 1.2838 acc_val: 0.7400 time: 5.0163s\n",
            "Epoch: 1189 loss_train: 0.6701 acc_train: 0.8400 loss_val: 1.2816 acc_val: 0.7400 time: 5.0202s\n",
            "Epoch: 1190 loss_train: 0.6488 acc_train: 0.8400 loss_val: 1.2288 acc_val: 0.7600 time: 5.0316s\n",
            "Epoch: 1191 loss_train: 0.6998 acc_train: 0.8400 loss_val: 1.2054 acc_val: 0.7800 time: 5.0364s\n",
            "Epoch: 1192 loss_train: 0.6389 acc_train: 0.8400 loss_val: 1.1959 acc_val: 0.7800 time: 5.0408s\n",
            "Epoch: 1193 loss_train: 0.6651 acc_train: 0.8200 loss_val: 1.1926 acc_val: 0.7800 time: 5.0450s\n",
            "Epoch: 1194 loss_train: 0.6698 acc_train: 0.8200 loss_val: 1.1914 acc_val: 0.7800 time: 5.0491s\n",
            "Epoch: 1195 loss_train: 0.6939 acc_train: 0.8200 loss_val: 1.1903 acc_val: 0.7800 time: 5.0531s\n",
            "Epoch: 1196 loss_train: 0.7258 acc_train: 0.8000 loss_val: 1.1899 acc_val: 0.7800 time: 5.0573s\n",
            "Epoch: 1197 loss_train: 0.6543 acc_train: 0.8400 loss_val: 1.1904 acc_val: 0.7800 time: 5.0616s\n",
            "Epoch: 1198 loss_train: 0.7019 acc_train: 0.8400 loss_val: 1.1917 acc_val: 0.7800 time: 5.0655s\n",
            "Epoch: 1199 loss_train: 0.6877 acc_train: 0.8400 loss_val: 1.1930 acc_val: 0.7800 time: 5.0694s\n",
            "Epoch: 1200 loss_train: 0.7051 acc_train: 0.8400 loss_val: 1.1943 acc_val: 0.7800 time: 5.0733s\n",
            "Epoch: 1201 loss_train: 0.6663 acc_train: 0.8400 loss_val: 1.1955 acc_val: 0.7800 time: 5.0773s\n",
            "Epoch: 1202 loss_train: 0.6738 acc_train: 0.8200 loss_val: 1.1962 acc_val: 0.7800 time: 5.0823s\n",
            "Epoch: 1203 loss_train: 0.7232 acc_train: 0.8200 loss_val: 1.1967 acc_val: 0.7800 time: 5.0874s\n",
            "Epoch: 1204 loss_train: 0.7401 acc_train: 0.8200 loss_val: 1.1969 acc_val: 0.7800 time: 5.0928s\n",
            "Epoch: 1205 loss_train: 0.7044 acc_train: 0.8200 loss_val: 1.1975 acc_val: 0.7800 time: 5.0972s\n",
            "Epoch: 1206 loss_train: 0.7495 acc_train: 0.8000 loss_val: 1.1974 acc_val: 0.7800 time: 5.1016s\n",
            "Epoch: 1207 loss_train: 0.6906 acc_train: 0.8200 loss_val: 1.1971 acc_val: 0.7800 time: 5.1057s\n",
            "Epoch: 1208 loss_train: 0.6819 acc_train: 0.8400 loss_val: 1.1994 acc_val: 0.7800 time: 5.1098s\n",
            "Epoch: 1209 loss_train: 0.6851 acc_train: 0.8200 loss_val: 1.2050 acc_val: 0.7800 time: 5.1152s\n",
            "Epoch: 1210 loss_train: 0.6644 acc_train: 0.8400 loss_val: 1.2099 acc_val: 0.7800 time: 5.1169s\n",
            "Epoch: 1211 loss_train: 0.7447 acc_train: 0.8200 loss_val: 1.2172 acc_val: 0.7400 time: 5.1186s\n",
            "Epoch: 1212 loss_train: 0.6828 acc_train: 0.8200 loss_val: 1.2287 acc_val: 0.7400 time: 5.1268s\n",
            "Epoch: 1213 loss_train: 0.6539 acc_train: 0.8400 loss_val: 1.2381 acc_val: 0.7400 time: 5.1311s\n",
            "Epoch: 1214 loss_train: 0.6808 acc_train: 0.8200 loss_val: 1.2524 acc_val: 0.7400 time: 5.1351s\n",
            "Epoch: 1215 loss_train: 0.6910 acc_train: 0.8200 loss_val: 1.2739 acc_val: 0.7400 time: 5.1392s\n",
            "Epoch: 1216 loss_train: 0.6719 acc_train: 0.8400 loss_val: 1.2972 acc_val: 0.7400 time: 5.1434s\n",
            "Epoch: 1217 loss_train: 0.6642 acc_train: 0.8200 loss_val: 1.3183 acc_val: 0.7400 time: 5.1474s\n",
            "Epoch: 1218 loss_train: 0.6614 acc_train: 0.8200 loss_val: 1.3382 acc_val: 0.7200 time: 5.1515s\n",
            "Epoch: 1219 loss_train: 0.6374 acc_train: 0.8400 loss_val: 1.3538 acc_val: 0.7200 time: 5.1555s\n",
            "Epoch: 1220 loss_train: 0.6329 acc_train: 0.8400 loss_val: 1.3678 acc_val: 0.7200 time: 5.1595s\n",
            "Epoch: 1221 loss_train: 0.6451 acc_train: 0.8600 loss_val: 1.3699 acc_val: 0.7200 time: 5.1639s\n",
            "Epoch: 1222 loss_train: 0.6361 acc_train: 0.8400 loss_val: 1.3701 acc_val: 0.7200 time: 5.1680s\n",
            "Epoch: 1223 loss_train: 0.7454 acc_train: 0.8200 loss_val: 1.3638 acc_val: 0.7200 time: 5.1721s\n",
            "Epoch: 1224 loss_train: 0.7232 acc_train: 0.8200 loss_val: 1.3542 acc_val: 0.7200 time: 5.1764s\n",
            "Epoch: 1225 loss_train: 0.6529 acc_train: 0.8400 loss_val: 1.3449 acc_val: 0.7200 time: 5.1805s\n",
            "Epoch: 1226 loss_train: 0.7024 acc_train: 0.8000 loss_val: 1.3384 acc_val: 0.7200 time: 5.1845s\n",
            "Epoch: 1227 loss_train: 0.7815 acc_train: 0.7800 loss_val: 1.3468 acc_val: 0.7200 time: 5.1885s\n",
            "Epoch: 1228 loss_train: 0.7328 acc_train: 0.8400 loss_val: 1.3534 acc_val: 0.7200 time: 5.1928s\n",
            "Epoch: 1229 loss_train: 0.6039 acc_train: 0.8600 loss_val: 1.3539 acc_val: 0.7200 time: 5.1970s\n",
            "Epoch: 1230 loss_train: 0.6418 acc_train: 0.8200 loss_val: 1.3515 acc_val: 0.7200 time: 5.2012s\n",
            "Epoch: 1231 loss_train: 0.6696 acc_train: 0.8400 loss_val: 1.3422 acc_val: 0.7200 time: 5.2055s\n",
            "Epoch: 1232 loss_train: 0.7096 acc_train: 0.8200 loss_val: 1.3388 acc_val: 0.7200 time: 5.2095s\n",
            "Epoch: 1233 loss_train: 0.6538 acc_train: 0.8400 loss_val: 1.3451 acc_val: 0.7200 time: 5.2137s\n",
            "Epoch: 1234 loss_train: 0.6664 acc_train: 0.8200 loss_val: 1.3503 acc_val: 0.7200 time: 5.2178s\n",
            "Epoch: 1235 loss_train: 0.7787 acc_train: 0.8200 loss_val: 1.3558 acc_val: 0.7200 time: 5.2221s\n",
            "Epoch: 1236 loss_train: 0.6086 acc_train: 0.8600 loss_val: 1.3535 acc_val: 0.7200 time: 5.2260s\n",
            "Epoch: 1237 loss_train: 0.6505 acc_train: 0.8400 loss_val: 1.3478 acc_val: 0.7200 time: 5.2299s\n",
            "Epoch: 1238 loss_train: 0.6800 acc_train: 0.8400 loss_val: 1.3353 acc_val: 0.7200 time: 5.2418s\n",
            "Epoch: 1239 loss_train: 0.6886 acc_train: 0.8200 loss_val: 1.3330 acc_val: 0.7400 time: 5.2430s\n",
            "Epoch: 1240 loss_train: 0.6925 acc_train: 0.8400 loss_val: 1.3280 acc_val: 0.7400 time: 5.2491s\n",
            "Epoch: 1241 loss_train: 0.6474 acc_train: 0.8400 loss_val: 1.3267 acc_val: 0.7400 time: 5.2530s\n",
            "Epoch: 1242 loss_train: 0.6960 acc_train: 0.8200 loss_val: 1.3255 acc_val: 0.7400 time: 5.2569s\n",
            "Epoch: 1243 loss_train: 0.6610 acc_train: 0.8400 loss_val: 1.3242 acc_val: 0.7400 time: 5.2607s\n",
            "Epoch: 1244 loss_train: 0.6552 acc_train: 0.8400 loss_val: 1.3222 acc_val: 0.7400 time: 5.2645s\n",
            "Epoch: 1245 loss_train: 0.6385 acc_train: 0.8400 loss_val: 1.3197 acc_val: 0.7400 time: 5.2683s\n",
            "Epoch: 1246 loss_train: 0.6923 acc_train: 0.8400 loss_val: 1.3233 acc_val: 0.7400 time: 5.2723s\n",
            "Epoch: 1247 loss_train: 0.6633 acc_train: 0.8400 loss_val: 1.3305 acc_val: 0.7400 time: 5.2762s\n",
            "Epoch: 1248 loss_train: 0.6645 acc_train: 0.8400 loss_val: 1.3395 acc_val: 0.7200 time: 5.2801s\n",
            "Epoch: 1249 loss_train: 0.6243 acc_train: 0.8400 loss_val: 1.3466 acc_val: 0.7200 time: 5.2840s\n",
            "Epoch: 1250 loss_train: 0.6875 acc_train: 0.8400 loss_val: 1.3550 acc_val: 0.7200 time: 5.2879s\n",
            "Epoch: 1251 loss_train: 0.6269 acc_train: 0.8600 loss_val: 1.3626 acc_val: 0.7200 time: 5.2919s\n",
            "Epoch: 1252 loss_train: 0.6332 acc_train: 0.8400 loss_val: 1.3674 acc_val: 0.7200 time: 5.2959s\n",
            "Epoch: 1253 loss_train: 0.6584 acc_train: 0.8200 loss_val: 1.3671 acc_val: 0.7200 time: 5.2999s\n",
            "Epoch: 1254 loss_train: 0.6300 acc_train: 0.8600 loss_val: 1.3671 acc_val: 0.7200 time: 5.3037s\n",
            "Epoch: 1255 loss_train: 0.6290 acc_train: 0.8600 loss_val: 1.3642 acc_val: 0.7200 time: 5.3076s\n",
            "Epoch: 1256 loss_train: 0.6744 acc_train: 0.8400 loss_val: 1.3569 acc_val: 0.7200 time: 5.3114s\n",
            "Epoch: 1257 loss_train: 0.8667 acc_train: 0.8000 loss_val: 1.3688 acc_val: 0.7200 time: 5.3152s\n",
            "Epoch: 1258 loss_train: 0.6330 acc_train: 0.8600 loss_val: 1.3768 acc_val: 0.7200 time: 5.3190s\n",
            "Epoch: 1259 loss_train: 0.8161 acc_train: 0.8200 loss_val: 1.3970 acc_val: 0.7200 time: 5.3229s\n",
            "Epoch: 1260 loss_train: 0.6913 acc_train: 0.8200 loss_val: 1.4128 acc_val: 0.7200 time: 5.3268s\n",
            "Epoch: 1261 loss_train: 0.6563 acc_train: 0.8400 loss_val: 1.4236 acc_val: 0.7200 time: 5.3306s\n",
            "Epoch: 1262 loss_train: 0.7064 acc_train: 0.8200 loss_val: 1.4284 acc_val: 0.7200 time: 5.3345s\n",
            "Epoch: 1263 loss_train: 0.6471 acc_train: 0.8400 loss_val: 1.4257 acc_val: 0.7200 time: 5.3383s\n",
            "Epoch: 1264 loss_train: 0.6679 acc_train: 0.8200 loss_val: 1.4155 acc_val: 0.7200 time: 5.3422s\n",
            "Epoch: 1265 loss_train: 0.6522 acc_train: 0.8200 loss_val: 1.4021 acc_val: 0.7200 time: 5.3460s\n",
            "Epoch: 1266 loss_train: 0.6116 acc_train: 0.8400 loss_val: 1.3876 acc_val: 0.7200 time: 5.3505s\n",
            "Epoch: 1267 loss_train: 0.6958 acc_train: 0.8200 loss_val: 1.3709 acc_val: 0.7200 time: 5.3560s\n",
            "Epoch: 1268 loss_train: 0.6280 acc_train: 0.8400 loss_val: 1.3575 acc_val: 0.7200 time: 5.3598s\n",
            "Epoch: 1269 loss_train: 0.6273 acc_train: 0.8400 loss_val: 1.3466 acc_val: 0.7400 time: 5.3636s\n",
            "Epoch: 1270 loss_train: 0.6490 acc_train: 0.8400 loss_val: 1.3410 acc_val: 0.7200 time: 5.3674s\n",
            "Epoch: 1271 loss_train: 0.6694 acc_train: 0.8400 loss_val: 1.3355 acc_val: 0.7200 time: 5.3711s\n",
            "Epoch: 1272 loss_train: 0.6921 acc_train: 0.8200 loss_val: 1.3331 acc_val: 0.7200 time: 5.3748s\n",
            "Epoch: 1273 loss_train: 0.6837 acc_train: 0.8200 loss_val: 1.3358 acc_val: 0.7200 time: 5.3786s\n",
            "Epoch: 1274 loss_train: 0.7076 acc_train: 0.8200 loss_val: 1.3445 acc_val: 0.7200 time: 5.3826s\n",
            "Epoch: 1275 loss_train: 0.6155 acc_train: 0.8600 loss_val: 1.3544 acc_val: 0.7200 time: 5.3865s\n",
            "Epoch: 1276 loss_train: 0.6686 acc_train: 0.8200 loss_val: 1.3683 acc_val: 0.7200 time: 5.3902s\n",
            "Epoch: 1277 loss_train: 0.6406 acc_train: 0.8400 loss_val: 1.3828 acc_val: 0.7200 time: 5.3941s\n",
            "Epoch: 1278 loss_train: 0.6890 acc_train: 0.8200 loss_val: 1.4038 acc_val: 0.7200 time: 5.3978s\n",
            "Epoch: 1279 loss_train: 0.6528 acc_train: 0.8400 loss_val: 1.4228 acc_val: 0.7200 time: 5.4037s\n",
            "Epoch: 1280 loss_train: 0.6305 acc_train: 0.8400 loss_val: 1.4404 acc_val: 0.7200 time: 5.4089s\n",
            "Epoch: 1281 loss_train: 0.7053 acc_train: 0.8200 loss_val: 1.4594 acc_val: 0.7200 time: 5.4131s\n",
            "Epoch: 1282 loss_train: 0.6405 acc_train: 0.8400 loss_val: 1.4771 acc_val: 0.7200 time: 5.4169s\n",
            "Epoch: 1283 loss_train: 0.6612 acc_train: 0.8200 loss_val: 1.4665 acc_val: 0.7200 time: 5.4207s\n",
            "Epoch: 1284 loss_train: 0.6676 acc_train: 0.8400 loss_val: 1.4559 acc_val: 0.7200 time: 5.4245s\n",
            "Epoch: 1285 loss_train: 0.6158 acc_train: 0.8600 loss_val: 1.4455 acc_val: 0.7200 time: 5.4283s\n",
            "Epoch: 1286 loss_train: 0.6402 acc_train: 0.8400 loss_val: 1.4369 acc_val: 0.7200 time: 5.4320s\n",
            "Epoch: 1287 loss_train: 0.6673 acc_train: 0.8200 loss_val: 1.4265 acc_val: 0.7200 time: 5.4360s\n",
            "Epoch: 1288 loss_train: 0.6370 acc_train: 0.8400 loss_val: 1.4135 acc_val: 0.7200 time: 5.4400s\n",
            "Epoch: 1289 loss_train: 0.6635 acc_train: 0.8400 loss_val: 1.4074 acc_val: 0.7200 time: 5.4429s\n",
            "Epoch: 1290 loss_train: 0.6745 acc_train: 0.8200 loss_val: 1.4036 acc_val: 0.7200 time: 5.4449s\n",
            "Epoch: 1291 loss_train: 0.6526 acc_train: 0.8400 loss_val: 1.3775 acc_val: 0.7200 time: 5.4470s\n",
            "Epoch: 1292 loss_train: 0.7248 acc_train: 0.8200 loss_val: 1.3552 acc_val: 0.7400 time: 5.4490s\n",
            "Epoch: 1293 loss_train: 0.6568 acc_train: 0.8400 loss_val: 1.3351 acc_val: 0.7400 time: 5.4619s\n",
            "Epoch: 1294 loss_train: 0.6534 acc_train: 0.8400 loss_val: 1.3209 acc_val: 0.7400 time: 5.4658s\n",
            "Epoch: 1295 loss_train: 0.7324 acc_train: 0.8200 loss_val: 1.3128 acc_val: 0.7400 time: 5.4694s\n",
            "Epoch: 1296 loss_train: 0.7110 acc_train: 0.8200 loss_val: 1.3096 acc_val: 0.7400 time: 5.4732s\n",
            "Epoch: 1297 loss_train: 0.6576 acc_train: 0.8400 loss_val: 1.3068 acc_val: 0.7400 time: 5.4777s\n",
            "Epoch: 1298 loss_train: 0.6548 acc_train: 0.8400 loss_val: 1.3066 acc_val: 0.7400 time: 5.4817s\n",
            "Epoch: 1299 loss_train: 0.6146 acc_train: 0.8600 loss_val: 1.3058 acc_val: 0.7400 time: 5.4861s\n",
            "Epoch: 1300 loss_train: 0.6727 acc_train: 0.8200 loss_val: 1.3036 acc_val: 0.7400 time: 5.4899s\n",
            "Epoch: 1301 loss_train: 0.6564 acc_train: 0.8400 loss_val: 1.2987 acc_val: 0.7400 time: 5.4941s\n",
            "Epoch: 1302 loss_train: 0.6287 acc_train: 0.8600 loss_val: 1.2986 acc_val: 0.7400 time: 5.4979s\n",
            "Epoch: 1303 loss_train: 0.6731 acc_train: 0.8200 loss_val: 1.3001 acc_val: 0.7400 time: 5.5021s\n",
            "Epoch: 1304 loss_train: 0.6317 acc_train: 0.8400 loss_val: 1.3044 acc_val: 0.7400 time: 5.5058s\n",
            "Epoch: 1305 loss_train: 0.6496 acc_train: 0.8400 loss_val: 1.3119 acc_val: 0.7400 time: 5.5094s\n",
            "Epoch: 1306 loss_train: 0.6548 acc_train: 0.8400 loss_val: 1.3187 acc_val: 0.7400 time: 5.5132s\n",
            "Epoch: 1307 loss_train: 0.6499 acc_train: 0.8200 loss_val: 1.3237 acc_val: 0.7400 time: 5.5171s\n",
            "Epoch: 1308 loss_train: 0.6700 acc_train: 0.8400 loss_val: 1.3298 acc_val: 0.7400 time: 5.5209s\n",
            "Epoch: 1309 loss_train: 0.6480 acc_train: 0.8400 loss_val: 1.3385 acc_val: 0.7200 time: 5.5247s\n",
            "Epoch: 1310 loss_train: 0.6814 acc_train: 0.8200 loss_val: 1.3553 acc_val: 0.7200 time: 5.5285s\n",
            "Epoch: 1311 loss_train: 0.6423 acc_train: 0.8200 loss_val: 1.3748 acc_val: 0.7200 time: 5.5322s\n",
            "Epoch: 1312 loss_train: 0.6278 acc_train: 0.8600 loss_val: 1.3907 acc_val: 0.7200 time: 5.5366s\n",
            "Epoch: 1313 loss_train: 0.6588 acc_train: 0.8200 loss_val: 1.4052 acc_val: 0.7200 time: 5.5440s\n",
            "Epoch: 1314 loss_train: 0.6733 acc_train: 0.8400 loss_val: 1.4200 acc_val: 0.7200 time: 5.5479s\n",
            "Epoch: 1315 loss_train: 0.6716 acc_train: 0.8400 loss_val: 1.4255 acc_val: 0.7200 time: 5.5516s\n",
            "Epoch: 1316 loss_train: 0.7045 acc_train: 0.8200 loss_val: 1.4208 acc_val: 0.7200 time: 5.5555s\n",
            "Epoch: 1317 loss_train: 0.6162 acc_train: 0.8600 loss_val: 1.4146 acc_val: 0.7200 time: 5.5594s\n",
            "Epoch: 1318 loss_train: 0.6761 acc_train: 0.8200 loss_val: 1.4206 acc_val: 0.7200 time: 5.5633s\n",
            "Epoch: 1319 loss_train: 0.6805 acc_train: 0.8400 loss_val: 1.4285 acc_val: 0.7200 time: 5.5670s\n",
            "Epoch: 1320 loss_train: 0.7033 acc_train: 0.8200 loss_val: 1.4133 acc_val: 0.7200 time: 5.5706s\n",
            "Epoch: 1321 loss_train: 0.6848 acc_train: 0.8200 loss_val: 1.4030 acc_val: 0.7200 time: 5.5745s\n",
            "Epoch: 1322 loss_train: 0.7418 acc_train: 0.8200 loss_val: 1.3903 acc_val: 0.7200 time: 5.5784s\n",
            "Epoch: 1323 loss_train: 0.6681 acc_train: 0.8400 loss_val: 1.3744 acc_val: 0.7400 time: 5.5821s\n",
            "Epoch: 1324 loss_train: 0.6894 acc_train: 0.8400 loss_val: 1.3613 acc_val: 0.7400 time: 5.5859s\n",
            "Epoch: 1325 loss_train: 0.6552 acc_train: 0.8400 loss_val: 1.3541 acc_val: 0.7400 time: 5.5895s\n",
            "Epoch: 1326 loss_train: 0.8063 acc_train: 0.8000 loss_val: 1.3477 acc_val: 0.7400 time: 5.5932s\n",
            "Epoch: 1327 loss_train: 0.6528 acc_train: 0.8400 loss_val: 1.3442 acc_val: 0.7400 time: 5.5970s\n",
            "Epoch: 1328 loss_train: 0.6806 acc_train: 0.8400 loss_val: 1.3461 acc_val: 0.7400 time: 5.6008s\n",
            "Epoch: 1329 loss_train: 0.8323 acc_train: 0.8000 loss_val: 1.3004 acc_val: 0.7400 time: 5.6052s\n",
            "Epoch: 1330 loss_train: 0.6662 acc_train: 0.8200 loss_val: 1.2687 acc_val: 0.7400 time: 5.6094s\n",
            "Epoch: 1331 loss_train: 0.7077 acc_train: 0.8200 loss_val: 1.2517 acc_val: 0.7400 time: 5.6132s\n",
            "Epoch: 1332 loss_train: 0.6310 acc_train: 0.8600 loss_val: 1.2462 acc_val: 0.7400 time: 5.6171s\n",
            "Epoch: 1333 loss_train: 0.6702 acc_train: 0.8200 loss_val: 1.2539 acc_val: 0.7400 time: 5.6210s\n",
            "Epoch: 1334 loss_train: 0.6401 acc_train: 0.8400 loss_val: 1.2642 acc_val: 0.7400 time: 5.6248s\n",
            "Epoch: 1335 loss_train: 0.6579 acc_train: 0.8200 loss_val: 1.2793 acc_val: 0.7400 time: 5.6286s\n",
            "Epoch: 1336 loss_train: 0.6968 acc_train: 0.8000 loss_val: 1.2953 acc_val: 0.7400 time: 5.6323s\n",
            "Epoch: 1337 loss_train: 0.6641 acc_train: 0.8400 loss_val: 1.3123 acc_val: 0.7400 time: 5.6361s\n",
            "Epoch: 1338 loss_train: 0.6712 acc_train: 0.8400 loss_val: 1.3302 acc_val: 0.7400 time: 5.6399s\n",
            "Epoch: 1339 loss_train: 0.7151 acc_train: 0.8000 loss_val: 1.3485 acc_val: 0.7400 time: 5.6437s\n",
            "Epoch: 1340 loss_train: 0.7617 acc_train: 0.8200 loss_val: 1.3624 acc_val: 0.7400 time: 5.6473s\n",
            "Epoch: 1341 loss_train: 0.7012 acc_train: 0.8400 loss_val: 1.3731 acc_val: 0.7400 time: 5.6512s\n",
            "Epoch: 1342 loss_train: 0.6860 acc_train: 0.8200 loss_val: 1.3865 acc_val: 0.7400 time: 5.6647s\n",
            "Epoch: 1343 loss_train: 0.6945 acc_train: 0.8000 loss_val: 1.3743 acc_val: 0.7400 time: 5.6687s\n",
            "Epoch: 1344 loss_train: 0.6747 acc_train: 0.8400 loss_val: 1.3641 acc_val: 0.7400 time: 5.6725s\n",
            "Epoch: 1345 loss_train: 0.6575 acc_train: 0.8400 loss_val: 1.3587 acc_val: 0.7400 time: 5.6764s\n",
            "Epoch: 1346 loss_train: 0.6562 acc_train: 0.8400 loss_val: 1.3566 acc_val: 0.7400 time: 5.6802s\n",
            "Epoch: 1347 loss_train: 0.7000 acc_train: 0.8400 loss_val: 1.3554 acc_val: 0.7400 time: 5.6840s\n",
            "Epoch: 1348 loss_train: 0.6755 acc_train: 0.8200 loss_val: 1.3544 acc_val: 0.7400 time: 5.6881s\n",
            "Epoch: 1349 loss_train: 0.7290 acc_train: 0.8000 loss_val: 1.3560 acc_val: 0.7400 time: 5.6921s\n",
            "Epoch: 1350 loss_train: 0.6585 acc_train: 0.8200 loss_val: 1.3610 acc_val: 0.7400 time: 5.6959s\n",
            "Epoch: 1351 loss_train: 0.6609 acc_train: 0.8400 loss_val: 1.3679 acc_val: 0.7400 time: 5.6996s\n",
            "Epoch: 1352 loss_train: 0.7346 acc_train: 0.8200 loss_val: 1.3603 acc_val: 0.7400 time: 5.7037s\n",
            "Epoch: 1353 loss_train: 0.7047 acc_train: 0.8200 loss_val: 1.3597 acc_val: 0.7400 time: 5.7075s\n",
            "Epoch: 1354 loss_train: 0.8113 acc_train: 0.8000 loss_val: 1.3475 acc_val: 0.7400 time: 5.7113s\n",
            "Epoch: 1355 loss_train: 0.6758 acc_train: 0.8200 loss_val: 1.3463 acc_val: 0.7400 time: 5.7151s\n",
            "Epoch: 1356 loss_train: 0.6507 acc_train: 0.8400 loss_val: 1.3476 acc_val: 0.7400 time: 5.7189s\n",
            "Epoch: 1357 loss_train: 0.6698 acc_train: 0.8400 loss_val: 1.3510 acc_val: 0.7200 time: 5.7229s\n",
            "Epoch: 1358 loss_train: 0.7025 acc_train: 0.8400 loss_val: 1.3493 acc_val: 0.7200 time: 5.7268s\n",
            "Epoch: 1359 loss_train: 0.6642 acc_train: 0.8400 loss_val: 1.3597 acc_val: 0.7200 time: 5.7322s\n",
            "Epoch: 1360 loss_train: 0.6338 acc_train: 0.8600 loss_val: 1.3696 acc_val: 0.7200 time: 5.7405s\n",
            "Epoch: 1361 loss_train: 0.6094 acc_train: 0.8400 loss_val: 1.3807 acc_val: 0.7200 time: 5.7446s\n",
            "Epoch: 1362 loss_train: 0.6725 acc_train: 0.8400 loss_val: 1.3907 acc_val: 0.7200 time: 5.7483s\n",
            "Epoch: 1363 loss_train: 0.6602 acc_train: 0.8400 loss_val: 1.4000 acc_val: 0.7200 time: 5.7521s\n",
            "Epoch: 1364 loss_train: 0.6682 acc_train: 0.8400 loss_val: 1.4107 acc_val: 0.7200 time: 5.7558s\n",
            "Epoch: 1365 loss_train: 0.6232 acc_train: 0.8600 loss_val: 1.4229 acc_val: 0.7200 time: 5.7596s\n",
            "Epoch: 1366 loss_train: 0.6550 acc_train: 0.8400 loss_val: 1.4355 acc_val: 0.7200 time: 5.7641s\n",
            "Epoch: 1367 loss_train: 0.7155 acc_train: 0.8200 loss_val: 1.4199 acc_val: 0.7200 time: 5.7677s\n",
            "Epoch: 1368 loss_train: 0.6935 acc_train: 0.8200 loss_val: 1.4033 acc_val: 0.7200 time: 5.7716s\n",
            "Epoch: 1369 loss_train: 0.6803 acc_train: 0.8200 loss_val: 1.3843 acc_val: 0.7200 time: 5.7753s\n",
            "Epoch: 1370 loss_train: 0.7301 acc_train: 0.8200 loss_val: 1.3717 acc_val: 0.7200 time: 5.7789s\n",
            "Epoch: 1371 loss_train: 0.6420 acc_train: 0.8600 loss_val: 1.3574 acc_val: 0.7200 time: 5.7827s\n",
            "Epoch: 1372 loss_train: 0.6336 acc_train: 0.8400 loss_val: 1.3472 acc_val: 0.7400 time: 5.7866s\n",
            "Epoch: 1373 loss_train: 0.6863 acc_train: 0.8400 loss_val: 1.3376 acc_val: 0.7400 time: 5.7902s\n",
            "Epoch: 1374 loss_train: 0.6591 acc_train: 0.8200 loss_val: 1.3231 acc_val: 0.7400 time: 5.7939s\n",
            "Epoch: 1375 loss_train: 0.6641 acc_train: 0.8400 loss_val: 1.3184 acc_val: 0.7400 time: 5.7976s\n",
            "Epoch: 1376 loss_train: 0.6598 acc_train: 0.8400 loss_val: 1.3004 acc_val: 0.7400 time: 5.8012s\n",
            "Epoch: 1377 loss_train: 0.7015 acc_train: 0.8200 loss_val: 1.3026 acc_val: 0.7400 time: 5.8051s\n",
            "Epoch: 1378 loss_train: 0.6763 acc_train: 0.8200 loss_val: 1.3169 acc_val: 0.7400 time: 5.8088s\n",
            "Epoch: 1379 loss_train: 0.6814 acc_train: 0.8400 loss_val: 1.3281 acc_val: 0.7400 time: 5.8126s\n",
            "Epoch: 1380 loss_train: 0.8084 acc_train: 0.8200 loss_val: 1.3431 acc_val: 0.7200 time: 5.8164s\n",
            "Epoch: 1381 loss_train: 0.6303 acc_train: 0.8400 loss_val: 1.3611 acc_val: 0.7200 time: 5.8201s\n",
            "Epoch: 1382 loss_train: 0.6607 acc_train: 0.8200 loss_val: 1.3608 acc_val: 0.7200 time: 5.8237s\n",
            "Epoch: 1383 loss_train: 0.7037 acc_train: 0.8200 loss_val: 1.3587 acc_val: 0.7400 time: 5.8279s\n",
            "Epoch: 1384 loss_train: 0.7013 acc_train: 0.8400 loss_val: 1.3604 acc_val: 0.7400 time: 5.8317s\n",
            "Epoch: 1385 loss_train: 0.6497 acc_train: 0.8600 loss_val: 1.3629 acc_val: 0.7400 time: 5.8354s\n",
            "Epoch: 1386 loss_train: 0.6990 acc_train: 0.8200 loss_val: 1.3655 acc_val: 0.7400 time: 5.8391s\n",
            "Epoch: 1387 loss_train: 0.6685 acc_train: 0.8400 loss_val: 1.3683 acc_val: 0.7400 time: 5.8429s\n",
            "Epoch: 1388 loss_train: 0.6094 acc_train: 0.8600 loss_val: 1.3688 acc_val: 0.7400 time: 5.8470s\n",
            "Epoch: 1389 loss_train: 0.7007 acc_train: 0.8200 loss_val: 1.3679 acc_val: 0.7400 time: 5.8506s\n",
            "Epoch: 1390 loss_train: 0.6485 acc_train: 0.8600 loss_val: 1.3668 acc_val: 0.7400 time: 5.8543s\n",
            "Epoch: 1391 loss_train: 0.6819 acc_train: 0.8400 loss_val: 1.3695 acc_val: 0.7200 time: 5.8580s\n",
            "Epoch: 1392 loss_train: 0.6416 acc_train: 0.8400 loss_val: 1.3704 acc_val: 0.7200 time: 5.8619s\n",
            "Epoch: 1393 loss_train: 0.6323 acc_train: 0.8400 loss_val: 1.3724 acc_val: 0.7200 time: 5.8727s\n",
            "Epoch: 1394 loss_train: 0.6530 acc_train: 0.8200 loss_val: 1.3570 acc_val: 0.7400 time: 5.8781s\n",
            "Epoch: 1395 loss_train: 0.6295 acc_train: 0.8400 loss_val: 1.3475 acc_val: 0.7400 time: 5.8826s\n",
            "Epoch: 1396 loss_train: 0.6663 acc_train: 0.8400 loss_val: 1.3427 acc_val: 0.7400 time: 5.8869s\n",
            "Epoch: 1397 loss_train: 0.6805 acc_train: 0.8400 loss_val: 1.3476 acc_val: 0.7400 time: 5.8913s\n",
            "Epoch: 1398 loss_train: 0.6600 acc_train: 0.8600 loss_val: 1.3497 acc_val: 0.7400 time: 5.8954s\n",
            "Epoch: 1399 loss_train: 0.7020 acc_train: 0.8400 loss_val: 1.3519 acc_val: 0.7400 time: 5.8996s\n",
            "Epoch: 1400 loss_train: 0.6682 acc_train: 0.8400 loss_val: 1.3539 acc_val: 0.7400 time: 5.9037s\n",
            "Epoch: 1401 loss_train: 0.6862 acc_train: 0.8200 loss_val: 1.3634 acc_val: 0.7400 time: 5.9078s\n",
            "Epoch: 1402 loss_train: 0.6568 acc_train: 0.8400 loss_val: 1.3686 acc_val: 0.7400 time: 5.9120s\n",
            "Epoch: 1403 loss_train: 0.7072 acc_train: 0.8200 loss_val: 1.3774 acc_val: 0.7200 time: 5.9162s\n",
            "Epoch: 1404 loss_train: 0.7451 acc_train: 0.8200 loss_val: 1.3820 acc_val: 0.7200 time: 5.9205s\n",
            "Epoch: 1405 loss_train: 0.6795 acc_train: 0.8200 loss_val: 1.3856 acc_val: 0.7200 time: 5.9244s\n",
            "Epoch: 1406 loss_train: 0.6327 acc_train: 0.8600 loss_val: 1.3884 acc_val: 0.7200 time: 5.9284s\n",
            "Epoch: 1407 loss_train: 0.6090 acc_train: 0.8600 loss_val: 1.3900 acc_val: 0.7200 time: 5.9323s\n",
            "Epoch: 1408 loss_train: 0.6951 acc_train: 0.8000 loss_val: 1.3983 acc_val: 0.7200 time: 5.9365s\n",
            "Epoch: 1409 loss_train: 0.7239 acc_train: 0.8000 loss_val: 1.4070 acc_val: 0.7200 time: 5.9407s\n",
            "Epoch: 1410 loss_train: 0.6541 acc_train: 0.8600 loss_val: 1.4158 acc_val: 0.7200 time: 5.9449s\n",
            "Epoch: 1411 loss_train: 0.7731 acc_train: 0.8000 loss_val: 1.4282 acc_val: 0.7200 time: 5.9542s\n",
            "Epoch: 1412 loss_train: 0.6573 acc_train: 0.8400 loss_val: 1.4401 acc_val: 0.7200 time: 5.9584s\n",
            "Epoch: 1413 loss_train: 0.6727 acc_train: 0.8000 loss_val: 1.4279 acc_val: 0.7200 time: 5.9626s\n",
            "Epoch: 1414 loss_train: 0.7352 acc_train: 0.8200 loss_val: 1.4122 acc_val: 0.7200 time: 5.9666s\n",
            "Epoch: 1415 loss_train: 0.6537 acc_train: 0.8600 loss_val: 1.3973 acc_val: 0.7200 time: 5.9707s\n",
            "Epoch: 1416 loss_train: 0.6315 acc_train: 0.8400 loss_val: 1.3848 acc_val: 0.7200 time: 5.9748s\n",
            "Epoch: 1417 loss_train: 0.7296 acc_train: 0.8200 loss_val: 1.3781 acc_val: 0.7200 time: 5.9788s\n",
            "Epoch: 1418 loss_train: 0.6707 acc_train: 0.8600 loss_val: 1.3712 acc_val: 0.7200 time: 5.9829s\n",
            "Epoch: 1419 loss_train: 0.6473 acc_train: 0.8600 loss_val: 1.3683 acc_val: 0.7200 time: 5.9870s\n",
            "Epoch: 1420 loss_train: 0.6638 acc_train: 0.8400 loss_val: 1.3523 acc_val: 0.7200 time: 5.9915s\n",
            "Epoch: 1421 loss_train: 0.6321 acc_train: 0.8600 loss_val: 1.3438 acc_val: 0.7400 time: 5.9968s\n",
            "Epoch: 1422 loss_train: 0.6824 acc_train: 0.8200 loss_val: 1.3348 acc_val: 0.7400 time: 6.0013s\n",
            "Epoch: 1423 loss_train: 0.6849 acc_train: 0.8400 loss_val: 1.3296 acc_val: 0.7400 time: 6.0084s\n",
            "Epoch: 1424 loss_train: 0.7204 acc_train: 0.8200 loss_val: 1.3257 acc_val: 0.7400 time: 6.0123s\n",
            "Epoch: 1425 loss_train: 0.6204 acc_train: 0.8400 loss_val: 1.3231 acc_val: 0.7400 time: 6.0164s\n",
            "Epoch: 1426 loss_train: 0.6468 acc_train: 0.8600 loss_val: 1.3214 acc_val: 0.7400 time: 6.0203s\n",
            "Epoch: 1427 loss_train: 0.6387 acc_train: 0.8400 loss_val: 1.3232 acc_val: 0.7400 time: 6.0242s\n",
            "Epoch: 1428 loss_train: 0.7095 acc_train: 0.8400 loss_val: 1.3202 acc_val: 0.7400 time: 6.0282s\n",
            "Epoch: 1429 loss_train: 0.6242 acc_train: 0.8400 loss_val: 1.3268 acc_val: 0.7400 time: 6.0324s\n",
            "Epoch: 1430 loss_train: 0.7188 acc_train: 0.8200 loss_val: 1.3468 acc_val: 0.7400 time: 6.0364s\n",
            "Epoch: 1431 loss_train: 0.6468 acc_train: 0.8400 loss_val: 1.3369 acc_val: 0.7400 time: 6.0402s\n",
            "Epoch: 1432 loss_train: 0.6099 acc_train: 0.8600 loss_val: 1.3280 acc_val: 0.7400 time: 6.0442s\n",
            "Epoch: 1433 loss_train: 0.6475 acc_train: 0.8400 loss_val: 1.3210 acc_val: 0.7400 time: 6.0501s\n",
            "Epoch: 1434 loss_train: 0.6616 acc_train: 0.8400 loss_val: 1.3141 acc_val: 0.7400 time: 6.0541s\n",
            "Epoch: 1435 loss_train: 0.6457 acc_train: 0.8400 loss_val: 1.3114 acc_val: 0.7400 time: 6.0580s\n",
            "Epoch: 1436 loss_train: 0.6495 acc_train: 0.8400 loss_val: 1.3143 acc_val: 0.7400 time: 6.0617s\n",
            "Epoch: 1437 loss_train: 0.6510 acc_train: 0.8400 loss_val: 1.3221 acc_val: 0.7400 time: 6.0658s\n",
            "Epoch: 1438 loss_train: 0.7074 acc_train: 0.8200 loss_val: 1.3302 acc_val: 0.7400 time: 6.0695s\n",
            "Epoch: 1439 loss_train: 0.6617 acc_train: 0.8400 loss_val: 1.3398 acc_val: 0.7400 time: 6.0748s\n",
            "Epoch: 1440 loss_train: 0.6602 acc_train: 0.8400 loss_val: 1.3536 acc_val: 0.7200 time: 6.0769s\n",
            "Epoch: 1441 loss_train: 0.6816 acc_train: 0.8200 loss_val: 1.3720 acc_val: 0.7200 time: 6.0787s\n",
            "Epoch: 1442 loss_train: 0.7312 acc_train: 0.8200 loss_val: 1.3918 acc_val: 0.7200 time: 6.0803s\n",
            "Epoch: 1443 loss_train: 0.6187 acc_train: 0.8400 loss_val: 1.4086 acc_val: 0.7200 time: 6.0820s\n",
            "Epoch: 1444 loss_train: 0.7036 acc_train: 0.8200 loss_val: 1.4302 acc_val: 0.7200 time: 6.0959s\n",
            "Epoch: 1445 loss_train: 0.6587 acc_train: 0.8400 loss_val: 1.4541 acc_val: 0.7200 time: 6.1013s\n",
            "Epoch: 1446 loss_train: 0.6693 acc_train: 0.8400 loss_val: 1.4464 acc_val: 0.7200 time: 6.1053s\n",
            "Epoch: 1447 loss_train: 0.6270 acc_train: 0.8600 loss_val: 1.4339 acc_val: 0.7200 time: 6.1091s\n",
            "Epoch: 1448 loss_train: 0.6579 acc_train: 0.8400 loss_val: 1.4239 acc_val: 0.7200 time: 6.1129s\n",
            "Epoch: 1449 loss_train: 0.6410 acc_train: 0.8400 loss_val: 1.4170 acc_val: 0.7200 time: 6.1168s\n",
            "Epoch: 1450 loss_train: 0.6408 acc_train: 0.8400 loss_val: 1.4156 acc_val: 0.7200 time: 6.1206s\n",
            "Epoch: 1451 loss_train: 0.6378 acc_train: 0.8400 loss_val: 1.4127 acc_val: 0.7200 time: 6.1244s\n",
            "Epoch: 1452 loss_train: 0.6548 acc_train: 0.8400 loss_val: 1.4127 acc_val: 0.7200 time: 6.1281s\n",
            "Epoch: 1453 loss_train: 0.6193 acc_train: 0.8600 loss_val: 1.4038 acc_val: 0.7200 time: 6.1319s\n",
            "Epoch: 1454 loss_train: 0.6559 acc_train: 0.8400 loss_val: 1.3996 acc_val: 0.7200 time: 6.1356s\n",
            "Epoch: 1455 loss_train: 0.6257 acc_train: 0.8600 loss_val: 1.3960 acc_val: 0.7200 time: 6.1393s\n",
            "Epoch: 1456 loss_train: 0.6611 acc_train: 0.8400 loss_val: 1.3788 acc_val: 0.7200 time: 6.1437s\n",
            "Epoch: 1457 loss_train: 0.6427 acc_train: 0.8200 loss_val: 1.3654 acc_val: 0.7200 time: 6.1476s\n",
            "Epoch: 1458 loss_train: 0.6864 acc_train: 0.8200 loss_val: 1.3509 acc_val: 0.7400 time: 6.1514s\n",
            "Epoch: 1459 loss_train: 0.6696 acc_train: 0.8200 loss_val: 1.3432 acc_val: 0.7400 time: 6.1552s\n",
            "Epoch: 1460 loss_train: 0.6827 acc_train: 0.8200 loss_val: 1.3395 acc_val: 0.7400 time: 6.1588s\n",
            "Epoch: 1461 loss_train: 0.7427 acc_train: 0.8200 loss_val: 1.3453 acc_val: 0.7200 time: 6.1625s\n",
            "Epoch: 1462 loss_train: 0.6191 acc_train: 0.8600 loss_val: 1.3512 acc_val: 0.7200 time: 6.1662s\n",
            "Epoch: 1463 loss_train: 0.6843 acc_train: 0.8200 loss_val: 1.3660 acc_val: 0.7200 time: 6.1700s\n",
            "Epoch: 1464 loss_train: 0.6151 acc_train: 0.8600 loss_val: 1.3813 acc_val: 0.7200 time: 6.1738s\n",
            "Epoch: 1465 loss_train: 0.6415 acc_train: 0.8400 loss_val: 1.4050 acc_val: 0.7200 time: 6.1776s\n",
            "Epoch: 1466 loss_train: 0.6934 acc_train: 0.8400 loss_val: 1.4271 acc_val: 0.7200 time: 6.1813s\n",
            "Epoch: 1467 loss_train: 0.6620 acc_train: 0.8400 loss_val: 1.4532 acc_val: 0.7200 time: 6.1850s\n",
            "Epoch: 1468 loss_train: 0.6871 acc_train: 0.8400 loss_val: 1.4869 acc_val: 0.7200 time: 6.1887s\n",
            "Epoch: 1469 loss_train: 0.6179 acc_train: 0.8600 loss_val: 1.5098 acc_val: 0.7200 time: 6.1924s\n",
            "Epoch: 1470 loss_train: 0.6247 acc_train: 0.8400 loss_val: 1.5168 acc_val: 0.7200 time: 6.1962s\n",
            "Epoch: 1471 loss_train: 0.6424 acc_train: 0.8600 loss_val: 1.5036 acc_val: 0.7200 time: 6.1998s\n",
            "Epoch: 1472 loss_train: 0.7230 acc_train: 0.8200 loss_val: 1.4852 acc_val: 0.7200 time: 6.2036s\n",
            "Epoch: 1473 loss_train: 0.6129 acc_train: 0.8400 loss_val: 1.4740 acc_val: 0.7200 time: 6.2074s\n",
            "Epoch: 1474 loss_train: 0.6627 acc_train: 0.8200 loss_val: 1.4665 acc_val: 0.7200 time: 6.2112s\n",
            "Epoch: 1475 loss_train: 0.6457 acc_train: 0.8200 loss_val: 1.4733 acc_val: 0.7200 time: 6.2149s\n",
            "Epoch: 1476 loss_train: 0.6632 acc_train: 0.8400 loss_val: 1.4844 acc_val: 0.7200 time: 6.2187s\n",
            "Epoch: 1477 loss_train: 0.6431 acc_train: 0.8600 loss_val: 1.4883 acc_val: 0.7200 time: 6.2224s\n",
            "Epoch: 1478 loss_train: 0.6706 acc_train: 0.8400 loss_val: 1.4861 acc_val: 0.7200 time: 6.2262s\n",
            "Epoch: 1479 loss_train: 0.6537 acc_train: 0.8600 loss_val: 1.4777 acc_val: 0.7200 time: 6.2301s\n",
            "Epoch: 1480 loss_train: 0.6930 acc_train: 0.8200 loss_val: 1.4796 acc_val: 0.7200 time: 6.2338s\n",
            "Epoch: 1481 loss_train: 0.7086 acc_train: 0.8200 loss_val: 1.4878 acc_val: 0.7200 time: 6.2374s\n",
            "Epoch: 1482 loss_train: 0.6500 acc_train: 0.8400 loss_val: 1.4931 acc_val: 0.7200 time: 6.2411s\n",
            "Epoch: 1483 loss_train: 0.6740 acc_train: 0.8400 loss_val: 1.5026 acc_val: 0.7200 time: 6.2449s\n",
            "Epoch: 1484 loss_train: 0.6093 acc_train: 0.8600 loss_val: 1.5060 acc_val: 0.7200 time: 6.2487s\n",
            "Epoch: 1485 loss_train: 0.6464 acc_train: 0.8200 loss_val: 1.5046 acc_val: 0.7200 time: 6.2524s\n",
            "Epoch: 1486 loss_train: 0.6630 acc_train: 0.8400 loss_val: 1.4975 acc_val: 0.7200 time: 6.2562s\n",
            "Epoch: 1487 loss_train: 0.6174 acc_train: 0.8600 loss_val: 1.4828 acc_val: 0.7200 time: 6.2609s\n",
            "Epoch: 1488 loss_train: 0.6834 acc_train: 0.8400 loss_val: 1.4684 acc_val: 0.7200 time: 6.2663s\n",
            "Epoch: 1489 loss_train: 0.6292 acc_train: 0.8200 loss_val: 1.4642 acc_val: 0.7200 time: 6.2705s\n",
            "Epoch: 1490 loss_train: 0.6445 acc_train: 0.8600 loss_val: 1.4605 acc_val: 0.7200 time: 6.2743s\n",
            "Epoch: 1491 loss_train: 0.6195 acc_train: 0.8400 loss_val: 1.4552 acc_val: 0.7200 time: 6.2780s\n",
            "Epoch: 1492 loss_train: 0.7152 acc_train: 0.8400 loss_val: 1.4454 acc_val: 0.7200 time: 6.2818s\n",
            "Epoch: 1493 loss_train: 0.6818 acc_train: 0.8200 loss_val: 1.4372 acc_val: 0.7200 time: 6.2856s\n",
            "Epoch: 1494 loss_train: 0.6221 acc_train: 0.8400 loss_val: 1.4287 acc_val: 0.7200 time: 6.2886s\n",
            "Epoch: 1495 loss_train: 0.6160 acc_train: 0.8400 loss_val: 1.4035 acc_val: 0.7200 time: 6.2904s\n",
            "Epoch: 1496 loss_train: 0.7209 acc_train: 0.8000 loss_val: 1.3948 acc_val: 0.7200 time: 6.3008s\n",
            "Epoch: 1497 loss_train: 0.6925 acc_train: 0.8400 loss_val: 1.3841 acc_val: 0.7200 time: 6.3047s\n",
            "Epoch: 1498 loss_train: 0.6545 acc_train: 0.8200 loss_val: 1.3794 acc_val: 0.7200 time: 6.3085s\n",
            "Epoch: 1499 loss_train: 0.6672 acc_train: 0.8400 loss_val: 1.3744 acc_val: 0.7400 time: 6.3123s\n",
            "Epoch: 1500 loss_train: 0.6100 acc_train: 0.8600 loss_val: 1.3698 acc_val: 0.7400 time: 6.3160s\n",
            "Test set results: loss= 3.0081 accuracy= 0.7000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Optimization Finished!\n",
            "Total time elapsed: 6.7146s\n",
            "GCN(\n",
            "  (gc1): GraphConvolution()\n",
            "  (gc2): GraphConvolution()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2UE74eOd5LMN",
        "outputId": "394d971b-200f-4979-ebc6-cbefd9172c0b"
      },
      "source": [
        "len(features[0])"
      ],
      "execution_count": 270,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "767"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 270
        }
      ]
    }
  ]
}